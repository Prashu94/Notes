# Database Design and Data Management

## Overview

Database architecture for stock exchanges must handle extreme write throughput, maintain ACID guarantees for financial transactions, support complex queries for analytics, scale horizontally, provide microsecond-level latency, and retain data for regulatory compliance periods. A polyglot persistence approach uses specialized databases for different access patterns.

## Relational Databases

### Trade and Order Database
PostgreSQL or Oracle serve as the system of record for critical financial data. Schema design uses normalized tables: orders table (order_id PK, account_id, symbol, side, order_type, price, quantity, status, timestamps), trades table (trade_id PK, buy_order_id FK, sell_order_id FK, symbol, price, quantity, execution_time), accounts table (account_id PK, customer_id FK, account_type, status, balances), positions table (position_id PK, account_id FK, symbol, quantity, average_price, realized_pl, unrealized_pl). Indexing strategy critical for performance: clustered index on primary key, non-clustered indexes on: account_id + symbol for position lookups, symbol + timestamp for market data queries, status + timestamp for pending orders, composite indexes for common query patterns. Partitioning improves manageability: range partitioning by date (trade_date), list partitioning by symbol or exchange, hash partitioning for even distribution, partition pruning eliminates scanning irrelevant partitions.

Transaction isolation level considerations: READ COMMITTED default (balance between consistency and concurrency), REPEATABLE READ for critical financial operations, SERIALIZABLE for regulatory reporting (prevent phantom reads), optimistic locking using version columns (detect concurrent modifications). Write-ahead logging ensures durability: synchronous_commit ON for trades (guarantee persistence), asynchronous for less critical updates, archive WAL to object storage, WAL retention supports point-in-time recovery. Replication for high availability: streaming replication to standby (near-real-time), synchronous for zero data loss (higher latency), asynchronous for better performance (small RPO), cascading replication reduces primary load, automatic failover with Patroni or Pacemaker.

Connection pooling essential for high concurrency: connection pool sizing (2-3x CPU cores typical), pgBouncer or PgPool for PostgreSQL, connection lifetime limits (recycle connections), statement timeout prevents long-running queries, idle connection timeout frees resources. Query optimization: EXPLAIN ANALYZE identifies slow queries, materialized views for complex aggregations, query result caching, prepared statements reduce parse overhead, batch inserts for bulk loading, parallel queries leverage multiple cores. Maintenance operations: VACUUM reclaims space and prevents wraparound, ANALYZE updates statistics for query planner, REINDEX rebuilds corrupted indexes, automated in cron jobs during off-hours.

### Reference Data Database
Master data management for symbols, exchanges, corporate actions. Tables include: symbols (symbol_id PK, ticker, isin, cusip, exchange, security_type, lot_size, tick_size), exchanges (exchange_id PK, exchange_code, name, timezone, trading_hours), corporate_actions (action_id PK, symbol_id FK, action_type, ex_date, record_date, payable_date, details), holidays (holiday_id PK, exchange_id FK, holiday_date, description). Data characteristics: low write volume (updates infrequent), high read volume (referenced by all trades), requires caching (Redis with daily refresh), slowly changing dimensions (track historical values with effective dates), master data governance (single source of truth). Integration points: market data vendors for symbol additions, exchange feeds for corporate actions, compliance team for trading halts, multiple consuming applications requiring consistency.

### Customer and Account Database
Stores customer information and account details. Tables: customers (customer_id PK, first_name, last_name, date_of_birth, ssn_encrypted, email, phone), addresses (address_id PK, customer_id FK, address_type, street, city, state, zip, country), accounts (account_id PK, customer_id FK, account_number, account_type, status, open_date), documents (document_id PK, customer_id FK, document_type, storage_location, uploaded_date, expiry_date). Security requirements: PII encryption at column level, field-level access control, data masking for non-privileged users, audit logging of all access, right to be forgotten compliance (GDPR), data retention policies. Compliance considerations: KYC documentation storage, AML screening results, FATCA/CRS reporting status, accreditation verification, pattern day trader flagging. Performance: read-heavy after initial onboarding, write spikes during mass mailings or market events, caching customer profiles, sharding by customer_id for very large scale.

## NoSQL Databases

### Order Book State - Cassandra
Distributed database for high-write throughput. Data model: wide column family, partition key (symbol), clustering columns (price, timestamp), order details as columns. Write characteristics: append-only (immutable), distributed across cluster nodes, eventual consistency acceptable (microseconds), tunable consistency (QUORUM for critical reads), no read-before-write (optimistic updates). Cluster topology: minimum 3 nodes per datacenter, replication factor 3 (RF=3), multiple datacenters for disaster recovery, token-aware driver routes requests optimally, gossip protocol for cluster coordination. Performance tuning: compaction strategy (Size-Tiered for write-heavy), bloom filters reduce disk seeks, key cache for frequently accessed partitions, row cache for hot data, commit log on separate disk, SSTable compression reduces storage.

Query patterns optimized for: get orders by symbol (partition key lookup), get orders at price level (clustering key range), time-range queries (clustering column order), avoid secondary indexes (full cluster scan), use materialized views for alternate access patterns. Consistency levels: ANY (fire and forget, don't use), ONE (fast but risky), QUORUM (majority, good balance), ALL (slowest, highest consistency), LOCAL_QUORUM (within datacenter), EACH_QUORUM (across datacenters). Maintenance: repair for consistency (merkle trees), nodetool for cluster management, backup via snapshots, monitoring via JMX metrics, upgrading via rolling restart.

### Market Data Time Series - TimescaleDB
PostgreSQL extension optimized for time-series data. Hypertables: automatically partition by time, chunks (typically 1 day or 1 week), chunk sizing balances query performance and management, retention policies auto-drop old chunks, compression on older chunks (10x space savings). Data ingestion: bulk copy for high throughput (COPY command), asynchronous commit for speed, parallel workers for partitions, continuous aggregates for real-time rollups, refresh policies keep aggregates current. Common queries: recent price history (WHERE time > NOW() - INTERVAL '1 day'), OHLCV bars (time_bucket aggregation), gaps and islands (find missing data), interpolation (fill missing points), last observation carried forward.

Continuous aggregates: materialized views that update incrementally, real-time aggregation layer, background job refreshes, partial aggregations combine with raw data, significant query speedup for dashboards. Compression: after chunk ages (e.g., older than 7 days), columnar format reduces size, transparent to queries, decompress on demand, trade storage for query speed on old data. Time-based retention: drop_chunks() removes old data, automated via policy, archive to S3 before dropping (compliance), compression before archival, regulatory retention (7 years typical).

### Document Store - MongoDB
Flexible schema for varying data structures. Use cases: user preferences and settings (each user different fields), notification templates (HTML content), compliance documents (metadata and content), API request/response logs, audit trails (schema evolution over time). Data model: BSON documents (binary JSON), collections (like tables), embedded documents vs references (denormalization vs normalization trade-off), schema validation (enforce required fields), indexes on commonly queried fields. Sharding for horizontal scale: shard key selection critical (even distribution, query locality), range-based (sequential values) or hash-based (random distribution), compound shard keys (multiple fields), avoid hotspots (monotonic values), balancer migrates chunks.

Aggregation pipeline: powerful data processing, stages (match, group, project, sort, limit), indexes support early stages, server-side processing, complex analytics without application code. Replication: replica sets (3+ members), primary receives writes, secondaries replicate asynchronously, automatic failover (election), read preference (primary, secondary, nearest), write concern (acknowledged by how many members). Transactions: multi-document ACID (since 4.0), cross-shard transactions (since 4.2), performance impact (use sparingly), causal consistency alternative (without transaction overhead).

### Key-Value Store - Redis
In-memory data structure store for ultra-low latency. Data structures: strings (simple values), hashes (object fields), lists (ordered collections), sets (unique members), sorted sets (scored members), streams (append-only log), geospatial (location data), bitmaps and HyperLogLog (analytics). Use cases: session management (user login state, shopping cart), caching (database query results, API responses, reference data), rate limiting (token bucket algorithm), real-time leaderboards (sorted sets), pub/sub messaging (market data fanout), distributed locking (RedLock algorithm), job queues (background processing).

Persistence options: RDB snapshots (point-in-time dumps), AOF (append-only file logs every write), combination (snapshot baseline with AOF incremental), persistence impacts performance (async background saves). Clustering: Redis Cluster (built-in sharding), 16384 hash slots, master-slave per shard, client-side routing, resharding online, sentinel for non-clustered HA. Memory management: eviction policies (LRU, LFU, random, volatile-only, allkeys), maxmemory limit, memory optimization (compression, encoding), persistence to disk if needed. Performance: single-threaded (6.0+ multi-threaded I/O), pipelining reduces round trips, Lua scripting for atomic operations, command batching with MULTI/EXEC.

## Analytical Databases

### ClickHouse for Analytics
Columnar database for fast analytical queries. Architecture: column-oriented storage (read only relevant columns), aggressive compression (10-100x typical), vectorized query execution (SIMD), distributed queries across cluster, merges handle updates (not true update-in-place). Use cases: trade analytics (volume analysis, price patterns), risk calculations (VaR, stress tests), reporting (daily P&L, compliance reports), market surveillance (pattern detection), user behavior analysis (clickstream).

Table engines: MergeTree family (default, supports indexes), ReplacingMergeTree (deduplication), SummingMergeTree (pre-aggregation), AggregatingMergeTree (stores aggregation states), CollapsingMergeTree (mutable updates via row pairs). Partitioning: by date or datetime typically, partition pruning excludes irrelevant data, too many partitions hurt performance (thousands ok, millions bad), OPTIMIZE merges partitions. Materialized views: incremental updates on insert, store pre-aggregated data, query rewriting uses view automatically, chain views for multi-level aggregation.

Distributed tables: Distributed engine shards data, local table on each node, query coordinator gathers results, replication via ReplicatedMergeTree, ZooKeeper coordinates replicas. Query optimization: primary key (sort order not unique constraint), secondary indexes (skip indexes, bloom filters), PREWHERE filters before reading columns, query profiling identifies bottlenecks, sampling for approximate results. Operations: background merges (async consolidation), mutations (async updates/deletes), monitoring (system tables), backup (filesystem snapshots), upgrades (rolling per node).

### Amazon Redshift for Data Warehousing
MPP (massively parallel processing) data warehouse. Architecture: leader node (query planning), compute nodes (data and processing), node slices (parallel units), columnar storage (efficient compression), zone maps (skip blocks). Loading data: COPY from S3 (parallel, fastest), INSERT (use batch, not row-by-row), Redshift Spectrum (query S3 directly), data pipelines (scheduled ETL), streaming ingestion (Kinesis).

Distribution styles: KEY (distribute by column, co-locate joins), EVEN (round-robin, uniform distribution), ALL (replicate to every node, dimension tables), AUTO (Redshift decides). Sort keys: compound (multiple columns, filter in order), interleaved (multiple column combinations, flexible), choose based on query patterns, vacuum reclaims space after deletes. Workload management: queues (separate ETL from reporting), query priorities, concurrency limits, memory allocation per queue, short query acceleration, automatic WLM tuning.

Performance optimization: analyze for statistics, vacuum for sort order, distribution key avoids shuffling, sort key enables zone map pruning, columnar encoding saves space, result caching, materialized views, late binding views, stored procedures. Monitoring: query performance (SVL_QUERY_REPORT), disk usage, queue wait times, compile cache, automatic tuning recommendations. Concurrency scaling: burst clusters for read queries, charged per-second, transparent to users, additional compute without resize.

## Data Modeling Approaches

### Event Sourcing
Store all state changes as immutable events. Benefits: complete audit trail (every change recorded), temporal queries (state at any point in time), replay events (reconstruct state, fix bugs), debugging (understand how current state reached), compliance (full history for regulators). Event store: append-only log (Kafka, EventStore), events immutable (never modified), event ordering (per aggregate), event schema evolution (versioning). Event types for trading: OrderSubmitted, OrderModified, OrderCancelled, OrderMatched, TradeCancelled, PositionOpened, PositionClosed. Projections: materialized views of current state, subscribe to event stream, update read model on event, multiple projections for different views, snapshot for performance (avoid replaying millions of events).

Challenges: eventual consistency (projections lag behind events), event schema evolution (older events different structure), storage growth (archive old events), query performance (projections vs querying events), complexity (more moving parts). Patterns: CQRS often paired with event sourcing (write events, read projections), snapshots reduce replay time (save state periodically), event versioning (upcasters transform old to new), compensating events (undo previous events, not delete). Implementation: event store (specialized database or Kafka), command handlers (validate and emit events), event handlers (update projections), saga pattern (long-running transactions across aggregates).

### CQRS (Command Query Responsibility Segregation)
Separate models for writes and reads. Write model: optimized for transaction processing, enforce business rules, append events, normalized schema, strong consistency required. Read model: optimized for queries, denormalized views, eventual consistency acceptable, multiple read models for different use cases (dashboard, reports, API), cached aggressively. Synchronization: events propagate from write to read model, message bus (Kafka) carries events, handlers update read models, retry on failure, idempotent handlers (handle duplicate events).

Benefits: independent scaling (scale reads and writes separately), optimized data structures (reads different from writes), flexibility (multiple read models), performance (queries don't compete with writes), evolution (change read model without affecting writes). Use cases: order management (complex write logic, simple reads), risk calculation (heavy writes during trading, complex queries for reports), market data (high write throughput, various read patterns). Challenges: complexity (two models to maintain), consistency (read model lags), duplication (data in multiple places), infrastructure (message bus, event handlers).

### Denormalization for Performance
Trade storage and consistency for speed. Techniques: embedded documents (nested objects vs foreign keys), redundant data (duplicate across tables), pre-computed aggregates (sum, count calculated ahead), materialized views (query results cached as tables). When to denormalize: read-heavy workloads (mostly queries), query complexity (joins slow), latency requirements (must be fast), data access patterns (frequently together). Examples: embed address in customer record (avoid join), duplicate symbol name in trade records (avoid reference lookup), store current position in account table (avoid sum aggregation), pre-calculate daily P&L (avoid scanning trades).

Synchronization challenges: writes update multiple places, eventual consistency (time lag), potential inconsistency (update failures), increased storage (duplication), maintenance complexity (more update logic). Mitigation: triggers update denormalized data, application logic handles updates, periodic reconciliation batch, accept staleness for non-critical, monitor for divergence. NoSQL databases encourage denormalization: document databases (embed related data), wide-column stores (store related columns together), key-value stores (store entire object), graph databases (property graphs with redundancy).

### Microservice Data Patterns
Each microservice owns its data. Database per service: private database, only accessible via service API, polyglot persistence (different database types), data isolation (failures don't spread), independent scaling (scale database with service). Challenges: distributed transactions (two-phase commit or saga pattern), data consistency (eventual consistency), querying across services (API composition, CQRS), data duplication (each service stores what it needs), migration (refactoring data ownership difficult).

Saga pattern: long-running transactions across services, compensating transactions undo on failure, choreography (events drive flow) vs orchestration (central coordinator), each step has rollback logic, eventual consistency result. API composition: query multiple services and combine, client calls each service, orchestrator aggregates results, parallel requests where possible, inconsistent data possible (timestamps differ). CQRS for cross-service queries: dedicated read database, aggregates data from multiple services, event-driven updates, optimized for queries, eventual consistency. Shared database anti-pattern: avoid database shared by multiple services, tight coupling (schema changes impact all), concurrency (locking across services), scalability bottleneck (single database).

## Data Migration and Schema Evolution

### Schema Versioning
Managing schema changes over time. Versioning strategies: timestamp-based (schema_20260117), semantic versioning (v1.2.3), sequential numbers (v001, v002), tag-based (git tags for schemas). Migration scripts: forward migrations (upgrade schema), backward migrations (rollback), idempotent (safe to run multiple times), tested (staging environment first), documented (reason for change). Tools: Flyway (Java-based migration), Liquibase (XML or YAML migrations), Alembic (Python SQLAlchemy), Django migrations (Python ORM), Rails Active Record (Ruby), custom scripts (database-specific).

Zero-downtime migrations: backwards compatible changes (add columns, don't remove), expand-contract pattern (add new, migrate data, remove old), feature flags (switch between schemas), dual writes (write to both old and new), read from old (until migration complete), cutover (switch reads to new), cleanup (remove old schema). Blue-green deployments: two identical environments, deploy new version to green, smoke test, switch traffic (load balancer), rollback if issues, old environment (blue) standby. Challenges: large tables (migration takes hours), locking (exclusive locks block transactions), foreign key constraints (check before drop), triggers and stored procedures (may break), replication lag (standby behind).

### Data Quality and Validation
Ensuring data integrity and accuracy. Validation rules: data type checking (int, string, date), range validation (price > 0, quantity > 0), format validation (email regex, phone format), referential integrity (foreign keys exist), business rules (account active before trade), cross-field validation (sell quantity <= position). Constraints in database: NOT NULL (required fields), UNIQUE (prevent duplicates), CHECK (condition must be true), FOREIGN KEY (referential integrity), DEFAULT (default values). Application-level validation: richer logic (complex rules), better error messages, avoid database round trip, validation framework (Bean Validation, Joi), async validation (external service calls).

Data cleansing: identify issues (duplicate records, missing values, inconsistent formats), scrubbing (remove or correct), standardization (consistent format), deduplication (merge duplicates), enrichment (add missing information), monitoring (ongoing quality checks). Master Data Management: golden record (single source of truth), data stewardship (ownership and rules), data governance (policies and procedures), data lineage (track origins and transformations), matching and merging (identify same entities), survivorship rules (choose best values). Monitoring: data quality metrics (completeness, accuracy, consistency), anomaly detection (unusual patterns), drift detection (schema or distribution changes), reconciliation (compare across systems), alerting (quality degradation).

## Data Retention and Archival

### Regulatory Retention Requirements
Financial data retained for compliance. Retention periods: trade data (7 years typically), customer communications (3-7 years), regulatory reports (5+ years), audit trails (7 years), varies by jurisdiction (US, EU, Asia different). Hot storage: recent data (last 1-2 years), online and fast access, expensive storage (SSD), indexed for queries. Warm storage: older data (2-5 years), slower access acceptable, cheaper storage (HDD or cloud), compressed, less frequently accessed. Cold storage: archival (5+ years), rare access (compliance only), very cheap (Glacier, tape), long retrieval time (hours), compressed heavily.

Compliance considerations: immutability (prevent tampering), encryption (protect sensitive data), access controls (audit access), searchability (respond to requests), legal holds (suspend deletion), chain of custody (track handling), format preservation (readable after years). Archive strategy: periodic archival jobs (nightly or monthly), partition drops (remove from main database), export to archive format (Parquet, Avro), store in object storage (S3 Glacier), catalog metadata (index for searching), test restores (verify recoverability), document process (runbooks).

### Data Lifecycle Management
Automated progression through storage tiers. Lifecycle policies: define rules (age-based, access-based), automatic transitions (to cheaper storage), automatic deletion (after retention period), versioning (keep multiple versions), cross-region replication (disaster recovery). S3 lifecycle example: Standard for 30 days, transition to Standard-IA after 30 days, transition to Glacier after 90 days, transition to Deep Archive after 1 year, delete after 7 years. Cost optimization: storage costs (cheaper over time), retrieval costs (more expensive for cold), balance (access patterns vs cost), monitoring (actual access vs expectations), tuning (adjust policies based on usage).

Deletion challenges: referential integrity (orphaned records), partial deletion (some tables not others), cascade deletes (related records), soft deletes (mark as deleted, not remove), hard deletes (actually remove), compliance (prove deletion occurred), shredding (multi-level delete for sensitivity). GDPR right to be forgotten: identify all data for user, pseudonymize or delete, update indexes and caches, document deletion, notify third parties, retention exceptions (legal obligation), retention periods post-request, user notification of completion.

## Backup and Recovery

### Backup Strategy
Comprehensive backup approach for data protection. Backup types: full (complete copy, weekly), incremental (changes since last, daily), differential (changes since last full), continuous (transaction logs), snapshot (point-in-time), logical (SQL dumps), physical (file-level). Backup frequency: critical data (continuous), daily (most data), weekly (static reference data), before major changes (migrations, upgrades), retention (daily for 30 days, weekly for 1 year, yearly for 7 years).

Backup storage: on-site (fast recovery, risk of site disaster), off-site (disaster protection, slower recovery), cloud (cost-effective, unlimited capacity, geographic redundancy), 3-2-1 rule (3 copies, 2 different media, 1 off-site). Backup validation: test restores (monthly), automate testing (restore to dev), verify integrity (checksums), test recovery procedures (runbooks), measure RTO (actual recovery time), document results (track trends). Encryption: encrypt in transit (network protection), encrypt at rest (storage protection), key management (separate from backups), compliance requirement, password protection for critical backups.

### Point-in-Time Recovery
Restore database to specific moment. PITR mechanism: base backup (full backup), transaction logs (all changes since), replay logs (apply transactions), stop at timestamp (desired recovery point). Use cases: application bug (restored before bug), data corruption (before corruption), user error (deleted important data), testing (clone production at specific point), audit (examine historical state). PITR challenges: storage (keep all logs), log shipping (archive logs), retention period (how far back), recovery time (replay takes time), testing (verify recovery works).

Database-specific implementations: PostgreSQL PITR (WAL archiving, recovery.conf), MySQL PITR (binlog), SQL Server PITR (transaction log backups), Oracle PITR (archived redo logs), MongoDB PITR (oplog), Cassandra PITR (commitlog). Cloud managed databases: RDS automated backups (5-35 days retention), Aurora backtrack (rewind without restore), Cosmos DB continuous backup (35 days), Cloud SQL PITR (7 days), automated snapshots (daily), manual snapshots (keep indefinitely).

### Disaster Recovery Testing
Regular DR drills validate recovery procedures. Testing scenarios: complete data center failure, database corruption, accidental deletion, ransomware attack, natural disaster, cyber attack. Testing process: define scenario (specific failure), execute failover (follow runbook), validate functionality (data accessible, applications work), measure metrics (RTO achieved, RPO verified), document issues (gaps identified), update runbooks (fix procedures), communicate results (stakeholders informed).

Frequency: quarterly full DR test (complete failover), monthly partial test (specific component), annual stress test (complex scenario), after major changes (architecture updates), tabletop exercises (walk through steps without actual failover). Challenges: production impact (testing affects live systems), cost (DR resources running), time (hours to complete), rollback (restore to normal), coordination (many teams involved), realistic (staging vs production differences). Metrics: RTO actual vs target, RPO actual vs target, time to detect failure, time to make decision, time to execute failover, time to validate, team readiness score.

---

**Document Version**: 1.0  
**Last Updated**: January 17, 2026  
**Status**: Complete
