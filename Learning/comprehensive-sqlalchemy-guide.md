# Complete End-to-End SQLAlchemy Learning Guide with PostgreSQL

## Table of Contents
1. [Environment Setup & Project Structure](#part-1-environment-setup--project-structure)
2. [Project 1: Complete Blog API with Authentication](#part-2-project-1-complete-blog-api)
3. [Project 2: E-commerce System with Complex Relationships](#part-3-project-2-e-commerce-system)
4. [Project 3: Working with Existing Legacy Database](#part-4-project-3-legacy-database-integration)
5. [Project 4: Advanced Transaction Management System](#part-5-project-4-transaction-management)
6. [Project 5: Real-time Analytics Dashboard](#part-6-project-5-analytics-system)
7. [Testing, Deployment & Best Practices](#part-7-testing-deployment--best-practices)

---

## Part 1: Environment Setup & Project Structure

### Complete Project Setup

#### 1.1 Directory Structure
```
sqlalchemy-complete-learning/
├── .env                          # Environment variables
├── .env.example                  # Environment template
├── .gitignore                   # Git ignore file
├── requirements.txt             # Python dependencies
├── docker-compose.yml           # PostgreSQL setup
├── README.md                    # Project documentation
├── alembic.ini                  # Alembic configuration
├── pytest.ini                  # Test configuration
│
├── alembic/                     # Database migrations
│   ├── env.py
│   └── versions/
│
├── app/                         # Main application
│   ├── __init__.py
│   ├── main.py                  # FastAPI application
│   ├── config.py                # Configuration management
│   ├── database.py              # Database setup
│   │
│   ├── core/                    # Core utilities
│   │   ├── __init__.py
│   │   ├── security.py          # Authentication/security
│   │   ├── exceptions.py        # Custom exceptions
│   │   └── middleware.py        # Custom middleware
│   │
│   ├── models/                  # SQLAlchemy models
│   │   ├── __init__.py
│   │   ├── base.py              # Base model class
│   │   ├── user.py              # User models
│   │   ├── blog.py              # Blog models
│   │   ├── ecommerce.py         # E-commerce models
│   │   └── analytics.py         # Analytics models
│   │
│   ├── schemas/                 # Pydantic schemas
│   │   ├── __init__.py
│   │   ├── user.py
│   │   ├── blog.py
│   │   └── common.py
│   │
│   ├── crud/                    # CRUD operations
│   │   ├── __init__.py
│   │   ├── base.py              # Base CRUD class
│   │   ├── user.py              # User CRUD
│   │   ├── blog.py              # Blog CRUD
│   │   └── transactions.py      # Transaction management
│   │
│   ├── api/                     # API endpoints
│   │   ├── __init__.py
│   │   ├── deps.py              # Dependencies
│   │   └── v1/                  # API version 1
│   │       ├── __init__.py
│   │       ├── auth.py          # Authentication endpoints
│   │       ├── users.py         # User endpoints
│   │       ├── blog.py          # Blog endpoints
│   │       └── analytics.py     # Analytics endpoints
│   │
│   └── utils/                   # Utility functions
│       ├── __init__.py
│       ├── helpers.py
│       └── decorators.py
│
├── tests/                       # Test files
│   ├── __init__.py
│   ├── conftest.py              # Test configuration
│   ├── test_models.py           # Model tests
│   ├── test_crud.py             # CRUD tests
│   ├── test_api.py              # API tests
│   └── test_transactions.py     # Transaction tests
│
└── scripts/                     # Utility scripts
    ├── init_db.py               # Database initialization
    ├── seed_data.py             # Sample data
    └── backup_restore.py        # Database backup/restore
```

#### 1.2 Environment Configuration

**Create .env file:**
```env
# Database Configuration
DATABASE_URL=postgresql://postgres:password@localhost:5432/sqlalchemy_learning
DATABASE_URL_ASYNC=postgresql+asyncpg://postgres:password@localhost:5432/sqlalchemy_learning
TEST_DATABASE_URL=postgresql://postgres:password@localhost:5432/sqlalchemy_test

# Application Configuration
SECRET_KEY=your-super-secret-key-change-in-production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Environment
ENVIRONMENT=development
DEBUG=True
API_V1_STR=/api/v1

# Redis (for caching and sessions)
REDIS_URL=redis://localhost:6379

# Logging
LOG_LEVEL=INFO
```

**Create .env.example:**
```env
DATABASE_URL=postgresql://username:password@localhost:5432/database_name
DATABASE_URL_ASYNC=postgresql+asyncpg://username:password@localhost:5432/database_name
TEST_DATABASE_URL=postgresql://username:password@localhost:5432/test_database_name
SECRET_KEY=change-me-in-production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
ENVIRONMENT=development
DEBUG=True
API_V1_STR=/api/v1
REDIS_URL=redis://localhost:6379
LOG_LEVEL=INFO
```

**Create requirements.txt:**
```txt
# Core SQLAlchemy and Database
sqlalchemy==2.0.23
psycopg2-binary==2.9.7
asyncpg==0.29.0
alembic==1.12.1

# FastAPI and Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6

# Authentication and Security
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-decouple==3.8

# Database Tools and ORM Extensions
sqlalchemy-utils==0.41.1
sqlalchemy-json==0.7.0

# Data Validation and Serialization
pydantic==2.5.0
pydantic-settings==2.1.0

# Development and Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
httpx==0.25.2
factory-boy==3.3.0

# Utilities
python-dotenv==1.0.0
click==8.1.7
rich==13.7.0

# Caching and Performance
redis==5.0.1
cachetools==5.3.2

# Monitoring and Logging
structlog==23.2.0
```

**Create docker-compose.yml:**
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    container_name: sqlalchemy_postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sqlalchemy_learning
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    command: postgres -c log_statement=all -c log_destination=stderr

  postgres_test:
    image: postgres:15
    container_name: sqlalchemy_postgres_test
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sqlalchemy_test
    ports:
      - "5433:5432"
    tmpfs:
      - /var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    container_name: sqlalchemy_redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

  pgadmin:
    image: dpage/pgadmin4
    container_name: sqlalchemy_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "8080:80"
    depends_on:
      - postgres

volumes:
  postgres_data:
  redis_data:
```

#### 1.3 Core Configuration Files

**app/config.py** - Complete Configuration Management:
```python
"""
Complete configuration management for SQLAlchemy learning projects.
This module handles all environment variables, database settings, and application configuration.
"""

import os
from functools import lru_cache
from typing import Optional, List
from pydantic import BaseSettings, validator, PostgresDsn
from pydantic_settings import SettingsConfigDict


class DatabaseSettings(BaseSettings):
    """Database configuration settings."""
    
    # Primary database URLs
    database_url: PostgresDsn
    database_url_async: Optional[str] = None
    test_database_url: Optional[PostgresDsn] = None
    
    # Connection pool settings
    pool_size: int = 10
    max_overflow: int = 20
    pool_pre_ping: bool = True
    pool_recycle: int = 3600
    pool_timeout: int = 30
    
    # Query and logging settings
    echo_sql: bool = False
    echo_pool: bool = False
    
    # Connection retry settings
    max_retries: int = 3
    retry_delay: float = 1.0
    
    @validator('database_url_async', pre=True, always=True)
    def build_async_url(cls, v: Optional[str], values: dict) -> str:
        if v:
            return v
        sync_url = str(values.get('database_url', ''))
        return sync_url.replace('postgresql://', 'postgresql+asyncpg://')
    
    @validator('test_database_url', pre=True, always=True)
    def build_test_url(cls, v: Optional[str], values: dict) -> Optional[str]:
        if v:
            return v
        # Auto-generate test URL from main URL
        main_url = str(values.get('database_url', ''))
        if main_url:
            return main_url.replace('/sqlalchemy_learning', '/sqlalchemy_test')
        return None
    
    model_config = SettingsConfigDict(env_prefix="DB_", case_sensitive=False)


class SecuritySettings(BaseSettings):
    """Security and authentication settings."""
    
    secret_key: str
    algorithm: str = "HS256"
    access_token_expire_minutes: int = 30
    refresh_token_expire_days: int = 7
    
    # Password settings
    password_min_length: int = 8
    password_require_uppercase: bool = True
    password_require_lowercase: bool = True
    password_require_numbers: bool = True
    password_require_special: bool = True
    
    # CORS settings
    cors_origins: List[str] = ["http://localhost:3000", "http://localhost:8000"]
    cors_methods: List[str] = ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    cors_headers: List[str] = ["*"]
    
    @validator('secret_key')
    def validate_secret_key(cls, v: str) -> str:
        if len(v) < 32:
            raise ValueError('Secret key must be at least 32 characters long')
        return v
    
    model_config = SettingsConfigDict(env_prefix="SECURITY_", case_sensitive=False)


class RedisSettings(BaseSettings):
    """Redis configuration for caching and sessions."""
    
    redis_url: str = "redis://localhost:6379"
    redis_password: Optional[str] = None
    redis_db: int = 0
    redis_max_connections: int = 20
    redis_socket_timeout: int = 5
    redis_socket_connect_timeout: int = 5
    
    # Cache settings
    cache_ttl: int = 3600  # 1 hour default TTL
    session_ttl: int = 86400  # 24 hours for sessions
    
    model_config = SettingsConfigDict(env_prefix="REDIS_", case_sensitive=False)


class ApplicationSettings(BaseSettings):
    """Main application settings that combine all configuration."""
    
    # App metadata
    app_name: str = "SQLAlchemy Complete Learning API"
    app_version: str = "1.0.0"
    app_description: str = "Comprehensive SQLAlchemy learning platform with PostgreSQL"
    
    # Environment
    environment: str = "development"
    debug: bool = True
    testing: bool = False
    
    # API settings
    api_v1_str: str = "/api/v1"
    docs_url: str = "/docs"
    redoc_url: str = "/redoc"
    
    # Logging
    log_level: str = "INFO"
    log_format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # File upload settings
    max_upload_size: int = 10 * 1024 * 1024  # 10MB
    allowed_extensions: List[str] = [".jpg", ".jpeg", ".png", ".gif", ".pdf", ".txt"]
    upload_path: str = "uploads"
    
    # Pagination defaults
    default_page_size: int = 20
    max_page_size: int = 100
    
    # Sub-settings
    database: DatabaseSettings = DatabaseSettings()
    security: SecuritySettings = SecuritySettings()
    redis: RedisSettings = RedisSettings()
    
    @validator('environment')
    def validate_environment(cls, v: str) -> str:
        allowed_envs = ['development', 'testing', 'staging', 'production']
        if v.lower() not in allowed_envs:
            raise ValueError(f'Environment must be one of: {allowed_envs}')
        return v.lower()
    
    @property
    def is_development(self) -> bool:
        return self.environment == "development"
    
    @property
    def is_production(self) -> bool:
        return self.environment == "production"
    
    @property
    def is_testing(self) -> bool:
        return self.environment == "testing"
    
    model_config = SettingsConfigDict(
        env_file=".env", 
        env_file_encoding="utf-8", 
        case_sensitive=False
    )


@lru_cache()
def get_settings() -> ApplicationSettings:
    """Get cached application settings."""
    return ApplicationSettings()


# Usage example
if __name__ == "__main__":
    settings = get_settings()
    print(f"App: {settings.app_name}")
    print(f"Environment: {settings.environment}")
    print(f"Database URL: {settings.database.database_url}")
    print(f"Debug mode: {settings.debug}")
```

**app/database.py** - Advanced Database Setup:
```python
"""
Complete database configuration and session management.
This module sets up both sync and async database connections with comprehensive
connection pooling, error handling, and monitoring.
"""

import logging
import time
from contextlib import contextmanager, asynccontextmanager
from typing import AsyncGenerator, Generator

from sqlalchemy import create_engine, event, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import QueuePool
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError

from app.config import get_settings

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Get settings
settings = get_settings()

# Create engines with optimized configuration
engine = create_engine(
    str(settings.database.database_url),
    poolclass=QueuePool,
    pool_size=settings.database.pool_size,
    max_overflow=settings.database.max_overflow,
    pool_pre_ping=settings.database.pool_pre_ping,
    pool_recycle=settings.database.pool_recycle,
    pool_timeout=settings.database.pool_timeout,
    echo=settings.database.echo_sql,
    echo_pool=settings.database.echo_pool,
    future=True,  # Use SQLAlchemy 2.0 style
    connect_args={
        "connect_timeout": 10,
        "application_name": settings.app_name,
    }
)

async_engine = create_async_engine(
    settings.database.database_url_async,
    pool_size=settings.database.pool_size,
    max_overflow=settings.database.max_overflow,
    pool_pre_ping=settings.database.pool_pre_ping,
    pool_recycle=settings.database.pool_recycle,
    echo=settings.database.echo_sql,
    future=True,
    connect_args={
        "server_settings": {
            "application_name": settings.app_name,
        }
    }
)

# Session factories
SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
    expire_on_commit=False
)

AsyncSessionLocal = sessionmaker(
    async_engine,
    class_=AsyncSession,
    autocommit=False,
    autoflush=False,
    expire_on_commit=False
)


# Database event handlers for monitoring and optimization
@event.listens_for(Engine, "connect")
def set_postgresql_settings(dbapi_connection, connection_record):
    """Set PostgreSQL-specific connection settings."""
    with dbapi_connection.cursor() as cursor:
        # Set timezone
        cursor.execute("SET timezone TO 'UTC'")
        
        # Set search path
        cursor.execute("SET search_path TO public")
        
        # Optimize for our workload
        cursor.execute("SET shared_preload_libraries = 'pg_stat_statements'")
        cursor.execute("SET log_statement = 'none'")  # Reduce logging in production
        
        # Connection-specific optimizations
        cursor.execute("SET statement_timeout = '30s'")
        cursor.execute("SET lock_timeout = '10s'")


@event.listens_for(Engine, "checkout")
def checkout_handler(dbapi_connection, connection_record, connection_proxy):
    """Log connection checkout events."""
    logger.debug(f"Connection {id(dbapi_connection)} checked out from pool")
    connection_record.checkout_time = time.time()


@event.listens_for(Engine, "checkin")
def checkin_handler(dbapi_connection, connection_record):
    """Log connection checkin events and track usage time."""
    if hasattr(connection_record, 'checkout_time'):
        usage_time = time.time() - connection_record.checkout_time
        logger.debug(f"Connection {id(dbapi_connection)} used for {usage_time:.2f}s")


@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """Track query execution time."""
    context._query_start_time = time.time()


@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """Log slow queries and execution time."""
    total_time = time.time() - context._query_start_time
    
    if total_time > 1.0:  # Log queries taking more than 1 second
        logger.warning(f"Slow query ({total_time:.2f}s): {statement[:100]}...")
    elif settings.database.echo_sql:
        logger.debug(f"Query executed in {total_time:.4f}s")


# Dependency injection for FastAPI
def get_db() -> Generator[Session, None, None]:
    """
    Dependency to get database session.
    
    Usage:
        @app.get("/users/")
        def get_users(db: Session = Depends(get_db)):
            return crud.get_users(db)
    """
    db = SessionLocal()
    try:
        yield db
    except SQLAlchemyError as e:
        logger.error(f"Database error in sync session: {e}")
        db.rollback()
        raise
    finally:
        db.close()


async def get_async_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Async dependency to get database session.
    
    Usage:
        @app.get("/users/")
        async def get_users(db: AsyncSession = Depends(get_async_db)):
            return await crud.get_users(db)
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
        except SQLAlchemyError as e:
            logger.error(f"Database error in async session: {e}")
            await session.rollback()
            raise


# Context managers for manual session handling
@contextmanager
def get_db_session() -> Generator[Session, None, None]:
    """
    Context manager for manual database session handling.
    
    Usage:
        with get_db_session() as db:
            user = crud.create_user(db, user_data)
    """
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception as e:
        logger.error(f"Session error: {e}")
        session.rollback()
        raise
    finally:
        session.close()


@asynccontextmanager
async def get_async_db_session() -> AsyncGenerator[AsyncSession, None]:
    """
    Async context manager for manual database session handling.
    
    Usage:
        async with get_async_db_session() as db:
            user = await crud.create_user(db, user_data)
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception as e:
            logger.error(f"Async session error: {e}")
            await session.rollback()
            raise


# Connection testing and health checks
class DatabaseHealthCheck:
    """Database health check utilities."""
    
    @staticmethod
    def check_connection() -> bool:
        """Check if database connection is healthy."""
        try:
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return False
    
    @staticmethod
    async def check_async_connection() -> bool:
        """Check if async database connection is healthy."""
        try:
            async with async_engine.begin() as conn:
                await conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            logger.error(f"Async database health check failed: {e}")
            return False
    
    @staticmethod
    def get_connection_info() -> dict:
        """Get detailed connection pool information."""
        pool = engine.pool
        return {
            "pool_size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid(),
        }


# Database initialization utilities
class DatabaseInitializer:
    """Database initialization and setup utilities."""
    
    @staticmethod
    def create_database_if_not_exists():
        """Create database if it doesn't exist."""
        try:
            # Import here to avoid circular imports
            from sqlalchemy import create_engine
            from sqlalchemy.sql import text
            
            # Connect to postgres database to create our database
            admin_url = str(settings.database.database_url).replace('/sqlalchemy_learning', '/postgres')
            admin_engine = create_engine(admin_url, isolation_level="AUTOCOMMIT")
            
            with admin_engine.connect() as conn:
                # Check if database exists
                result = conn.execute(text(
                    "SELECT 1 FROM pg_database WHERE datname='sqlalchemy_learning'"
                ))
                
                if not result.fetchone():
                    conn.execute(text("CREATE DATABASE sqlalchemy_learning"))
                    logger.info("Database 'sqlalchemy_learning' created successfully")
                else:
                    logger.info("Database 'sqlalchemy_learning' already exists")
                    
        except Exception as e:
            logger.error(f"Failed to create database: {e}")
            raise
    
    @staticmethod
    def setup_extensions():
        """Set up PostgreSQL extensions."""
        extensions = [
            "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"",
            "CREATE EXTENSION IF NOT EXISTS \"pg_trgm\"",  # For text search
            "CREATE EXTENSION IF NOT EXISTS \"btree_gin\"",  # For GIN indexes
            "CREATE EXTENSION IF NOT EXISTS \"pg_stat_statements\"",  # For query analysis
        ]
        
        try:
            with engine.connect() as conn:
                for extension in extensions:
                    conn.execute(text(extension))
                    conn.commit()
            logger.info("PostgreSQL extensions set up successfully")
        except Exception as e:
            logger.error(f"Failed to set up extensions: {e}")
            raise
    
    @staticmethod
    def initialize_full_database():
        """Complete database initialization."""
        DatabaseInitializer.create_database_if_not_exists()
        DatabaseInitializer.setup_extensions()
        logger.info("Database initialization completed")


# Export commonly used items
__all__ = [
    'engine',
    'async_engine',
    'SessionLocal',
    'AsyncSessionLocal',
    'get_db',
    'get_async_db',
    'get_db_session',
    'get_async_db_session',
    'DatabaseHealthCheck',
    'DatabaseInitializer',
]

# Initialize database on module import in development
if settings.is_development:
    try:
        DatabaseInitializer.initialize_full_database()
    except Exception as e:
        logger.warning(f"Database auto-initialization failed: {e}")
```

**app/models/base.py** - Enhanced Base Model:
```python
"""
Enhanced base model with common functionality for all SQLAlchemy models.
This provides audit fields, soft delete, and common utility methods.
"""

import uuid
from datetime import datetime
from typing import Any, Dict, List

from sqlalchemy import Column, Integer, DateTime, Boolean, String, text
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base, declared_attr
from sqlalchemy.orm import Session
from sqlalchemy.sql import func


class BaseModel:
    """
    Enhanced base model with common functionality.
    Provides audit fields, soft delete, and utility methods.
    """
    
    # Primary key with auto-increment
    id = Column(Integer, primary_key=True, index=True)
    
    # UUID for external references (more secure than sequential IDs)
    uuid = Column(
        UUID(as_uuid=True),
        default=uuid.uuid4,
        unique=True,
        index=True,
        nullable=False
    )
    
    # Audit fields
    created_at = Column(
        DateTime(timezone=True),
        server_default=func.now(),
        nullable=False,
        index=True
    )
    updated_at = Column(
        DateTime(timezone=True),
        server_default=func.now(),
        onupdate=func.now(),
        nullable=False
    )
    
    # Soft delete functionality
    is_deleted = Column(Boolean, default=False, index=True)
    deleted_at = Column(DateTime(timezone=True), nullable=True)
    
    # Audit trail
    created_by_id = Column(Integer, nullable=True)  # Foreign key to users table
    updated_by_id = Column(Integer, nullable=True)  # Foreign key to users table
    
    # Version control (optimistic locking)
    version = Column(Integer, default=1, nullable=False)
    
    @declared_attr
    def __tablename__(cls) -> str:
        """Auto-generate table name from class name."""
        return cls.__name__.lower() + 's'
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert model instance to dictionary."""
        return {
            column.key: getattr(self, column.key)
            for column in self.__table__.columns
        }
    
    def update_from_dict(self, data: Dict[str, Any]) -> None:
        """Update model instance from dictionary."""
        for key, value in data.items():
            if hasattr(self, key) and key not in ['id', 'uuid', 'created_at']:
                setattr(self, key, value)
    
    def soft_delete(self, user_id: int = None) -> None:
        """Perform soft delete."""
        self.is_deleted = True
        self.deleted_at = datetime.utcnow()
        if user_id:
            self.updated_by_id = user_id
    
    def restore(self, user_id: int = None) -> None:
        """Restore soft-deleted record."""
        self.is_deleted = False
        self.deleted_at = None
        if user_id:
            self.updated_by_id = user_id
    
    @classmethod
    def get_active_query(cls, session: Session):
        """Get query for non-deleted records."""
        return session.query(cls).filter(cls.is_deleted == False)
    
    @classmethod
    def get_deleted_query(cls, session: Session):
        """Get query for deleted records."""
        return session.query(cls).filter(cls.is_deleted == True)
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return f"<{self.__class__.__name__}(id={self.id}, uuid={self.uuid})>"


# Create the declarative base
Base = declarative_base(cls=BaseModel)


# Database utilities
class DatabaseUtils:
    """Utility functions for database operations."""
    
    @staticmethod
    def truncate_all_tables(session: Session) -> None:
        """Truncate all tables (useful for testing)."""
        for table in reversed(Base.metadata.sorted_tables):
            session.execute(text(f'TRUNCATE TABLE {table.name} RESTART IDENTITY CASCADE'))
        session.commit()
    
    @staticmethod
    def get_table_info(session: Session, table_name: str) -> Dict[str, Any]:
        """Get detailed information about a table."""
        queries = {
            'row_count': f"SELECT COUNT(*) FROM {table_name}",
            'table_size': f"""
                SELECT pg_size_pretty(pg_total_relation_size('{table_name}')) as size,
                       pg_size_pretty(pg_relation_size('{table_name}')) as table_size,
                       pg_size_pretty(pg_indexes_size('{table_name}')) as index_size
            """,
            'column_info': f"""
                SELECT column_name, data_type, is_nullable, column_default
                FROM information_schema.columns 
                WHERE table_name = '{table_name}'
                ORDER BY ordinal_position
            """
        }
        
        results = {}
        for key, query in queries.items():
            try:
                result = session.execute(text(query))
                if key == 'row_count':
                    results[key] = result.scalar()
                else:
                    results[key] = [dict(row._mapping) for row in result]
            except Exception as e:
                logger.error(f"Failed to get {key} for {table_name}: {e}")
                results[key] = None
        
        return results
    
    @staticmethod
    def analyze_query_performance(session: Session, query: str) -> Dict[str, Any]:
        """Analyze query performance using EXPLAIN ANALYZE."""
        try:
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}"
            result = session.execute(text(explain_query))
            return result.fetchone()[0][0]  # First element of JSON result
        except Exception as e:
            logger.error(f"Failed to analyze query: {e}")
            return {}


# Advanced session management
class SessionManager:
    """Advanced session management with retry logic and monitoring."""
    
    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
    
    @contextmanager
    def get_session_with_retry(self) -> Generator[Session, None, None]:
        """Get database session with automatic retry on failure."""
        for attempt in range(self.max_retries):
            session = SessionLocal()
            try:
                yield session
                session.commit()
                break
            except SQLAlchemyError as e:
                session.rollback()
                logger.warning(f"Database operation failed (attempt {attempt + 1}): {e}")
                
                if attempt == self.max_retries - 1:
                    logger.error("Max retries exceeded, giving up")
                    raise
                
                time.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff
            finally:
                session.close()
    
    @asynccontextmanager
    async def get_async_session_with_retry(self) -> AsyncGenerator[AsyncSession, None]:
        """Get async database session with automatic retry on failure."""
        for attempt in range(self.max_retries):
            async with AsyncSessionLocal() as session:
                try:
                    yield session
                    await session.commit()
                    break
                except SQLAlchemyError as e:
                    await session.rollback()
                    logger.warning(f"Async database operation failed (attempt {attempt + 1}): {e}")
                    
                    if attempt == self.max_retries - 1:
                        logger.error("Max retries exceeded for async operation")
                        raise
                    
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))


# Connection pool monitoring
class ConnectionPoolMonitor:
    """Monitor database connection pool health."""
    
    @staticmethod
    def get_pool_status() -> Dict[str, Any]:
        """Get current connection pool status."""
        pool = engine.pool
        return {
            "size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid(),
            "total_connections": pool.size() + pool.overflow(),
            "utilization": (pool.checkedout() / (pool.size() + pool.overflow())) * 100
        }
    
    @staticmethod
    def log_pool_status():
        """Log current pool status."""
        status = ConnectionPoolMonitor.get_pool_status()
        logger.info(f"Connection pool status: {status}")
        
        # Alert if utilization is high
        if status['utilization'] > 80:
            logger.warning(f"High connection pool utilization: {status['utilization']:.1f}%")


# Initialize session manager
session_manager = SessionManager()

# Health check endpoints data
def get_database_health() -> Dict[str, Any]:
    """Get comprehensive database health information."""
    health_check = DatabaseHealthCheck()
    pool_monitor = ConnectionPoolMonitor()
    
    return {
        "database_connected": health_check.check_connection(),
        "pool_status": pool_monitor.get_pool_status(),
        "engine_info": {
            "driver": engine.driver,
            "database": engine.url.database,
            "host": engine.url.host,
            "port": engine.url.port,
        }
    }


# Export for easy imports
__all__ = [
    'Base',
    'engine',
    'async_engine',
    'SessionLocal',
    'AsyncSessionLocal',
    'get_db',
    'get_async_db',
    'get_db_session',
    'get_async_db_session',
    'DatabaseUtils',
    'SessionManager',
    'ConnectionPoolMonitor',
    'DatabaseHealthCheck',
    'get_database_health',
    'session_manager'
]
```

---

## Part 2: Project 1 - Complete Blog API with Authentication

This project demonstrates a complete, production-ready blog API with user authentication, covering all fundamental SQLAlchemy concepts through a real-world application.

### 2.1 Complete User Authentication Models

**app/models/user.py** - User Management with Security:
```python
"""
Complete user management models with authentication, roles, and security features.
Demonstrates: Enums, relationships, hybrid properties, validation, and security.
"""

import enum
from datetime import datetime, timedelta
from typing import Optional, List

from sqlalchemy import Column, Integer, String, Boolean, DateTime, Text, Enum as SQLEnum
from sqlalchemy import ForeignKey, Table, UniqueConstraint, CheckConstraint
from sqlalchemy.orm import relationship, validates
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.dialects.postgresql import INET
from passlib.context import CryptContext

from app.models.base import Base

# Password encryption context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


class UserRole(enum.Enum):
    """User role enumeration."""
    ADMIN = "admin"
    MODERATOR = "moderator"
    AUTHOR = "author"
    READER = "reader"


class UserStatus(enum.Enum):
    """User status enumeration."""
    ACTIVE = "active"
    INACTIVE = "inactive"
    SUSPENDED = "suspended"
    PENDING_VERIFICATION = "pending_verification"


# Many-to-many table for user followers
user_followers = Table(
    'user_followers',
    Base.metadata,
    Column('follower_id', Integer, ForeignKey('users.id'), primary_key=True),
    Column('followed_id', Integer, ForeignKey('users.id'), primary_key=True),
    Column('created_at', DateTime, default=datetime.utcnow),
    UniqueConstraint('follower_id', 'followed_id', name='unique_follow_relationship')
)


class User(Base):
    """
    Complete user model with authentication, profile, and social features.
    
    Demonstrates:
    - Enum usage
    - Password hashing
    - Validation methods
    - Hybrid properties
    - Self-referential relationships
    - Audit trails
    """
    __tablename__ = "users"
    
    # Basic user information
    username = Column(String(50), unique=True, index=True, nullable=False)
    email = Column(String(100), unique=True, index=True, nullable=False)
    full_name = Column(String(100), nullable=False)
    
    # Authentication
    hashed_password = Column(String(255), nullable=False)
    is_active = Column(Boolean, default=True)
    is_email_verified = Column(Boolean, default=False)
    email_verification_token = Column(String(255), nullable=True)
    
    # Role and permissions
    role = Column(SQLEnum(UserRole), default=UserRole.READER, nullable=False)
    status = Column(SQLEnum(UserStatus), default=UserStatus.PENDING_VERIFICATION)
    
    # Profile information
    bio = Column(Text, nullable=True)
    avatar_url = Column(String(500), nullable=True)
    website_url = Column(String(500), nullable=True)
    location = Column(String(100), nullable=True)
    birth_date = Column(DateTime, nullable=True)
    
    # Activity tracking
    last_login_at = Column(DateTime, nullable=True)
    last_login_ip = Column(INET, nullable=True)
    login_count = Column(Integer, default=0)
    
    # Password reset
    reset_password_token = Column(String(255), nullable=True)
    reset_password_expires = Column(DateTime, nullable=True)
    
    # Account limits and preferences
    posts_count = Column(Integer, default=0)
    max_posts_per_day = Column(Integer, default=10)
    timezone = Column(String(50), default='UTC')
    language = Column(String(10), default='en')
    
    # Relationships
    posts = relationship("Post", back_populates="author", lazy="dynamic", cascade="all, delete-orphan")
    comments = relationship("Comment", back_populates="author", lazy="dynamic")
    
    # Self-referential many-to-many for followers
    following = relationship(
        "User",
        secondary=user_followers,
        primaryjoin=id == user_followers.c.follower_id,
        secondaryjoin=id == user_followers.c.followed_id,
        back_populates="followers",
        lazy="dynamic"
    )
    followers = relationship(
        "User",
        secondary=user_followers,
        primaryjoin=id == user_followers.c.followed_id,
        secondaryjoin=id == user_followers.c.follower_id,
        back_populates="following",
        lazy="dynamic"
    )
    
    # Login sessions for tracking active sessions
    login_sessions = relationship("UserSession", back_populates="user", cascade="all, delete-orphan")
    
    # Constraints
    __table_args__ = (
        CheckConstraint('posts_count >= 0', name='positive_posts_count'),
        CheckConstraint('login_count >= 0', name='positive_login_count'),
        CheckConstraint('max_posts_per_day > 0', name='positive_max_posts'),
        CheckConstraint("email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'", name='valid_email_format'),
    )
    
    # Validation methods
    @validates('username')
    def validate_username(self, key, username):
        """Validate username format and length."""
        if not username:
            raise ValueError("Username cannot be empty")
        if len(username) < 3:
            raise ValueError("Username must be at least 3 characters long")
        if len(username) > 50:
            raise ValueError("Username cannot be longer than 50 characters")
        if not username.replace('_', '').replace('-', '').isalnum():
            raise ValueError("Username can only contain letters, numbers, underscore, and hyphen")
        return username.lower()
    
    @validates('email')
    def validate_email(self, key, email):
        """Validate email format."""
        import re
        email_pattern = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'
        if not re.match(email_pattern, email):
            raise ValueError("Invalid email format")
        return email.lower()
    
    @validates('role')
    def validate_role_change(self, key, role):
        """Log role changes for audit purposes."""
        if hasattr(self, 'role') and self.role != role:
            # In a real app, you'd log this to an audit table
            pass
        return role
    
    # Hybrid properties
    @hybrid_property
    def is_admin(self):
        """Check if user is admin."""
        return self.role == UserRole.ADMIN
    
    @hybrid_property
    def is_author_or_above(self):
        """Check if user can create content."""
        return self.role in [UserRole.ADMIN, UserRole.MODERATOR, UserRole.AUTHOR]
    
    @hybrid_property
    def followers_count(self):
        """Get follower count (for individual instances)."""
        return self.followers.count() if self.followers else 0
    
    @hybrid_property
    def following_count(self):
        """Get following count (for individual instances)."""
        return self.following.count() if self.following else 0
    
    # Authentication methods
    def set_password(self, password: str) -> None:
        """Hash and set password."""
        if len(password) < 8:
            raise ValueError("Password must be at least 8 characters long")
        self.hashed_password = pwd_context.hash(password)
    
    def verify_password(self, password: str) -> bool:
        """Verify password against hash."""
        return pwd_context.verify(password, self.hashed_password)
    
    def can_post_today(self) -> bool:
        """Check if user can post today based on daily limits."""
        from sqlalchemy import func, and_
        from app.models.blog import Post
        
        today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
        today_posts = self.posts.filter(
            and_(Post.created_at >= today_start, Post.is_deleted == False)
        ).count()
        
        return today_posts < self.max_posts_per_day
    
    def follow_user(self, user_to_follow: 'User') -> bool:
        """Follow another user."""
        if user_to_follow.id == self.id:
            raise ValueError("Cannot follow yourself")
        
        if not self.is_following(user_to_follow):
            self.following.append(user_to_follow)
            return True
        return False
    
    def unfollow_user(self, user_to_unfollow: 'User') -> bool:
        """Unfollow a user."""
        if self.is_following(user_to_unfollow):
            self.following.remove(user_to_unfollow)
            return True
        return False
    
    def is_following(self, user: 'User') -> bool:
        """Check if following a specific user."""
        return self.following.filter(user_followers.c.followed_id == user.id).count() > 0
    
    def record_login(self, ip_address: str = None) -> None:
        """Record user login activity."""
        self.last_login_at = datetime.utcnow()
        self.login_count += 1
        if ip_address:
            self.last_login_ip = ip_address


class UserSession(Base):
    """
    User session tracking for security and analytics.
    
    Demonstrates:
    - Session management
    - Security tracking
    - Foreign key relationships
    """
    __tablename__ = "user_sessions"
    
    # Session information
    session_token = Column(String(255), unique=True, index=True, nullable=False)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Session metadata
    ip_address = Column(INET, nullable=True)
    user_agent = Column(Text, nullable=True)
    device_info = Column(Text, nullable=True)
    
    # Session lifecycle
    is_active = Column(Boolean, default=True)
    expires_at = Column(DateTime, nullable=False)
    last_activity_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    user = relationship("User", back_populates="login_sessions")
    
    # Constraints
    __table_args__ = (
        CheckConstraint('expires_at > created_at', name='valid_expiry_date'),
    )
    
    @classmethod
    def create_session(cls, user_id: int, expires_in_hours: int = 24, **kwargs):
        """Create a new user session."""
        import secrets
        
        return cls(
            session_token=secrets.token_urlsafe(32),
            user_id=user_id,
            expires_at=datetime.utcnow() + timedelta(hours=expires_in_hours),
            **kwargs
        )
    
    def is_expired(self) -> bool:
        """Check if session is expired."""
        return datetime.utcnow() > self.expires_at
    
    def extend_session(self, hours: int = 24) -> None:
        """Extend session expiry time."""
        self.expires_at = datetime.utcnow() + timedelta(hours=hours)
        self.last_activity_at = datetime.utcnow()


class UserPermission(Base):
    """
    User permissions for fine-grained access control.
    
    Demonstrates:
    - Permission management
    - Many-to-many relationships with additional data
    """
    __tablename__ = "user_permissions"
    
    user_id = Column(Integer, ForeignKey('users.id'), primary_key=True)
    permission_name = Column(String(100), primary_key=True)
    granted_at = Column(DateTime, default=datetime.utcnow)
    granted_by_id = Column(Integer, ForeignKey('users.id'))
    expires_at = Column(DateTime, nullable=True)
    
    # Relationships
    user = relationship("User", foreign_keys=[user_id])
    granted_by = relationship("User", foreign_keys=[granted_by_id])
    
    def is_valid(self) -> bool:
        """Check if permission is still valid."""
        if self.expires_at:
            return datetime.utcnow() <= self.expires_at
        return True
```

### 2.2 Complete Blog System Models

**app/models/blog.py** - Blog System with Rich Relationships:
```python
"""
Complete blog system models demonstrating all SQLAlchemy relationship types.
Includes posts, comments, categories, tags, and advanced features.
"""

import enum
from datetime import datetime
from typing import List, Optional

from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, ForeignKey
from sqlalchemy import Numeric, JSON, Index, CheckConstraint, UniqueConstraint
from sqlalchemy import Table, func, select
from sqlalchemy.orm import relationship, validates, column_property
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.dialects.postgresql import TSVECTOR, UUID

from app.models.base import Base


class PostStatus(enum.Enum):
    """Post status enumeration."""
    DRAFT = "draft"
    PUBLISHED = "published"
    ARCHIVED = "archived"
    SCHEDULED = "scheduled"


class CommentStatus(enum.Enum):
    """Comment status enumeration."""
    APPROVED = "approved"
    PENDING = "pending"
    REJECTED = "rejected"
    SPAM = "spam"


# Many-to-many association table for post tags
post_tags = Table(
    'post_tags',
    Base.metadata,
    Column('post_id', Integer, ForeignKey('posts.id'), primary_key=True),
    Column('tag_id', Integer, ForeignKey('tags.id'), primary_key=True),
    Column('created_at', DateTime, default=datetime.utcnow),
    UniqueConstraint('post_id', 'tag_id', name='unique_post_tag')
)


class Category(Base):
    """
    Blog categories with hierarchical structure.
    
    Demonstrates:
    - Self-referential relationships
    - Hierarchical data
    - Computed properties
    """
    __tablename__ = "categories"
    
    name = Column(String(100), unique=True, nullable=False, index=True)
    slug = Column(String(100), unique=True, nullable=False, index=True)
    description = Column(Text, nullable=True)
    
    # Hierarchical structure
    parent_id = Column(Integer, ForeignKey('categories.id'), nullable=True)
    sort_order = Column(Integer, default=0)
    
    # SEO and metadata
    meta_title = Column(String(200), nullable=True)
    meta_description = Column(String(500), nullable=True)
    
    # Visibility and status
    is_visible = Column(Boolean, default=True)
    posts_count = Column(Integer, default=0)  # Denormalized counter
    
    # Relationships
    parent = relationship("Category", remote_side=[id], back_populates="children")
    children = relationship("Category", back_populates="parent", lazy="dynamic")
    posts = relationship("Post", back_populates="category", lazy="dynamic")
    
    # Constraints
    __table_args__ = (
        CheckConstraint('posts_count >= 0', name='positive_posts_count'),
        CheckConstraint('sort_order >= 0', name='positive_sort_order'),
        Index('idx_category_parent_sort', 'parent_id', 'sort_order'),
    )
    
    @validates('slug')
    def validate_slug(self, key, slug):
        """Validate slug format."""
        import re
        if not re.match(r'^[a-z0-9-]+$', slug):
            raise ValueError("Slug can only contain lowercase letters, numbers, and hyphens")
        return slug
    
    @hybrid_property
    def full_path(self):
        """Get full category path (e.g., 'Technology/Programming/Python')."""
        if self.parent:
            return f"{self.parent.full_path}/{self.name}"
        return self.name
    
    def get_all_children(self) -> List['Category']:
        """Get all descendant categories recursively."""
        children = []
        for child in self.children:
            children.append(child)
            children.extend(child.get_all_children())
        return children
    
    def increment_posts_count(self) -> None:
        """Increment the posts count (for denormalization)."""
        self.posts_count += 1
    
    def decrement_posts_count(self) -> None:
        """Decrement the posts count."""
        if self.posts_count > 0:
            self.posts_count -= 1


class Tag(Base):
    """
    Blog tags with usage statistics.
    
    Demonstrates:
    - Many-to-many relationships
    - Usage tracking
    - Color coding
    """
    __tablename__ = "tags"
    
    name = Column(String(50), unique=True, nullable=False, index=True)
    slug = Column(String(50), unique=True, nullable=False, index=True)
    description = Column(Text, nullable=True)
    
    # Visual customization
    color = Column(String(7), default='#007bff')  # Hex color code
    
    # Usage statistics
    usage_count = Column(Integer, default=0)
    
    # Relationships
    posts = relationship("Post", secondary=post_tags, back_populates="tags", lazy="dynamic")
    
    # Constraints
    __table_args__ = (
        CheckConstraint('usage_count >= 0', name='positive_usage_count'),
        CheckConstraint("color ~* '^#[0-9a-f]{6}$'", name='valid_hex_color'),
    )
    
    @validates('color')
    def validate_color(self, key, color):
        """Validate hex color format."""
        import re
        if not re.match(r'^#[0-9a-fA-F]{6}$', color):
            raise ValueError("Color must be a valid hex code (e.g., #007bff)")
        return color.lower()
    
    def increment_usage(self) -> None:
        """Increment usage count when tag is used."""
        self.usage_count += 1


class Post(Base):
    """
    Complete blog post model with rich features.
    
    Demonstrates:
    - All relationship types
    - JSON fields
    - Full-text search
    - Computed properties
    - Complex validation
    """
    __tablename__ = "posts"
    
    # Basic post information
    title = Column(String(200), nullable=False, index=True)
    slug = Column(String(200), unique=True, nullable=False, index=True)
    content = Column(Text, nullable=False)
    excerpt = Column(String(500), nullable=True)
    
    # Author and categorization
    author_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    category_id = Column(Integer, ForeignKey('categories.id'), nullable=True)
    
    # Publishing information
    status = Column(SQLEnum(PostStatus), default=PostStatus.DRAFT, nullable=False)
    published_at = Column(DateTime, nullable=True)
    scheduled_for = Column(DateTime, nullable=True)
    
    # SEO and metadata
    meta_title = Column(String(200), nullable=True)
    meta_description = Column(String(500), nullable=True)
    featured_image_url = Column(String(500), nullable=True)
    
    # Engagement metrics
    view_count = Column(Integer, default=0)
    like_count = Column(Integer, default=0)
    share_count = Column(Integer, default=0)
    comment_count = Column(Integer, default=0)  # Denormalized
    
    # Content metadata
    word_count = Column(Integer, default=0)
    read_time_minutes = Column(Integer, default=0)
    
    # Advanced features
    is_featured = Column(Boolean, default=False)
    is_pinned = Column(Boolean, default=False)
    allow_comments = Column(Boolean, default=True)
    
    # Full-text search (PostgreSQL specific)
    search_vector = Column(TSVECTOR, nullable=True)
    
    # JSON field for flexible metadata
    metadata = Column(JSON, nullable=True)
    
    # Relationships
    author = relationship("User", back_populates="posts")
    category = relationship("Category", back_populates="posts")
    tags = relationship("Tag", secondary=post_tags, back_populates="posts", lazy="dynamic")
    comments = relationship("Comment", back_populates="post", lazy="dynamic", cascade="all, delete-orphan")
    likes = relationship("PostLike", back_populates="post", cascade="all, delete-orphan")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_post_author_status', 'author_id', 'status'),
        Index('idx_post_published', 'published_at', postgresql_where=status == PostStatus.PUBLISHED),
        Index('idx_post_search', 'search_vector', postgresql_using='gin'),
        Index('idx_post_category_published', 'category_id', 'published_at'),
        CheckConstraint('view_count >= 0', name='positive_view_count'),
        CheckConstraint('like_count >= 0', name='positive_like_count'),
        CheckConstraint('word_count >= 0', name='positive_word_count'),
    )
    
    @validates('slug')
    def validate_slug(self, key, slug):
        """Validate and auto-generate slug."""
        import re
        if not slug:
            # Auto-generate from title if not provided
            slug = re.sub(r'[^a-zA-Z0-9\s-]', '', self.title.lower())
            slug = re.sub(r'\s+', '-', slug.strip())
        
        if not re.match(r'^[a-z0-9-]+$', slug):
            raise ValueError("Slug can only contain lowercase letters, numbers, and hyphens")
        return slug
    
    @validates('content')
    def update_word_count(self, key, content):
        """Auto-update word count and read time."""
        if content:
            words = len(content.split())
            self.word_count = words
            self.read_time_minutes = max(1, words // 200)  # Assume 200 words per minute
        return content
    
    # Hybrid properties
    @hybrid_property
    def is_published(self):
        """Check if post is published."""
        return self.status == PostStatus.PUBLISHED
    
    @hybrid_property
    def engagement_score(self):
        """Calculate engagement score."""
        return (self.like_count * 3) + (self.comment_count * 5) + (self.share_count * 10)
    
    # Business logic methods
    def publish(self) -> None:
        """Publish the post."""
        if self.status != PostStatus.PUBLISHED:
            self.status = PostStatus.PUBLISHED
            self.published_at = datetime.utcnow()
            
            # Increment category post count
            if self.category:
                self.category.increment_posts_count()
    
    def unpublish(self) -> None:
        """Unpublish the post."""
        if self.status == PostStatus.PUBLISHED:
            self.status = PostStatus.DRAFT
            self.published_at = None
            
            # Decrement category post count
            if self.category:
                self.category.decrement_posts_count()
    
    def can_be_edited_by(self, user: 'User') -> bool:
        """Check if user can edit this post."""
        return (
            user.id == self.author_id or 
            user.is_admin or 
            user.role == UserRole.MODERATOR
        )
    
    def update_search_vector(self) -> None:
        """Update full-text search vector (called by trigger in production)."""
        # This would typically be handled by a PostgreSQL trigger
        pass


class Comment(Base):
    """
    Comment system with threading and moderation.
    
    Demonstrates:
    - Self-referential relationships (threaded comments)
    - Moderation workflow
    - Polymorphic behavior
    """
    __tablename__ = "comments"
    
    # Comment content
    content = Column(Text, nullable=False)
    
    # Relationships
    author_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    post_id = Column(Integer, ForeignKey('posts.id'), nullable=False)
    
    # Threading (self-referential)
    parent_id = Column(Integer, ForeignKey('comments.id'), nullable=True)
    
    # Moderation
    status = Column(SQLEnum(CommentStatus), default=CommentStatus.PENDING)
    moderated_by_id = Column(Integer, ForeignKey('users.id'), nullable=True)
    moderated_at = Column(DateTime, nullable=True)
    moderation_reason = Column(String(500), nullable=True)
    
    # Engagement
    like_count = Column(Integer, default=0)
    
    # Spam detection
    is_spam = Column(Boolean, default=False)
    spam_score = Column(Numeric(3, 2), default=0.0)  # 0.00 to 1.00
    
    # Relationships
    author = relationship("User", foreign_keys=[author_id], back_populates="comments")
    post = relationship("Post", back_populates="comments")
    moderator = relationship("User", foreign_keys=[moderated_by_id])
    
    # Self-referential for threading
    parent = relationship("Comment", remote_side=[id], back_populates="replies")
    replies = relationship("Comment", back_populates="parent", lazy="dynamic")
    
    # Constraints
    __table_args__ = (
        CheckConstraint('like_count >= 0', name='positive_like_count'),
        CheckConstraint('spam_score >= 0 AND spam_score <= 1', name='valid_spam_score'),
        Index('idx_comment_post_status', 'post_id', 'status'),
        Index('idx_comment_author', 'author_id'),
        Index('idx_comment_parent', 'parent_id'),
    )
    
    @validates('content')
    def validate_content(self, key, content):
        """Validate comment content."""
        if not content or len(content.strip()) < 1:
            raise ValueError("Comment content cannot be empty")
        if len(content) > 5000:
            raise ValueError("Comment cannot exceed 5000 characters")
        return content
    
    @hybrid_property
    def is_approved(self):
        """Check if comment is approved."""
        return self.status == CommentStatus.APPROVED
    
    @hybrid_property
    def depth_level(self):
        """Calculate comment thread depth."""
        level = 0
        parent = self.parent
        while parent:
            level += 1
            parent = parent.parent
        return level
    
    def approve(self, moderator_id: int, reason: str = None) -> None:
        """Approve comment."""
        self.status = CommentStatus.APPROVED
        self.moderated_by_id = moderator_id
        self.moderated_at = datetime.utcnow()
        self.moderation_reason = reason
    
    def reject(self, moderator_id: int, reason: str) -> None:
        """Reject comment."""
        self.status = CommentStatus.REJECTED
        self.moderated_by_id = moderator_id
        self.moderated_at = datetime.utcnow()
        self.moderation_reason = reason
    
    def mark_as_spam(self, moderator_id: int = None) -> None:
        """Mark comment as spam."""
        self.status = CommentStatus.SPAM
        self.is_spam = True
        if moderator_id:
            self.moderated_by_id = moderator_id
            self.moderated_at = datetime.utcnow()


class PostLike(Base):
    """
    Post likes with user tracking.
    
    Demonstrates:
    - Composite primary keys
    - Unique constraints
    - Audit trails
    """
    __tablename__ = "post_likes"
    
    user_id = Column(Integer, ForeignKey('users.id'), primary_key=True)
    post_id = Column(Integer, ForeignKey('posts.id'), primary_key=True)
    
    # Relationships
    user = relationship("User")
    post = relationship("Post", back_populates="likes")
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('user_id', 'post_id', name='unique_user_post_like'),
        Index('idx_like_post', 'post_id'),
        Index('idx_like_user', 'user_id'),
    )


class PostView(Base):
    """
    Track post views for analytics.
    
    Demonstrates:
    - Analytics tracking
    - IP address storage
    - Time-based analysis
    """
    __tablename__ = "post_views"
    
    post_id = Column(Integer, ForeignKey('posts.id'), nullable=False)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=True)  # Null for anonymous
    
    # View metadata
    ip_address = Column(String(45), nullable=True)  # IPv6 support
    user_agent = Column(Text, nullable=True)
    referrer = Column(String(500), nullable=True)
    
    # Geographic data (simplified)
    country = Column(String(2), nullable=True)  # ISO country code
    city = Column(String(100), nullable=True)
    
    # View duration tracking
    duration_seconds = Column(Integer, nullable=True)
    
    # Relationships
    post = relationship("Post")
    user = relationship("User")
    
    # Indexes for analytics queries
    __table_args__ = (
        Index('idx_view_post_date', 'post_id', 'created_at'),
        Index('idx_view_user_date', 'user_id', 'created_at'),
        Index('idx_view_ip_date', 'ip_address', 'created_at'),
    )


class PostRevision(Base):
    """
    Track post revision history.
    
    Demonstrates:
    - Audit trails
    - Version control
    - Content history
    """
    __tablename__ = "post_revisions"
    
    post_id = Column(Integer, ForeignKey('posts.id'), nullable=False)
    revision_number = Column(Integer, nullable=False)
    
    # Revision content
    title = Column(String(200), nullable=False)
    content = Column(Text, nullable=False)
    excerpt = Column(String(500), nullable=True)
    
    # Revision metadata
    change_summary = Column(String(500), nullable=True)
    word_count = Column(Integer, default=0)
    
    # Relationships
    post = relationship("Post")
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('post_id', 'revision_number', name='unique_post_revision'),
        Index('idx_revision_post_number', 'post_id', 'revision_number'),
    )


# Computed columns using column_property
Post.published_comments_count = column_property(
    select(func.count(Comment.id))
    .where(Comment.post_id == Post.id)
    .where(Comment.status == CommentStatus.APPROVED)
    .correlate_except(Comment)
    .scalar_subquery()
)

User.published_posts_count = column_property(
    select(func.count(Post.id))
    .where(Post.author_id == User.id)
    .where(Post.status == PostStatus.PUBLISHED)
    .correlate_except(Post)
    .scalar_subquery()
)
```

### 2.3 Authentication and Security System

**app/core/security.py** - Complete Authentication:
```python
"""
Complete authentication and security system.
Includes JWT tokens, password hashing, permissions, and security utilities.
"""

import secrets
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List

import jwt
from passlib.context import CryptContext
from sqlalchemy.orm import Session

from app.config import get_settings
from app.models.user import User, UserRole, UserStatus, UserSession

settings = get_settings()

# Password encryption context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


class SecurityManager:
    """Complete security management system."""
    
    @staticmethod
    def hash_password(password: str) -> str:
        """Hash password using bcrypt."""
        return pwd_context.hash(password)
    
    @staticmethod
    def verify_password(plain_password: str, hashed_password: str) -> bool:
        """Verify password against hash."""
        return pwd_context.verify(plain_password, hashed_password)
    
    @staticmethod
    def generate_password_reset_token() -> str:
        """Generate secure password reset token."""
        return secrets.token_urlsafe(32)
    
    @staticmethod
    def generate_email_verification_token() -> str:
        """Generate email verification token."""
        return secrets.token_urlsafe(32)


class JWTManager:
    """JWT token management."""
    
    @staticmethod
    def create_access_token(data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
        """Create JWT access token."""
        to_encode = data.copy()
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=settings.security.access_token_expire_minutes)
        
        to_encode.update({
            "exp": expire,
            "iat": datetime.utcnow(),
            "type": "access"
        })
        
        return jwt.encode(to_encode, settings.security.secret_key, algorithm=settings.security.algorithm)
    
    @staticmethod
    def create_refresh_token(data: Dict[str, Any]) -> str:
        """Create JWT refresh token."""
        to_encode = data.copy()
        expire = datetime.utcnow() + timedelta(days=settings.security.refresh_token_expire_days)
        
        to_encode.update({
            "exp": expire,
            "iat": datetime.utcnow(),
            "type": "refresh"
        })
        
        return jwt.encode(to_encode, settings.security.secret_key, algorithm=settings.security.algorithm)
    
    @staticmethod
    def verify_token(token: str, token_type: str = "access") -> Optional[Dict[str, Any]]:
        """Verify and decode JWT token."""
        try:
            payload = jwt.decode(token, settings.security.secret_key, algorithms=[settings.security.algorithm])
            
            # Verify token type
            if payload.get("type") != token_type:
                return None
            
            return payload
        except jwt.ExpiredSignatureError:
            return None
        except jwt.JWTError:
            return None
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[str]:
        """Create new access token from refresh token."""
        payload = JWTManager.verify_token(refresh_token, "refresh")
        if not payload:
            return None
        
        # Create new access token
        user_data = {
            "sub": payload["sub"],
            "user_id": payload["user_id"],
            "role": payload["role"]
        }
        
        return JWTManager.create_access_token(user_data)


class AuthenticationService:
    """Complete authentication service."""
    
    def __init__(self, db: Session):
        self.db = db
    
    def authenticate_user(self, username_or_email: str, password: str, ip_address: str = None) -> Optional[User]:
        """Authenticate user by username or email."""
        # Find user by username or email
        user = self.db.query(User).filter(
            (User.username == username_or_email) | 
            (User.email == username_or_email)
        ).first()
        
        if not user:
            return None
        
        # Check password
        if not user.verify_password(password):
            return None
        
        # Check if user is active
        if not user.is_active or user.status == UserStatus.SUSPENDED:
            return None
        
        # Record login
        user.record_login(ip_address)
        self.db.commit()
        
        return user
    
    def create_user_tokens(self, user: User) -> Dict[str, str]:
        """Create access and refresh tokens for user."""
        token_data = {
            "sub": user.username,
            "user_id": user.id,
            "role": user.role.value,
            "status": user.status.value
        }
        
        access_token = JWTManager.create_access_token(token_data)
        refresh_token = JWTManager.create_refresh_token(token_data)
        
        return {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "token_type": "bearer"
        }
    
    def register_user(self, username: str, email: str, full_name: str, password: str) -> User:
        """Register new user."""
        # Check if user already exists
        existing_user = self.db.query(User).filter(
            (User.username == username) | (User.email == email)
        ).first()
        
        if existing_user:
            raise ValueError("Username or email already exists")
        
        # Create new user
        user = User(
            username=username,
            email=email,
            full_name=full_name,
            email_verification_token=SecurityManager.generate_email_verification_token()
        )
        user.set_password(password)
        
        self.db.add(user)
        self.db.commit()
        self.db.refresh(user)
        
        return user
    
    def verify_email(self, token: str) -> bool:
        """Verify user email with token."""
        user = self.db.query(User).filter(
            User.email_verification_token == token
        ).first()
        
        if user:
            user.is_email_verified = True
            user.email_verification_token = None
            user.status = UserStatus.ACTIVE
            self.db.commit()
            return True
        return False
    
    def initiate_password_reset(self, email: str) -> Optional[str]:
        """Initiate password reset process."""
        user = self.db.query(User).filter(User.email == email).first()
        if not user:
            return None
        
        token = SecurityManager.generate_password_reset_token()
        user.reset_password_token = token
        user.reset_password_expires = datetime.utcnow() + timedelta(hours=1)
        
        self.db.commit()
        return token
    
    def reset_password(self, token: str, new_password: str) -> bool:
        """Reset password using token."""
        user = self.db.query(User).filter(
            User.reset_password_token == token,
            User.reset_password_expires > datetime.utcnow()
        ).first()
        
        if user:
            user.set_password(new_password)
            user.reset_password_token = None
            user.reset_password_expires = None
            self.db.commit()
            return True
        return False


class PermissionManager:
    """Permission management system."""
    
    # Define application permissions
    PERMISSIONS = {
        'posts.create': 'Create posts',
        'posts.edit': 'Edit posts',
        'posts.delete': 'Delete posts',
        'posts.publish': 'Publish posts',
        'posts.moderate': 'Moderate posts',
        
        'comments.create': 'Create comments',
        'comments.edit': 'Edit comments',
        'comments.delete': 'Delete comments',
        'comments.moderate': 'Moderate comments',
        
        'users.view': 'View users',
        'users.edit': 'Edit users',
        'users.delete': 'Delete users',
        'users.manage_roles': 'Manage user roles',
        
        'categories.create': 'Create categories',
        'categories.edit': 'Edit categories',
        'categories.delete': 'Delete categories',
        
        'tags.create': 'Create tags',
        'tags.edit': 'Edit tags',
        'tags.delete': 'Delete tags',
        
        'admin.access': 'Access admin panel',
        'admin.system': 'System administration',
    }
    
    # Role-based permissions
    ROLE_PERMISSIONS = {
        UserRole.ADMIN: list(PERMISSIONS.keys()),  # Admin has all permissions
        UserRole.MODERATOR: [
            'posts.create', 'posts.edit', 'posts.delete', 'posts.publish', 'posts.moderate',
            'comments.create', 'comments.edit', 'comments.delete', 'comments.moderate',
            'users.view', 'categories.create', 'categories.edit', 'tags.create', 'tags.edit'
        ],
        UserRole.AUTHOR: [
            'posts.create', 'posts.edit', 'posts.publish',
            'comments.create', 'comments.edit',
            'categories.create', 'tags.create'
        ],
        UserRole.READER: [
            'comments.create', 'comments.edit'
        ]
    }
    
    @classmethod
    def user_has_permission(cls, user: User, permission: str) -> bool:
        """Check if user has specific permission."""
        if not user.is_active:
            return False
        
        return permission in cls.ROLE_PERMISSIONS.get(user.role, [])
    
    @classmethod
    def get_user_permissions(cls, user: User) -> List[str]:
        """Get all permissions for user."""
        return cls.ROLE_PERMISSIONS.get(user.role, [])
    
    @classmethod
    def require_permission(cls, permission: str):
        """Decorator to require specific permission."""
        def decorator(func):
            def wrapper(*args, **kwargs):
                # This would be implemented with FastAPI dependencies
                # For now, it's a placeholder
                return func(*args, **kwargs)
            return wrapper
        return decorator
```

This is Part 2 focusing on the complete authentication and blog system. I've implemented:

**✅ User Authentication System:**
- Complete user model with roles, status, and security features
- Session tracking and management
- Permission system with role-based access control
- JWT token management (access + refresh tokens)
- Password reset and email verification flows

**✅ Blog System Models:**
- Posts with full metadata, SEO, and engagement tracking
- Categories with hierarchical structure
- Tags with usage statistics
- Comments with threading and moderation
- Post likes and view tracking
- Revision history system

**✅ Advanced SQLAlchemy Features Demonstrated:**
- Enums and constraints
- Self-referential relationships
- Many-to-many with association tables
- Hybrid properties and computed columns
- Validation methods
- JSON fields and full-text search
- Composite primary keys

---

## Part 3: Complete CRUD Operations and API Endpoints

This section demonstrates advanced CRUD operations with complex querying, pagination, filtering, and full FastAPI implementation with authentication.

### 3.1 Pydantic Schemas for Request/Response

**app/schemas/common.py** - Common Schemas and Utilities:
```python
"""
Common Pydantic schemas and utilities for API request/response validation.
Provides base classes, pagination, and common response patterns.
"""

from datetime import datetime
from typing import Optional, List, Generic, TypeVar, Dict, Any
from uuid import UUID

from pydantic import BaseModel, Field, ConfigDict
from pydantic.generics import GenericModel


# Generic type for paginated responses
T = TypeVar('T')


class BaseSchema(BaseModel):
    """Base schema with common configuration."""
    
    model_config = ConfigDict(
        from_attributes=True,
        use_enum_values=True,
        arbitrary_types_allowed=True,
        str_strip_whitespace=True
    )


class TimestampMixin(BaseSchema):
    """Mixin for models with timestamps."""
    created_at: datetime
    updated_at: datetime


class UUIDMixin(BaseSchema):
    """Mixin for models with UUID."""
    uuid: UUID


class PaginationParams(BaseModel):
    """Pagination parameters for API requests."""
    page: int = Field(default=1, ge=1, description="Page number (starts from 1)")
    size: int = Field(default=20, ge=1, le=100, description="Items per page")
    
    @property
    def offset(self) -> int:
        """Calculate offset for database queries."""
        return (self.page - 1) * self.size


class SortParams(BaseModel):
    """Sorting parameters for API requests."""
    sort_by: Optional[str] = Field(default=None, description="Field to sort by")
    sort_order: str = Field(default="asc", regex="^(asc|desc)$", description="Sort order")


class FilterParams(BaseModel):
    """Base filtering parameters."""
    search: Optional[str] = Field(default=None, description="Search query")
    created_after: Optional[datetime] = Field(default=None, description="Filter by created date")
    created_before: Optional[datetime] = Field(default=None, description="Filter by created date")


class PaginatedResponse(GenericModel, Generic[T]):
    """Generic paginated response."""
    items: List[T]
    total: int
    page: int
    size: int
    pages: int
    
    @classmethod
    def create(cls, items: List[T], total: int, page: int, size: int):
        """Create paginated response with calculated pages."""
        pages = (total + size - 1) // size  # Ceiling division
        return cls(
            items=items,
            total=total,
            page=page,
            size=size,
            pages=pages
        )


class APIResponse(BaseModel, Generic[T]):
    """Standard API response wrapper."""
    success: bool = True
    message: Optional[str] = None
    data: Optional[T] = None
    errors: Optional[List[str]] = None
    
    @classmethod
    def success_response(cls, data: T, message: str = "Success"):
        """Create success response."""
        return cls(success=True, message=message, data=data)
    
    @classmethod
    def error_response(cls, message: str, errors: Optional[List[str]] = None):
        """Create error response."""
        return cls(success=False, message=message, errors=errors or [])


class HealthCheckResponse(BaseSchema):
    """Health check response schema."""
    status: str
    timestamp: datetime
    version: str
    database: Dict[str, Any]
    uptime_seconds: float
```

**app/schemas/user.py** - User Schemas:
```python
"""
User-related Pydantic schemas for API request/response validation.
Includes user creation, updates, authentication, and profile management.
"""

from datetime import datetime
from typing import Optional, List
from uuid import UUID

from pydantic import BaseModel, EmailStr, Field, validator, root_validator

from app.models.user import UserRole, UserStatus
from app.schemas.common import BaseSchema, TimestampMixin, UUIDMixin, PaginatedResponse


# Base User Schemas
class UserBase(BaseSchema):
    """Base user schema with common fields."""
    username: str = Field(..., min_length=3, max_length=50, description="Unique username")
    email: EmailStr = Field(..., description="User email address")
    full_name: str = Field(..., min_length=1, max_length=100, description="User's full name")
    
    @validator('username')
    def validate_username(cls, v):
        """Validate username format."""
        if not v.replace('_', '').replace('-', '').isalnum():
            raise ValueError('Username can only contain letters, numbers, underscore, and hyphen')
        return v.lower()


class UserCreate(UserBase):
    """Schema for user creation."""
    password: str = Field(..., min_length=8, max_length=100, description="User password")
    
    @validator('password')
    def validate_password(cls, v):
        """Validate password strength."""
        if not any(c.isupper() for c in v):
            raise ValueError('Password must contain at least one uppercase letter')
        if not any(c.islower() for c in v):
            raise ValueError('Password must contain at least one lowercase letter')
        if not any(c.isdigit() for c in v):
            raise ValueError('Password must contain at least one digit')
        if not any(c in '!@#$%^&*()_+-=[]{}|;:,.<>?' for c in v):
            raise ValueError('Password must contain at least one special character')
        return v


class UserUpdate(BaseSchema):
    """Schema for user updates."""
    full_name: Optional[str] = Field(None, min_length=1, max_length=100)
    bio: Optional[str] = Field(None, max_length=1000)
    location: Optional[str] = Field(None, max_length=100)
    website_url: Optional[str] = Field(None, max_length=500)
    avatar_url: Optional[str] = Field(None, max_length=500)
    timezone: Optional[str] = Field(None, max_length=50)
    language: Optional[str] = Field(None, max_length=10)


class UserProfileUpdate(BaseSchema):
    """Schema for profile-specific updates."""
    bio: Optional[str] = Field(None, max_length=1000)
    location: Optional[str] = Field(None, max_length=100)
    website_url: Optional[str] = Field(None, max_length=500)
    birth_date: Optional[datetime] = None


class UserPasswordChange(BaseSchema):
    """Schema for password change."""
    current_password: str = Field(..., description="Current password")
    new_password: str = Field(..., min_length=8, max_length=100, description="New password")
    
    @validator('new_password')
    def validate_new_password(cls, v):
        """Validate new password strength."""
        if not any(c.isupper() for c in v):
            raise ValueError('Password must contain at least one uppercase letter')
        if not any(c.islower() for c in v):
            raise ValueError('Password must contain at least one lowercase letter')
        if not any(c.isdigit() for c in v):
            raise ValueError('Password must contain at least one digit')
        return v


# Response Schemas
class UserResponse(UserBase, TimestampMixin, UUIDMixin):
    """User response schema."""
    id: int
    role: UserRole
    status: UserStatus
    is_active: bool
    is_email_verified: bool
    posts_count: int
    last_login_at: Optional[datetime] = None
    login_count: int


class UserProfileResponse(UserResponse):
    """Extended user profile response."""
    bio: Optional[str] = None
    location: Optional[str] = None
    website_url: Optional[str] = None
    birth_date: Optional[datetime] = None
    avatar_url: Optional[str] = None
    timezone: str
    language: str
    followers_count: int
    following_count: int


class UserSummaryResponse(BaseSchema):
    """Minimal user info for references."""
    id: int
    uuid: UUID
    username: str
    full_name: str
    avatar_url: Optional[str] = None
    role: UserRole


# Authentication Schemas
class UserLogin(BaseSchema):
    """User login schema."""
    username_or_email: str = Field(..., description="Username or email")
    password: str = Field(..., description="Password")


class TokenResponse(BaseSchema):
    """Token response schema."""
    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int
    user: UserResponse


class TokenRefresh(BaseSchema):
    """Token refresh schema."""
    refresh_token: str = Field(..., description="Refresh token")


class PasswordResetRequest(BaseSchema):
    """Password reset request schema."""
    email: EmailStr = Field(..., description="User email")


class PasswordReset(BaseSchema):
    """Password reset schema."""
    token: str = Field(..., description="Reset token")
    new_password: str = Field(..., min_length=8, max_length=100)
    
    @validator('new_password')
    def validate_password(cls, v):
        """Validate password strength."""
        if not any(c.isupper() for c in v):
            raise ValueError('Password must contain at least one uppercase letter')
        if not any(c.islower() for c in v):
            raise ValueError('Password must contain at least one lowercase letter')
        if not any(c.isdigit() for c in v):
            raise ValueError('Password must contain at least one digit')
        return v


class EmailVerification(BaseSchema):
    """Email verification schema."""
    token: str = Field(..., description="Verification token")


# Filter and Search Schemas
class UserFilterParams(BaseSchema):
    """User filtering parameters."""
    search: Optional[str] = Field(None, description="Search by username, email, or full name")
    role: Optional[UserRole] = Field(None, description="Filter by role")
    status: Optional[UserStatus] = Field(None, description="Filter by status")
    is_active: Optional[bool] = Field(None, description="Filter by active status")
    is_email_verified: Optional[bool] = Field(None, description="Filter by email verification")
    created_after: Optional[datetime] = Field(None, description="Filter by creation date")
    created_before: Optional[datetime] = Field(None, description="Filter by creation date")


class UserSearchResponse(BaseSchema):
    """User search result."""
    users: List[UserSummaryResponse]
    total_count: int
    search_query: str


# Social Features Schemas
class FollowUserRequest(BaseSchema):
    """Request to follow a user."""
    user_id: int = Field(..., gt=0, description="ID of user to follow")


class UserFollowersResponse(BaseSchema):
    """User followers response."""
    followers: List[UserSummaryResponse]
    total_count: int


class UserFollowingResponse(BaseSchema):
    """User following response."""
    following: List[UserSummaryResponse]
    total_count: int


# Admin Schemas
class UserAdminUpdate(BaseSchema):
    """Admin user update schema."""
    role: Optional[UserRole] = None
    status: Optional[UserStatus] = None
    is_active: Optional[bool] = None
    max_posts_per_day: Optional[int] = Field(None, gt=0, le=100)


class UserStatistics(BaseSchema):
    """User statistics schema."""
    total_users: int
    active_users: int
    verified_users: int
    users_by_role: dict
    users_by_status: dict
    recent_registrations: int


# Paginated Responses
UsersPaginatedResponse = PaginatedResponse[UserResponse]
UserProfilesPaginatedResponse = PaginatedResponse[UserProfileResponse]
```

### 3.2 Advanced CRUD Operations

**app/crud/base.py** - Base CRUD Class with Advanced Features:
```python
"""
Base CRUD class with advanced querying capabilities.
Provides common CRUD operations with filtering, pagination, sorting, and caching.
"""

import logging
from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union
from datetime import datetime, timedelta

from sqlalchemy import and_, or_, desc, asc, func, text
from sqlalchemy.orm import Session, Query, joinedload, selectinload
from sqlalchemy.exc import IntegrityError
from pydantic import BaseModel

from app.models.base import Base
from app.schemas.common import PaginationParams, SortParams

logger = logging.getLogger(__name__)

ModelType = TypeVar("ModelType", bound=Base)
CreateSchemaType = TypeVar("CreateSchemaType", bound=BaseModel)
UpdateSchemaType = TypeVar("UpdateSchemaType", bound=BaseModel)


class CRUDBase(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):
    """Base CRUD class with advanced querying capabilities."""
    
    def __init__(self, model: Type[ModelType]):
        """Initialize with SQLAlchemy model."""
        self.model = model
    
    # Basic CRUD Operations
    def get(self, db: Session, id: Any) -> Optional[ModelType]:
        """Get a single record by ID."""
        return db.query(self.model).filter(self.model.id == id).first()
    
    def get_by_uuid(self, db: Session, uuid: str) -> Optional[ModelType]:
        """Get a single record by UUID."""
        return db.query(self.model).filter(self.model.uuid == uuid).first()
    
    def get_multi(
        self, 
        db: Session, 
        *, 
        skip: int = 0, 
        limit: int = 100,
        filters: Optional[Dict[str, Any]] = None,
        sort_by: Optional[str] = None,
        sort_order: str = "asc"
    ) -> List[ModelType]:
        """Get multiple records with optional filtering and sorting."""
        query = db.query(self.model)
        
        # Apply filters
        if filters:
            query = self._apply_filters(query, filters)
        
        # Apply sorting
        if sort_by:
            query = self._apply_sorting(query, sort_by, sort_order)
        
        return query.offset(skip).limit(limit).all()
    
    def get_multi_paginated(
        self,
        db: Session,
        *,
        pagination: PaginationParams,
        filters: Optional[Dict[str, Any]] = None,
        sort_params: Optional[SortParams] = None
    ) -> Dict[str, Any]:
        """Get paginated records with filtering and sorting."""
        query = db.query(self.model)
        
        # Apply filters
        if filters:
            query = self._apply_filters(query, filters)
        
        # Get total count before pagination
        total = query.count()
        
        # Apply sorting
        if sort_params and sort_params.sort_by:
            query = self._apply_sorting(query, sort_params.sort_by, sort_params.sort_order)
        else:
            # Default sorting by ID
            query = query.order_by(desc(self.model.id))
        
        # Apply pagination
        items = query.offset(pagination.offset).limit(pagination.size).all()
        
        return {
            "items": items,
            "total": total,
            "page": pagination.page,
            "size": pagination.size,
            "pages": (total + pagination.size - 1) // pagination.size
        }
    
    def create(self, db: Session, *, obj_in: CreateSchemaType, **kwargs) -> ModelType:
        """Create a new record."""
        obj_in_data = obj_in.model_dump() if hasattr(obj_in, 'model_dump') else obj_in.dict()
        obj_in_data.update(kwargs)
        
        db_obj = self.model(**obj_in_data)
        db.add(db_obj)
        
        try:
            db.commit()
            db.refresh(db_obj)
            return db_obj
        except IntegrityError as e:
            db.rollback()
            logger.error(f"Failed to create {self.model.__name__}: {e}")
            raise ValueError(f"Failed to create record: {str(e)}")
    
    def update(
        self,
        db: Session,
        *,
        db_obj: ModelType,
        obj_in: Union[UpdateSchemaType, Dict[str, Any]]
    ) -> ModelType:
        """Update an existing record."""
        obj_data = db_obj.to_dict()
        
        if isinstance(obj_in, dict):
            update_data = obj_in
        else:
            update_data = obj_in.model_dump(exclude_unset=True) if hasattr(obj_in, 'model_dump') else obj_in.dict(exclude_unset=True)
        
        for field in obj_data:
            if field in update_data:
                setattr(db_obj, field, update_data[field])
        
        # Update the version for optimistic locking
        if hasattr(db_obj, 'version'):
            db_obj.version += 1
        
        try:
            db.commit()
            db.refresh(db_obj)
            return db_obj
        except IntegrityError as e:
            db.rollback()
            logger.error(f"Failed to update {self.model.__name__}: {e}")
            raise ValueError(f"Failed to update record: {str(e)}")
    
    def remove(self, db: Session, *, id: int) -> Optional[ModelType]:
        """Remove a record (hard delete)."""
        obj = db.query(self.model).get(id)
        if obj:
            db.delete(obj)
            db.commit()
            return obj
        return None
    
    def soft_delete(self, db: Session, *, id: int, user_id: Optional[int] = None) -> Optional[ModelType]:
        """Soft delete a record."""
        obj = db.query(self.model).get(id)
        if obj and hasattr(obj, 'soft_delete'):
            obj.soft_delete(user_id)
            db.commit()
            db.refresh(obj)
            return obj
        return None
    
    def restore(self, db: Session, *, id: int, user_id: Optional[int] = None) -> Optional[ModelType]:
        """Restore a soft-deleted record."""
        obj = db.query(self.model).get(id)
        if obj and hasattr(obj, 'restore'):
            obj.restore(user_id)
            db.commit()
            db.refresh(obj)
            return obj
        return None
    
    # Advanced Query Methods
    def search(
        self, 
        db: Session, 
        *, 
        search_term: str, 
        search_fields: List[str],
        limit: int = 50
    ) -> List[ModelType]:
        """Full-text search across specified fields."""
        if not search_term or not search_fields:
            return []
        
        search_conditions = []
        for field in search_fields:
            if hasattr(self.model, field):
                column = getattr(self.model, field)
                search_conditions.append(column.ilike(f"%{search_term}%"))
        
        if not search_conditions:
            return []
        
        query = db.query(self.model).filter(or_(*search_conditions))
        return query.limit(limit).all()
    
    def get_by_field(self, db: Session, field: str, value: Any) -> Optional[ModelType]:
        """Get record by specific field value."""
        if not hasattr(self.model, field):
            return None
        
        column = getattr(self.model, field)
        return db.query(self.model).filter(column == value).first()
    
    def get_multi_by_field(
        self, 
        db: Session, 
        field: str, 
        values: List[Any]
    ) -> List[ModelType]:
        """Get multiple records by field values."""
        if not hasattr(self.model, field):
            return []
        
        column = getattr(self.model, field)
        return db.query(self.model).filter(column.in_(values)).all()
    
    def count(self, db: Session, filters: Optional[Dict[str, Any]] = None) -> int:
        """Count records with optional filters."""
        query = db.query(self.model)
        if filters:
            query = self._apply_filters(query, filters)
        return query.count()
    
    def exists(self, db: Session, id: int) -> bool:
        """Check if record exists."""
        return db.query(db.query(self.model).filter(self.model.id == id).exists()).scalar()
    
    def bulk_create(self, db: Session, *, obj_list: List[CreateSchemaType]) -> List[ModelType]:
        """Bulk create records."""
        db_objs = []
        for obj_in in obj_list:
            obj_in_data = obj_in.model_dump() if hasattr(obj_in, 'model_dump') else obj_in.dict()
            db_obj = self.model(**obj_in_data)
            db_objs.append(db_obj)
        
        try:
            db.add_all(db_objs)
            db.commit()
            
            # Refresh all objects
            for db_obj in db_objs:
                db.refresh(db_obj)
            
            return db_objs
        except IntegrityError as e:
            db.rollback()
            logger.error(f"Failed to bulk create {self.model.__name__}: {e}")
            raise ValueError(f"Failed to bulk create records: {str(e)}")
    
    def bulk_update(
        self, 
        db: Session, 
        *, 
        updates: List[Dict[str, Any]]
    ) -> int:
        """Bulk update records."""
        if not updates:
            return 0
        
        try:
            result = db.bulk_update_mappings(self.model, updates)
            db.commit()
            return len(updates)
        except Exception as e:
            db.rollback()
            logger.error(f"Failed to bulk update {self.model.__name__}: {e}")
            raise ValueError(f"Failed to bulk update records: {str(e)}")
    
    # Analytics and Statistics
    def get_statistics(self, db: Session) -> Dict[str, Any]:
        """Get basic statistics for the model."""
        total = db.query(self.model).count()
        
        stats = {"total": total}
        
        # Add soft-delete statistics if applicable
        if hasattr(self.model, 'is_deleted'):
            active = db.query(self.model).filter(self.model.is_deleted == False).count()
            deleted = total - active
            stats.update({
                "active": active,
                "deleted": deleted
            })
        
        # Add date-based statistics
        if hasattr(self.model, 'created_at'):
            today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
            created_today = db.query(self.model).filter(
                self.model.created_at >= today
            ).count()
            
            week_ago = today - timedelta(days=7)
            created_this_week = db.query(self.model).filter(
                self.model.created_at >= week_ago
            ).count()
            
            stats.update({
                "created_today": created_today,
                "created_this_week": created_this_week
            })
        
        return stats
    
    # Helper Methods
    def _apply_filters(self, query: Query, filters: Dict[str, Any]) -> Query:
        """Apply filters to query."""
        for field, value in filters.items():
            if value is None:
                continue
            
            if not hasattr(self.model, field):
                continue
            
            column = getattr(self.model, field)
            
            # Handle different filter types
            if isinstance(value, list):
                query = query.filter(column.in_(value))
            elif isinstance(value, dict):
                # Support for range queries like {'gte': 10, 'lte': 20}
                if 'gte' in value:
                    query = query.filter(column >= value['gte'])
                if 'lte' in value:
                    query = query.filter(column <= value['lte'])
                if 'gt' in value:
                    query = query.filter(column > value['gt'])
                if 'lt' in value:
                    query = query.filter(column < value['lt'])
            elif isinstance(value, str) and field.endswith('_like'):
                # Support for LIKE queries
                actual_field = field[:-5]  # Remove '_like' suffix
                if hasattr(self.model, actual_field):
                    actual_column = getattr(self.model, actual_field)
                    query = query.filter(actual_column.ilike(f"%{value}%"))
            else:
                query = query.filter(column == value)
        
        return query
    
    def _apply_sorting(self, query: Query, sort_by: str, sort_order: str = "asc") -> Query:
        """Apply sorting to query."""
        if not hasattr(self.model, sort_by):
            return query
        
        column = getattr(self.model, sort_by)
        
        if sort_order.lower() == "desc":
            query = query.order_by(desc(column))
        else:
            query = query.order_by(asc(column))
        
        return query
    
    def get_with_relationships(
        self, 
        db: Session, 
        id: int, 
        *,
        joined_loads: Optional[List[str]] = None,
        select_in_loads: Optional[List[str]] = None
    ) -> Optional[ModelType]:
        """Get record with specific relationships loaded."""
        query = db.query(self.model).filter(self.model.id == id)
        
        # Apply joined loads
        if joined_loads:
            for relationship in joined_loads:
                if hasattr(self.model, relationship):
                    query = query.options(joinedload(getattr(self.model, relationship)))
        
        # Apply select in loads
        if select_in_loads:
            for relationship in select_in_loads:
                if hasattr(self.model, relationship):
                    query = query.options(selectinload(getattr(self.model, relationship)))
        
        return query.first()


class CRUDWithSoftDelete(CRUDBase[ModelType, CreateSchemaType, UpdateSchemaType]):
    """CRUD class with soft delete support."""
    
    def get_active(self, db: Session, id: int) -> Optional[ModelType]:
        """Get active (non-deleted) record by ID."""
        return db.query(self.model).filter(
            and_(self.model.id == id, self.model.is_deleted == False)
        ).first()
    
    def get_multi_active(
        self,
        db: Session,
        *,
        skip: int = 0,
        limit: int = 100,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[ModelType]:
        """Get multiple active records."""
        query = db.query(self.model).filter(self.model.is_deleted == False)
        
        if filters:
            query = self._apply_filters(query, filters)
        
        return query.offset(skip).limit(limit).all()
    
    def get_deleted(self, db: Session, *, skip: int = 0, limit: int = 100) -> List[ModelType]:
        """Get deleted records."""
        return db.query(self.model).filter(
            self.model.is_deleted == True
        ).offset(skip).limit(limit).all()
```

This is the first part of Section 3.1-3.2. The implementation includes:

**✅ Section 3.1 - Pydantic Schemas:**
- **Common Schemas**: Base classes, pagination, filtering, and API responses
- **User Schemas**: Complete request/response models with validation
- **Advanced Validation**: Password strength, email format, business rules

**✅ Section 3.2 - Advanced CRUD Operations:**
- **Base CRUD Class**: Generic CRUD with advanced querying capabilities
- **Filtering & Sorting**: Dynamic filters, range queries, LIKE operations
- **Pagination**: Efficient pagination with total counts
- **Bulk Operations**: Bulk create/update with transaction safety
- **Search Functionality**: Full-text search across multiple fields
- **Relationship Loading**: Optimized loading strategies
- **Statistics**: Built-in analytics and reporting
- **Soft Delete Support**: Complete soft delete implementation

**Should I continue with Section 3.3: User CRUD Implementation?**

This will include:
- Complete User CRUD class with authentication features
- Advanced user queries (followers, search, analytics)
- Social features implementation
- Role-based operations

Let me know to continue with the next section!

### 3.3 User CRUD Implementation

**app/crud/user.py** - Complete User CRUD Operations:
```python
"""
Complete User CRUD operations with authentication, social features, and analytics.
Demonstrates advanced querying, relationships, and business logic integration.
"""

import logging
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Union

from sqlalchemy import and_, or_, func, desc, asc, text
from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy.exc import IntegrityError

from app.crud.base import CRUDBase
from app.models.user import User, UserRole, UserStatus, UserSession, user_followers
from app.schemas.user import UserCreate, UserUpdate, UserFilterParams
from app.core.security import SecurityManager
from app.schemas.common import PaginationParams

logger = logging.getLogger(__name__)


class UserCRUD(CRUDBase[User, UserCreate, UserUpdate]):
    """Complete User CRUD operations with advanced features."""
    
    def __init__(self):
        super().__init__(User)
    
    # Authentication Methods
    def create_user(
        self, 
        db: Session, 
        *, 
        user_create: UserCreate,
        created_by_id: Optional[int] = None
    ) -> User:
        """Create new user with password hashing and validation."""
        try:
            # Check if username or email already exists
            existing_user = db.query(User).filter(
                or_(
                    User.username == user_create.username,
                    User.email == user_create.email
                )
            ).first()
            
            if existing_user:
                if existing_user.username == user_create.username:
                    raise ValueError("Username already exists")
                if existing_user.email == user_create.email:
                    raise ValueError("Email already exists")
            
            # Create user with hashed password
            user_data = user_create.model_dump(exclude={'password'})
            user = User(
                **user_data,
                created_by_id=created_by_id,
                email_verification_token=SecurityManager.generate_email_verification_token()
            )
            
            # Hash password
            user.set_password(user_create.password)
            
            db.add(user)
            db.commit()
            db.refresh(user)
            
            logger.info(f"User created: {user.username} (ID: {user.id})")
            return user
            
        except IntegrityError as e:
            db.rollback()
            logger.error(f"Failed to create user: {e}")
            raise ValueError("Failed to create user due to database constraint")
    
    def authenticate_user(
        self, 
        db: Session, 
        *, 
        username_or_email: str, 
        password: str
    ) -> Optional[User]:
        """Authenticate user by username or email and password."""
        user = db.query(User).filter(
            or_(
                User.username == username_or_email.lower(),
                User.email == username_or_email.lower()
            )
        ).first()
        
        if not user:
            return None
        
        if not user.verify_password(password):
            return None
        
        # Check if user is active and not suspended
        if not user.is_active or user.status == UserStatus.SUSPENDED:
            return None
        
        return user
    
    def verify_email(self, db: Session, *, token: str) -> bool:
        """Verify user email using verification token."""
        user = db.query(User).filter(
            User.email_verification_token == token,
            User.is_email_verified == False
        ).first()
        
        if not user:
            return False
        
        user.is_email_verified = True
        user.status = UserStatus.ACTIVE
        user.email_verification_token = None
        
        db.commit()
        logger.info(f"Email verified for user: {user.username}")
        return True
    
    def initiate_password_reset(self, db: Session, *, email: str) -> Optional[str]:
        """Generate password reset token for user."""
        user = db.query(User).filter(User.email == email.lower()).first()
        
        if not user:
            return None
        
        token = SecurityManager.generate_password_reset_token()
        user.reset_password_token = token
        user.reset_password_expires = datetime.utcnow() + timedelta(hours=1)
        
        db.commit()
        logger.info(f"Password reset initiated for user: {user.username}")
        return token
    
    def reset_password(self, db: Session, *, token: str, new_password: str) -> bool:
        """Reset user password using reset token."""
        user = db.query(User).filter(
            User.reset_password_token == token,
            User.reset_password_expires > datetime.utcnow()
        ).first()
        
        if not user:
            return False
        
        user.set_password(new_password)
        user.reset_password_token = None
        user.reset_password_expires = None
        
        # Invalidate all user sessions
        db.query(UserSession).filter(UserSession.user_id == user.id).update({
            'is_active': False
        })
        
        db.commit()
        logger.info(f"Password reset completed for user: {user.username}")
        return True
    
    # Profile and User Management
    def get_user_profile(self, db: Session, *, user_id: int) -> Optional[User]:
        """Get user profile with relationships loaded."""
        return db.query(User).options(
            selectinload(User.posts),
            selectinload(User.followers),
            selectinload(User.following)
        ).filter(User.id == user_id).first()
    
    def update_user_profile(
        self, 
        db: Session, 
        *, 
        user_id: int, 
        user_update: UserUpdate,
        updated_by_id: Optional[int] = None
    ) -> Optional[User]:
        """Update user profile information."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            return None
        
        update_data = user_update.model_dump(exclude_unset=True)
        
        for field, value in update_data.items():
            if hasattr(user, field):
                setattr(user, field, value)
        
        user.updated_by_id = updated_by_id
        
        db.commit()
        db.refresh(user)
        
        logger.info(f"User profile updated: {user.username}")
        return user
    
    def change_password(
        self, 
        db: Session, 
        *, 
        user_id: int, 
        current_password: str, 
        new_password: str
    ) -> bool:
        """Change user password with current password verification."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            return False
        
        if not user.verify_password(current_password):
            return False
        
        user.set_password(new_password)
        
        # Invalidate all other sessions
        db.query(UserSession).filter(
            UserSession.user_id == user.id
        ).update({'is_active': False})
        
        db.commit()
        logger.info(f"Password changed for user: {user.username}")
        return True
    
    # Advanced Search and Filtering
    def search_users(
        self, 
        db: Session, 
        *, 
        search_query: str,
        filters: Optional[UserFilterParams] = None,
        pagination: Optional[PaginationParams] = None
    ) -> Dict[str, Any]:
        """Advanced user search with filters and pagination."""
        query = db.query(User)
        
        # Apply search across multiple fields
        if search_query:
            search_conditions = [
                User.username.ilike(f"%{search_query}%"),
                User.email.ilike(f"%{search_query}%"),
                User.full_name.ilike(f"%{search_query}%"),
                User.bio.ilike(f"%{search_query}%"),
                User.location.ilike(f"%{search_query}%")
            ]
            query = query.filter(or_(*search_conditions))
        
        # Apply filters
        if filters:
            if filters.role:
                query = query.filter(User.role == filters.role)
            if filters.status:
                query = query.filter(User.status == filters.status)
            if filters.is_active is not None:
                query = query.filter(User.is_active == filters.is_active)
            if filters.is_email_verified is not None:
                query = query.filter(User.is_email_verified == filters.is_email_verified)
            if filters.created_after:
                query = query.filter(User.created_at >= filters.created_after)
            if filters.created_before:
                query = query.filter(User.created_at <= filters.created_before)
        
        # Get total count
        total = query.count()
        
        # Apply pagination
        if pagination:
            query = query.offset(pagination.offset).limit(pagination.size)
            page = pagination.page
            size = pagination.size
        else:
            page = 1
            size = total
        
        # Execute query
        users = query.all()
        
        return {
            "users": users,
            "total": total,
            "page": page,
            "size": size,
            "pages": (total + size - 1) // size if size > 0 else 0
        }
    
    def get_users_by_role(self, db: Session, *, role: UserRole) -> List[User]:
        """Get all users with specific role."""
        return db.query(User).filter(User.role == role).all()
    
    def get_active_users_count(self, db: Session) -> int:
        """Get count of active users."""
        return db.query(User).filter(
            and_(User.is_active == True, User.status == UserStatus.ACTIVE)
        ).count()
    
    # Social Features Implementation
    def follow_user(self, db: Session, *, follower_id: int, followed_id: int) -> bool:
        """Follow a user (social feature)."""
        if follower_id == followed_id:
            raise ValueError("User cannot follow themselves")
        
        # Check if users exist
        follower = db.query(User).filter(User.id == follower_id).first()
        followed = db.query(User).filter(User.id == followed_id).first()
        
        if not follower or not followed:
            return False
        
        # Check if already following
        existing_follow = db.query(user_followers).filter(
            and_(
                user_followers.c.follower_id == follower_id,
                user_followers.c.followed_id == followed_id
            )
        ).first()
        
        if existing_follow:
            return False  # Already following
        
        # Create follow relationship
        db.execute(user_followers.insert().values(
            follower_id=follower_id,
            followed_id=followed_id
        ))
        
        db.commit()
        logger.info(f"User {follower.username} followed {followed.username}")
        return True
    
    def unfollow_user(self, db: Session, *, follower_id: int, followed_id: int) -> bool:
        """Unfollow a user."""
        result = db.execute(user_followers.delete().where(
            and_(
                user_followers.c.follower_id == follower_id,
                user_followers.c.followed_id == followed_id
            )
        ))
        
        if result.rowcount > 0:
            db.commit()
            logger.info(f"User {follower_id} unfollowed {followed_id}")
            return True
        
        return False
    
    def get_user_followers(
        self, 
        db: Session, 
        *, 
        user_id: int,
        pagination: Optional[PaginationParams] = None
    ) -> Dict[str, Any]:
        """Get user's followers with pagination."""
        query = db.query(User).join(
            user_followers, User.id == user_followers.c.follower_id
        ).filter(user_followers.c.followed_id == user_id)
        
        total = query.count()
        
        if pagination:
            query = query.offset(pagination.offset).limit(pagination.size)
            page = pagination.page
            size = pagination.size
        else:
            page = 1
            size = total
        
        followers = query.all()
        
        return {
            "followers": followers,
            "total": total,
            "page": page,
            "size": size,
            "pages": (total + size - 1) // size if size > 0 else 0
        }
    
    def get_user_following(
        self, 
        db: Session, 
        *, 
        user_id: int,
        pagination: Optional[PaginationParams] = None
    ) -> Dict[str, Any]:
        """Get users that this user is following."""
        query = db.query(User).join(
            user_followers, User.id == user_followers.c.followed_id
        ).filter(user_followers.c.follower_id == user_id)
        
        total = query.count()
        
        if pagination:
            query = query.offset(pagination.offset).limit(pagination.size)
            page = pagination.page
            size = pagination.size
        else:
            page = 1
            size = total
        
        following = query.all()
        
        return {
            "following": following,
            "total": total,
            "page": page,
            "size": size,
            "pages": (total + size - 1) // size if size > 0 else 0
        }
    
    def is_following(self, db: Session, *, follower_id: int, followed_id: int) -> bool:
        """Check if one user is following another."""
        result = db.query(user_followers).filter(
            and_(
                user_followers.c.follower_id == follower_id,
                user_followers.c.followed_id == followed_id
            )
        ).first()
        
        return result is not None
    
    def get_mutual_followers(self, db: Session, *, user1_id: int, user2_id: int) -> List[User]:
        """Get mutual followers between two users."""
        # Users who follow both user1 and user2
        query = text("""
            SELECT u.* FROM users u
            WHERE u.id IN (
                SELECT uf1.follower_id 
                FROM user_followers uf1
                WHERE uf1.followed_id = :user1_id
                INTERSECT
                SELECT uf2.follower_id 
                FROM user_followers uf2
                WHERE uf2.followed_id = :user2_id
            )
        """)
        
        result = db.execute(query, {'user1_id': user1_id, 'user2_id': user2_id})
        return [User(**row._asdict()) for row in result.fetchall()]
    
    # User Activity and Analytics
    def record_user_login(
        self, 
        db: Session, 
        *, 
        user_id: int, 
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None
    ) -> None:
        """Record user login activity."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if user:
            user.record_login(ip_address)
            
            # Create session record
            session = UserSession.create_session(
                user_id=user_id,
                ip_address=ip_address,
                user_agent=user_agent
            )
            db.add(session)
            
            db.commit()
    
    def get_user_login_history(
        self, 
        db: Session, 
        *, 
        user_id: int,
        limit: int = 50
    ) -> List[UserSession]:
        """Get user's login history."""
        return db.query(UserSession).filter(
            UserSession.user_id == user_id
        ).order_by(desc(UserSession.created_at)).limit(limit).all()
    
    def get_user_statistics(self, db: Session, *, user_id: int) -> Dict[str, Any]:
        """Get comprehensive user statistics."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            return {}
        
        # Get follower and following counts
        followers_count = db.query(user_followers).filter(
            user_followers.c.followed_id == user_id
        ).count()
        
        following_count = db.query(user_followers).filter(
            user_followers.c.follower_id == user_id
        ).count()
        
        # Get post statistics (assuming Post model exists)
        total_posts = user.posts_count
        
        # Get recent activity
        recent_logins = db.query(UserSession).filter(
            UserSession.user_id == user_id,
            UserSession.created_at >= datetime.utcnow() - timedelta(days=30)
        ).count()
        
        return {
            "user_id": user_id,
            "username": user.username,
            "followers_count": followers_count,
            "following_count": following_count,
            "total_posts": total_posts,
            "login_count": user.login_count,
            "last_login": user.last_login_at,
            "recent_logins_30d": recent_logins,
            "account_age_days": (datetime.utcnow() - user.created_at).days,
            "is_verified": user.is_email_verified,
            "role": user.role.value,
            "status": user.status.value
        }
    
    # Admin Operations
    def get_user_overview(self, db: Session) -> Dict[str, Any]:
        """Get system-wide user statistics for admin dashboard."""
        total_users = db.query(User).count()
        active_users = db.query(User).filter(User.is_active == True).count()
        verified_users = db.query(User).filter(User.is_email_verified == True).count()
        
        # Users by role
        role_stats = db.query(
            User.role,
            func.count(User.id).label('count')
        ).group_by(User.role).all()
        
        users_by_role = {role.value: count for role, count in role_stats}
        
        # Users by status
        status_stats = db.query(
            User.status,
            func.count(User.id).label('count')
        ).group_by(User.status).all()
        
        users_by_status = {status.value: count for status, count in status_stats}
        
        # Recent registrations
        week_ago = datetime.utcnow() - timedelta(days=7)
        recent_registrations = db.query(User).filter(
            User.created_at >= week_ago
        ).count()
        
        return {
            "total_users": total_users,
            "active_users": active_users,
            "verified_users": verified_users,
            "users_by_role": users_by_role,
            "users_by_status": users_by_status,
            "recent_registrations": recent_registrations,
            "verification_rate": (verified_users / total_users * 100) if total_users > 0 else 0
        }
    
    def update_user_role(
        self, 
        db: Session, 
        *, 
        user_id: int, 
        new_role: UserRole,
        updated_by_id: int
    ) -> Optional[User]:
        """Update user role (admin operation)."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            return None
        
        old_role = user.role
        user.role = new_role
        user.updated_by_id = updated_by_id
        
        db.commit()
        db.refresh(user)
        
        logger.info(f"User role updated: {user.username} from {old_role.value} to {new_role.value}")
        return user
    
    def suspend_user(
        self, 
        db: Session, 
        *, 
        user_id: int, 
        suspended_by_id: int,
        reason: Optional[str] = None
    ) -> Optional[User]:
        """Suspend user account."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            return None
        
        user.status = UserStatus.SUSPENDED
        user.is_active = False
        user.updated_by_id = suspended_by_id
        
        # Invalidate all user sessions
        db.query(UserSession).filter(UserSession.user_id == user_id).update({
            'is_active': False
        })
        
        db.commit()
        db.refresh(user)
        
        logger.info(f"User suspended: {user.username} by user {suspended_by_id}")
        return user
    
    def activate_user(
        self, 
        db: Session, 
        *, 
        user_id: int, 
        activated_by_id: int
    ) -> Optional[User]:
        """Reactivate suspended user account."""
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            return None
        
        user.status = UserStatus.ACTIVE
        user.is_active = True
        user.updated_by_id = activated_by_id
        
        db.commit()
        db.refresh(user)
        
        logger.info(f"User activated: {user.username} by user {activated_by_id}")
        return user
    
    # Bulk Operations
    def bulk_update_user_status(
        self, 
        db: Session, 
        *, 
        user_ids: List[int], 
        new_status: UserStatus,
        updated_by_id: int
    ) -> int:
        """Bulk update user status."""
        result = db.query(User).filter(User.id.in_(user_ids)).update(
            {
                User.status: new_status,
                User.updated_by_id: updated_by_id,
                User.updated_at: datetime.utcnow()
            },
            synchronize_session=False
        )
        
        db.commit()
        logger.info(f"Bulk updated {result} users to status {new_status.value}")
        return result
    
    def delete_inactive_users(
        self, 
        db: Session, 
        *, 
        inactive_days: int = 365,
        dry_run: bool = True
    ) -> List[int]:
        """Delete users who have been inactive for specified days."""
        cutoff_date = datetime.utcnow() - timedelta(days=inactive_days)
        
        inactive_users = db.query(User).filter(
            or_(
                User.last_login_at.is_(None),
                User.last_login_at < cutoff_date
            ),
            User.status != UserStatus.SUSPENDED  # Don't delete suspended users
        ).all()
        
        user_ids = [user.id for user in inactive_users]
        
        if not dry_run:
            # Perform soft delete
            for user in inactive_users:
                user.soft_delete()
            
            db.commit()
            logger.info(f"Soft deleted {len(inactive_users)} inactive users")
        else:
            logger.info(f"Would delete {len(inactive_users)} inactive users (dry run)")
        
        return user_ids


# Create CRUD instance
user_crud = UserCRUD()
```

### 3.4 Session Management CRUD

**app/crud/session.py** - User Session Management:
```python
"""
User session management CRUD operations.
Handles user sessions, token management, and security features.
"""

import logging
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any

from sqlalchemy import and_, desc
from sqlalchemy.orm import Session

from app.crud.base import CRUDBase
from app.models.user import UserSession, User
from app.schemas.common import PaginationParams

logger = logging.getLogger(__name__)


class SessionCRUD(CRUDBase[UserSession, dict, dict]):
    """User session management operations."""
    
    def __init__(self):
        super().__init__(UserSession)
    
    def create_session(
        self,
        db: Session,
        *,
        user_id: int,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
        device_info: Optional[str] = None,
        expires_in_hours: int = 24
    ) -> UserSession:
        """Create new user session."""
        session = UserSession.create_session(
            user_id=user_id,
            expires_in_hours=expires_in_hours,
            ip_address=ip_address,
            user_agent=user_agent,
            device_info=device_info
        )
        
        db.add(session)
        db.commit()
        db.refresh(session)
        
        logger.info(f"Session created for user {user_id}")
        return session
    
    def get_session_by_token(self, db: Session, *, token: str) -> Optional[UserSession]:
        """Get session by token."""
        return db.query(UserSession).filter(
            and_(
                UserSession.session_token == token,
                UserSession.is_active == True,
                UserSession.expires_at > datetime.utcnow()
            )
        ).first()
    
    def get_user_sessions(
        self,
        db: Session,
        *,
        user_id: int,
        active_only: bool = True,
        pagination: Optional[PaginationParams] = None
    ) -> Dict[str, Any]:
        """Get all sessions for a user."""
        query = db.query(UserSession).filter(UserSession.user_id == user_id)
        
        if active_only:
            query = query.filter(
                and_(
                    UserSession.is_active == True,
                    UserSession.expires_at > datetime.utcnow()
                )
            )
        
        query = query.order_by(desc(UserSession.last_activity_at))
        
        total = query.count()
        
        if pagination:
            query = query.offset(pagination.offset).limit(pagination.size)
            page = pagination.page
            size = pagination.size
        else:
            page = 1
            size = total
        
        sessions = query.all()
        
        return {
            "sessions": sessions,
            "total": total,
            "page": page,
            "size": size,
            "pages": (total + size - 1) // size if size > 0 else 0
        }
    
    def update_session_activity(self, db: Session, *, session_token: str) -> bool:
        """Update session last activity timestamp."""
        session = db.query(UserSession).filter(
            UserSession.session_token == session_token
        ).first()
        
        if session and session.is_active and not session.is_expired():
            session.last_activity_at = datetime.utcnow()
            db.commit()
            return True
        
        return False
    
    def invalidate_session(self, db: Session, *, session_token: str) -> bool:
        """Invalidate a specific session."""
        result = db.query(UserSession).filter(
            UserSession.session_token == session_token
        ).update({'is_active': False})
        
        if result > 0:
            db.commit()
            logger.info(f"Session invalidated: {session_token[:10]}...")
            return True
        
        return False
    
    def invalidate_user_sessions(
        self,
        db: Session,
        *,
        user_id: int,
        exclude_token: Optional[str] = None
    ) -> int:
        """Invalidate all sessions for a user (except optionally one)."""
        query = db.query(UserSession).filter(UserSession.user_id == user_id)
        
        if exclude_token:
            query = query.filter(UserSession.session_token != exclude_token)
        
        result = query.update({'is_active': False})
        db.commit()
        
        logger.info(f"Invalidated {result} sessions for user {user_id}")
        return result
    
    def cleanup_expired_sessions(self, db: Session) -> int:
        """Clean up expired sessions."""
        result = db.query(UserSession).filter(
            or_(
                UserSession.expires_at <= datetime.utcnow(),
                UserSession.is_active == False
            )
        ).delete()
        
        db.commit()
        logger.info(f"Cleaned up {result} expired sessions")
        return result
    
    def get_session_analytics(self, db: Session, *, days: int = 30) -> Dict[str, Any]:
        """Get session analytics for the last N days."""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        # Total sessions created
        total_sessions = db.query(UserSession).filter(
            UserSession.created_at >= cutoff_date
        ).count()
        
        # Active sessions
        active_sessions = db.query(UserSession).filter(
            and_(
                UserSession.is_active == True,
                UserSession.expires_at > datetime.utcnow()
            )
        ).count()
        
        # Unique users with sessions
        unique_users = db.query(UserSession.user_id).filter(
            UserSession.created_at >= cutoff_date
        ).distinct().count()
        
        # Average session duration (for completed sessions)
        avg_duration_query = db.query(
            func.avg(
                func.extract('epoch', UserSession.last_activity_at) - 
                func.extract('epoch', UserSession.created_at)
            ).label('avg_duration')
        ).filter(
            and_(
                UserSession.created_at >= cutoff_date,
                UserSession.is_active == False,
                UserSession.last_activity_at.isnot(None)
            )
        ).first()
        
        avg_duration = avg_duration_query.avg_duration if avg_duration_query.avg_duration else 0
        
        return {
            "total_sessions": total_sessions,
            "active_sessions": active_sessions,
            "unique_users": unique_users,
            "average_duration_seconds": int(avg_duration) if avg_duration else 0,
            "period_days": days
        }


# Create CRUD instance
session_crud = SessionCRUD()
```

### 3.5 Blog CRUD Implementation

**app/schemas/blog.py** - Blog Schemas:
```python
"""
Blog-related Pydantic schemas for API request/response validation.
Includes posts, comments, categories, tags, and content management.
"""

from datetime import datetime
from typing import Optional, List, Dict, Any
from uuid import UUID

from pydantic import BaseModel, Field, validator, HttpUrl

from app.models.blog import PostStatus, CommentStatus
from app.schemas.common import BaseSchema, TimestampMixin, UUIDMixin, PaginatedResponse
from app.schemas.user import UserSummaryResponse


# Category Schemas
class CategoryBase(BaseSchema):
    """Base category schema."""
    name: str = Field(..., min_length=1, max_length=100)
    slug: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=1000)
    sort_order: int = Field(default=0, ge=0)
    is_visible: bool = Field(default=True)


class CategoryCreate(CategoryBase):
    """Schema for category creation."""
    parent_id: Optional[int] = Field(None, gt=0)
    meta_title: Optional[str] = Field(None, max_length=200)
    meta_description: Optional[str] = Field(None, max_length=500)


class CategoryUpdate(BaseSchema):
    """Schema for category updates."""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=1000)
    parent_id: Optional[int] = Field(None, gt=0)
    sort_order: Optional[int] = Field(None, ge=0)
    is_visible: Optional[bool] = None
    meta_title: Optional[str] = Field(None, max_length=200)
    meta_description: Optional[str] = Field(None, max_length=500)


class CategoryResponse(CategoryBase, TimestampMixin, UUIDMixin):
    """Category response schema."""
    id: int
    posts_count: int
    full_path: str
    parent_id: Optional[int] = None
    
    # Include children for nested responses
    children: List['CategoryResponse'] = []


# Tag Schemas
class TagBase(BaseSchema):
    """Base tag schema."""
    name: str = Field(..., min_length=1, max_length=50)
    slug: str = Field(..., min_length=1, max_length=50)
    description: Optional[str] = Field(None, max_length=500)
    color: str = Field(default="#007bff", regex="^#[0-9a-fA-F]{6}$")


class TagCreate(TagBase):
    """Schema for tag creation."""
    pass


class TagUpdate(BaseSchema):
    """Schema for tag updates."""
    name: Optional[str] = Field(None, min_length=1, max_length=50)
    description: Optional[str] = Field(None, max_length=500)
    color: Optional[str] = Field(None, regex="^#[0-9a-fA-F]{6}$")


class TagResponse(TagBase, TimestampMixin, UUIDMixin):
    """Tag response schema."""
    id: int
    usage_count: int


# Post Schemas
class PostBase(BaseSchema):
    """Base post schema."""
    title: str = Field(..., min_length=1, max_length=200)
    slug: str = Field(..., min_length=1, max_length=200)
    content: str = Field(..., min_length=1)
    excerpt: Optional[str] = Field(None, max_length=500)
    category_id: Optional[int] = Field(None, gt=0)
    allow_comments: bool = Field(default=True)
    is_featured: bool = Field(default=False)
    is_pinned: bool = Field(default=False)


class PostCreate(PostBase):
    """Schema for post creation."""
    status: PostStatus = Field(default=PostStatus.DRAFT)
    scheduled_for: Optional[datetime] = None
    meta_title: Optional[str] = Field(None, max_length=200)
    meta_description: Optional[str] = Field(None, max_length=500)
    featured_image_url: Optional[HttpUrl] = None
    tag_ids: List[int] = Field(default_factory=list, description="List of tag IDs")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
    
    @validator('scheduled_for')
    def validate_scheduled_date(cls, v):
        if v and v <= datetime.utcnow():
            raise ValueError('Scheduled date must be in the future')
        return v


class PostUpdate(BaseSchema):
    """Schema for post updates."""
    title: Optional[str] = Field(None, min_length=1, max_length=200)
    content: Optional[str] = Field(None, min_length=1)
    excerpt: Optional[str] = Field(None, max_length=500)
    category_id: Optional[int] = Field(None, gt=0)
    status: Optional[PostStatus] = None
    scheduled_for: Optional[datetime] = None
    allow_comments: Optional[bool] = None
    is_featured: Optional[bool] = None
    is_pinned: Optional[bool] = None
    meta_title: Optional[str] = Field(None, max_length=200)
    meta_description: Optional[str] = Field(None, max_length=500)
    featured_image_url: Optional[HttpUrl] = None
    tag_ids: Optional[List[int]] = None
    metadata: Optional[Dict[str, Any]] = None


class PostResponse(PostBase, TimestampMixin, UUIDMixin):
    """Post response schema."""
    id: int
    author_id: int
    status: PostStatus
    published_at: Optional[datetime]
    scheduled_for: Optional[datetime]
    view_count: int
    like_count: int
    comment_count: int
    share_count: int
    word_count: int
    read_time_minutes: int
    engagement_score: int
    
    # Related objects
    author: UserSummaryResponse
    category: Optional[CategoryResponse] = None
    tags: List[TagResponse] = []
    
    # Metadata
    meta_title: Optional[str] = None
    meta_description: Optional[str] = None
    featured_image_url: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class PostSummaryResponse(BaseSchema):
    """Minimal post info for listings."""
    id: int
    uuid: UUID
    title: str
    slug: str
    excerpt: Optional[str]
    author: UserSummaryResponse
    category: Optional[CategoryResponse] = None
    status: PostStatus
    published_at: Optional[datetime]
    view_count: int
    like_count: int
    comment_count: int
    created_at: datetime
    is_featured: bool
    is_pinned: bool


# Comment Schemas
class CommentBase(BaseSchema):
    """Base comment schema."""
    content: str = Field(..., min_length=1, max_length=5000)


class CommentCreate(CommentBase):
    """Schema for comment creation."""
    post_id: int = Field(..., gt=0)
    parent_id: Optional[int] = Field(None, gt=0)


class CommentUpdate(BaseSchema):
    """Schema for comment updates."""
    content: Optional[str] = Field(None, min_length=1, max_length=5000)


class CommentResponse(CommentBase, TimestampMixin, UUIDMixin):
    """Comment response schema."""
    id: int
    post_id: int
    author_id: int
    parent_id: Optional[int] = None
    status: CommentStatus
    like_count: int
    depth_level: int
    is_spam: bool
    
    # Related objects
    author: UserSummaryResponse
    replies: List['CommentResponse'] = []
    
    # Moderation info (only for moderators)
    moderated_by_id: Optional[int] = None
    moderated_at: Optional[datetime] = None
    moderation_reason: Optional[str] = None


class CommentModerationUpdate(BaseSchema):
    """Schema for comment moderation."""
    status: CommentStatus
    moderation_reason: Optional[str] = Field(None, max_length=500)


# Filter and Search Schemas
class PostFilterParams(BaseSchema):
    """Post filtering parameters."""
    category_id: Optional[int] = Field(None, gt=0)
    author_id: Optional[int] = Field(None, gt=0)
    status: Optional[PostStatus] = None
    is_featured: Optional[bool] = None
    is_pinned: Optional[bool] = None
    tag_ids: Optional[List[int]] = Field(None, description="Filter by tag IDs")
    published_after: Optional[datetime] = None
    published_before: Optional[datetime] = None
    min_views: Optional[int] = Field(None, ge=0)
    min_likes: Optional[int] = Field(None, ge=0)
    search_query: Optional[str] = Field(None, max_length=200)


class CommentFilterParams(BaseSchema):
    """Comment filtering parameters."""
    post_id: Optional[int] = Field(None, gt=0)
    author_id: Optional[int] = Field(None, gt=0)
    status: Optional[CommentStatus] = None
    parent_id: Optional[int] = Field(None, gt=0)
    is_spam: Optional[bool] = None
    created_after: Optional[datetime] = None
    created_before: Optional[datetime] = None


# Analytics Schemas
class PostAnalytics(BaseSchema):
    """Post analytics response."""
    post_id: int
    total_views: int
    unique_views: int
    avg_read_time: float
    bounce_rate: float
    top_referrers: List[Dict[str, Any]]
    geographic_data: Dict[str, int]
    hourly_views: List[Dict[str, Any]]


class BlogStatistics(BaseSchema):
    """Overall blog statistics."""
    total_posts: int
    published_posts: int
    draft_posts: int
    total_comments: int
    approved_comments: int
    pending_comments: int
    total_views: int
    total_likes: int
    top_categories: List[Dict[str, Any]]
    top_tags: List[Dict[str, Any]]
    recent_activity: List[Dict[str, Any]]


# Paginated Responses
PostsPaginatedResponse = PaginatedResponse[PostSummaryResponse]
PostsDetailPaginatedResponse = PaginatedResponse[PostResponse]
CommentsPaginatedResponse = PaginatedResponse[CommentResponse]
CategoriesPaginatedResponse = PaginatedResponse[CategoryResponse]
TagsPaginatedResponse = PaginatedResponse[TagResponse]
```

**app/crud/blog.py** - Complete Blog CRUD Operations:
```python
"""
Complete Blog CRUD operations for posts, comments, categories, and tags.
Demonstrates advanced querying, full-text search, and content management.
"""

import logging
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Union, Tuple

from sqlalchemy import and_, or_, func, desc, asc, text, case
from sqlalchemy.orm import Session, joinedload, selectinload, contains_eager
from sqlalchemy.exc import IntegrityError

from app.crud.base import CRUDBase, CRUDWithSoftDelete
from app.models.blog import (
    Post, Comment, Category, Tag, PostLike, PostView, PostRevision,
    PostStatus, CommentStatus, post_tags
)
from app.models.user import User, UserRole
from app.schemas.blog import (
    PostCreate, PostUpdate, CommentCreate, CommentUpdate,
    CategoryCreate, CategoryUpdate, TagCreate, TagUpdate,
    PostFilterParams, CommentFilterParams
)
from app.schemas.common import PaginationParams

logger = logging.getLogger(__name__)


class CategoryCRUD(CRUDWithSoftDelete[Category, CategoryCreate, CategoryUpdate]):
    """Category CRUD operations with hierarchical support."""
    
    def get_root_categories(self, db: Session) -> List[Category]:
        """Get all root categories (no parent)."""
        return db.query(Category).filter(
            and_(
                Category.parent_id.is_(None),
                Category.is_visible == True,
                Category.is_deleted == False
            )
        ).order_by(Category.sort_order, Category.name).all()
    
    def get_category_tree(self, db: Session) -> List[Dict[str, Any]]:
        """Get complete category tree with hierarchy."""
        root_categories = self.get_root_categories(db)
        
        def build_tree(categories: List[Category]) -> List[Dict[str, Any]]:
            result = []
            for category in categories:
                children = db.query(Category).filter(
                    and_(
                        Category.parent_id == category.id,
                        Category.is_visible == True,
                        Category.is_deleted == False
                    )
                ).order_by(Category.sort_order, Category.name).all()
                
                result.append({
                    "id": category.id,
                    "name": category.name,
                    "slug": category.slug,
                    "description": category.description,
                    "posts_count": category.posts_count,
                    "children": build_tree(children) if children else []
                })
            return result
        
        return build_tree(root_categories)
    
    def get_category_with_ancestors(self, db: Session, category_id: int) -> Dict[str, Any]:
        """Get category with full ancestor chain."""
        category = db.query(Category).filter(Category.id == category_id).first()
        if not category:
            return None
        
        ancestors = []
        current = category
        while current.parent_id:
            parent = db.query(Category).filter(Category.id == current.parent_id).first()
            if parent:
                ancestors.insert(0, {
                    "id": parent.id,
                    "name": parent.name,
                    "slug": parent.slug
                })
                current = parent
            else:
                break
        
        return {
            "category": category,
            "ancestors": ancestors,
            "full_path": " > ".join([a["name"] for a in ancestors] + [category.name])
        }
    
    def update_posts_count(self, db: Session, category_id: int) -> None:
        """Update denormalized posts count for category."""
        count = db.query(func.count(Post.id)).filter(
            and_(
                Post.category_id == category_id,
                Post.status == PostStatus.PUBLISHED,
                Post.is_deleted == False
            )
        ).scalar()
        
        db.query(Category).filter(Category.id == category_id).update({
            "posts_count": count
        })
        db.commit()


class TagCRUD(CRUDWithSoftDelete[Tag, TagCreate, TagUpdate]):
    """Tag CRUD operations with usage tracking."""
    
    def get_popular_tags(self, db: Session, limit: int = 20) -> List[Tag]:
        """Get most popular tags by usage count."""
        return db.query(Tag).filter(
            Tag.is_deleted == False
        ).order_by(desc(Tag.usage_count)).limit(limit).all()
    
    def get_trending_tags(self, db: Session, days: int = 30, limit: int = 10) -> List[Dict[str, Any]]:
        """Get trending tags based on recent usage."""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        # Get tags with recent post associations
        trending = db.query(
            Tag.id,
            Tag.name,
            Tag.color,
            func.count(post_tags.c.post_id).label('recent_usage')
        ).join(
            post_tags, Tag.id == post_tags.c.tag_id
        ).join(
            Post, post_tags.c.post_id == Post.id
        ).filter(
            and_(
                Post.published_at >= cutoff_date,
                Post.status == PostStatus.PUBLISHED,
                Tag.is_deleted == False
            )
        ).group_by(
            Tag.id, Tag.name, Tag.color
        ).order_by(
            desc(func.count(post_tags.c.post_id))
        ).limit(limit).all()
        
        return [
            {
                "id": row.id,
                "name": row.name,
                "color": row.color,
                "recent_usage": row.recent_usage
            }
            for row in trending
        ]
    
    def search_tags(self, db: Session, query: str, limit: int = 10) -> List[Tag]:
        """Search tags by name or description."""
        search_filter = or_(
            Tag.name.ilike(f"%{query}%"),
            Tag.description.ilike(f"%{query}%")
        )
        
        return db.query(Tag).filter(
            and_(
                search_filter,
                Tag.is_deleted == False
            )
        ).order_by(
            # Prioritize exact matches
            case(
                (Tag.name.ilike(query), 1),
                (Tag.name.ilike(f"{query}%"), 2),
                else_=3
            ),
            desc(Tag.usage_count)
        ).limit(limit).all()
    
    def update_usage_count(self, db: Session, tag_id: int) -> None:
        """Update tag usage count."""
        count = db.query(func.count(post_tags.c.post_id)).filter(
            post_tags.c.tag_id == tag_id
        ).scalar()
        
        db.query(Tag).filter(Tag.id == tag_id).update({
            "usage_count": count
        })


class PostCRUD(CRUDWithSoftDelete[Post, PostCreate, PostUpdate]):
    """Post CRUD operations with advanced content management."""
    
    def create_with_tags(self, db: Session, obj_in: PostCreate, author_id: int) -> Post:
        """Create post with associated tags."""
        # Extract tags from create data
        tag_ids = obj_in.tag_ids if hasattr(obj_in, 'tag_ids') else []
        
        # Create post data without tags
        post_data = obj_in.dict(exclude={'tag_ids'})
        post_data['author_id'] = author_id
        post_data['word_count'] = len(post_data['content'].split())
        post_data['read_time_minutes'] = max(1, post_data['word_count'] // 200)
        
        # Create post
        db_post = Post(**post_data)
        db.add(db_post)
        db.flush()  # Get the ID
        
        # Associate tags
        if tag_ids:
            tags = db.query(Tag).filter(Tag.id.in_(tag_ids)).all()
            db_post.tags.extend(tags)
            
            # Update tag usage counts
            for tag in tags:
                tag.increment_usage()
        
        # Create first revision
        self.create_revision(db, db_post, "Initial creation")
        
        db.commit()
        db.refresh(db_post)
        return db_post
    
    def update_with_tags(self, db: Session, post_id: int, obj_in: PostUpdate, 
                        updated_by_id: int) -> Optional[Post]:
        """Update post with tag management and revision tracking."""
        post = db.query(Post).filter(Post.id == post_id).first()
        if not post:
            return None
        
        # Store original content for revision
        original_title = post.title
        original_content = post.content
        
        # Extract and process update data
        update_data = obj_in.dict(exclude_unset=True, exclude={'tag_ids'})
        
        # Update word count if content changed
        if 'content' in update_data:
            update_data['word_count'] = len(update_data['content'].split())
            update_data['read_time_minutes'] = max(1, update_data['word_count'] // 200)
        
        # Apply updates
        for field, value in update_data.items():
            setattr(post, field, value)
        
        post.updated_by_id = updated_by_id
        post.version += 1
        
        # Handle tag updates
        if hasattr(obj_in, 'tag_ids') and obj_in.tag_ids is not None:
            # Clear existing tags
            post.tags.clear()
            
            # Add new tags
            if obj_in.tag_ids:
                new_tags = db.query(Tag).filter(Tag.id.in_(obj_in.tag_ids)).all()
                post.tags.extend(new_tags)
                
                # Update usage counts
                for tag in new_tags:
                    tag.increment_usage()
        
        # Create revision if content changed
        if 'title' in update_data or 'content' in update_data:
            change_summary = self._build_change_summary(original_title, original_content, post)
            self.create_revision(db, post, change_summary)
        
        db.commit()
        db.refresh(post)
        return post
    
    def create_revision(self, db: Session, post: Post, change_summary: str) -> None:
        """Create a revision entry for post history."""
        latest_revision = db.query(PostRevision).filter(
            PostRevision.post_id == post.id
        ).order_by(desc(PostRevision.revision_number)).first()
        
        next_revision_number = (latest_revision.revision_number + 1) if latest_revision else 1
        
        revision = PostRevision(
            post_id=post.id,
            revision_number=next_revision_number,
            title=post.title,
            content=post.content,
            excerpt=post.excerpt,
            change_summary=change_summary,
            word_count=post.word_count
        )
        db.add(revision)
    
    def get_with_relationships(self, db: Session, post_id: int, 
                             include_comments: bool = False) -> Optional[Post]:
        """Get post with all relationships loaded."""
        query = db.query(Post).options(
            joinedload(Post.author),
            joinedload(Post.category),
            selectinload(Post.tags)
        )
        
        if include_comments:
            query = query.options(
                selectinload(Post.comments).joinedload(Comment.author)
            )
        
        return query.filter(Post.id == post_id).first()
    
    def get_published_posts(self, db: Session, filters: PostFilterParams, 
                           pagination: PaginationParams) -> Tuple[List[Post], int]:
        """Get published posts with advanced filtering."""
        query = db.query(Post).filter(
            and_(
                Post.status == PostStatus.PUBLISHED,
                Post.is_deleted == False
            )
        )
        
        # Apply filters
        query = self._apply_post_filters(query, filters)
        
        # Get total count
        total = query.count()
        
        # Apply sorting
        if hasattr(filters, 'sort_by'):
            query = self._apply_post_sorting(query, filters.sort_by)
        else:
            # Default sorting: pinned first, then by publication date
            query = query.order_by(
                desc(Post.is_pinned),
                desc(Post.published_at)
            )
        
        # Apply pagination
        posts = query.offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return posts, total
    
    def search_posts(self, db: Session, search_query: str, 
                    pagination: PaginationParams) -> Tuple[List[Post], int]:
        """Full-text search across posts."""
        # PostgreSQL full-text search
        search_vector_query = func.to_tsquery('english', search_query)
        
        query = db.query(Post).filter(
            and_(
                Post.search_vector.op('@@')(search_vector_query),
                Post.status == PostStatus.PUBLISHED,
                Post.is_deleted == False
            )
        ).order_by(
            # Order by relevance (ts_rank)
            desc(func.ts_rank(Post.search_vector, search_vector_query))
        )
        
        total = query.count()
        posts = query.offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return posts, total
    
    def get_related_posts(self, db: Session, post_id: int, limit: int = 5) -> List[Post]:
        """Get related posts based on tags and category."""
        post = db.query(Post).filter(Post.id == post_id).first()
        if not post:
            return []
        
        # Get post tag IDs
        post_tag_ids = [tag.id for tag in post.tags]
        
        if not post_tag_ids:
            # If no tags, use same category
            return db.query(Post).filter(
                and_(
                    Post.category_id == post.category_id,
                    Post.id != post_id,
                    Post.status == PostStatus.PUBLISHED,
                    Post.is_deleted == False
                )
            ).order_by(desc(Post.published_at)).limit(limit).all()
        
        # Find posts with similar tags
        related_query = db.query(
            Post,
            func.count(post_tags.c.tag_id).label('common_tags')
        ).join(
            post_tags, Post.id == post_tags.c.post_id
        ).filter(
            and_(
                post_tags.c.tag_id.in_(post_tag_ids),
                Post.id != post_id,
                Post.status == PostStatus.PUBLISHED,
                Post.is_deleted == False
            )
        ).group_by(Post.id).order_by(
            desc(func.count(post_tags.c.tag_id)),  # Most common tags first
            desc(Post.published_at)
        ).limit(limit)
        
        return [row.Post for row in related_query.all()]
    
    def publish_post(self, db: Session, post_id: int, publisher_id: int) -> Optional[Post]:
        """Publish a post with all necessary updates."""
        post = db.query(Post).filter(Post.id == post_id).first()
        if not post:
            return None
        
        if post.status == PostStatus.PUBLISHED:
            return post  # Already published
        
        # Update post status
        post.status = PostStatus.PUBLISHED
        post.published_at = datetime.utcnow()
        post.updated_by_id = publisher_id
        post.version += 1
        
        # Update category posts count
        if post.category_id:
            db.query(Category).filter(Category.id == post.category_id).update({
                "posts_count": Category.posts_count + 1
            })
        
        # Update author's published posts count
        db.query(User).filter(User.id == post.author_id).update({
            "posts_count": User.posts_count + 1
        })
        
        db.commit()
        db.refresh(post)
        return post
    
    def get_post_analytics(self, db: Session, post_id: int, days: int = 30) -> Dict[str, Any]:
        """Get comprehensive post analytics."""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        # Basic metrics
        post = db.query(Post).filter(Post.id == post_id).first()
        if not post:
            return {}
        
        # View analytics
        view_stats = db.query(
            func.count(PostView.id).label('total_views'),
            func.count(func.distinct(PostView.user_id)).label('unique_users'),
            func.count(func.distinct(PostView.ip_address)).label('unique_ips'),
            func.avg(PostView.duration_seconds).label('avg_duration')
        ).filter(
            and_(
                PostView.post_id == post_id,
                PostView.created_at >= cutoff_date
            )
        ).first()
        
        # Hourly view distribution
        hourly_views = db.query(
            func.extract('hour', PostView.created_at).label('hour'),
            func.count(PostView.id).label('views')
        ).filter(
            and_(
                PostView.post_id == post_id,
                PostView.created_at >= cutoff_date
            )
        ).group_by(
            func.extract('hour', PostView.created_at)
        ).order_by('hour').all()
        
        # Geographic distribution
        geo_data = db.query(
            PostView.country,
            func.count(PostView.id).label('views')
        ).filter(
            and_(
                PostView.post_id == post_id,
                PostView.created_at >= cutoff_date,
                PostView.country.isnot(None)
            )
        ).group_by(PostView.country).order_by(
            desc(func.count(PostView.id))
        ).limit(10).all()
        
        # Referrer data
        referrers = db.query(
            PostView.referrer,
            func.count(PostView.id).label('views')
        ).filter(
            and_(
                PostView.post_id == post_id,
                PostView.created_at >= cutoff_date,
                PostView.referrer.isnot(None)
            )
        ).group_by(PostView.referrer).order_by(
            desc(func.count(PostView.id))
        ).limit(10).all()
        
        return {
            "post_id": post_id,
            "period_days": days,
            "total_views": view_stats.total_views or 0,
            "unique_users": view_stats.unique_users or 0,
            "unique_visitors": view_stats.unique_ips or 0,
            "avg_read_time_seconds": int(view_stats.avg_duration or 0),
            "hourly_distribution": [
                {"hour": int(row.hour), "views": row.views}
                for row in hourly_views
            ],
            "geographic_data": {
                row.country: row.views for row in geo_data
            },
            "top_referrers": [
                {"referrer": row.referrer, "views": row.views}
                for row in referrers
            ]
        }
    
    def _apply_post_filters(self, query, filters: PostFilterParams):
        """Apply filtering to post query."""
        if filters.category_id:
            query = query.filter(Post.category_id == filters.category_id)
        
        if filters.author_id:
            query = query.filter(Post.author_id == filters.author_id)
        
        if filters.status:
            query = query.filter(Post.status == filters.status)
        
        if filters.is_featured is not None:
            query = query.filter(Post.is_featured == filters.is_featured)
        
        if filters.is_pinned is not None:
            query = query.filter(Post.is_pinned == filters.is_pinned)
        
        if filters.published_after:
            query = query.filter(Post.published_at >= filters.published_after)
        
        if filters.published_before:
            query = query.filter(Post.published_at <= filters.published_before)
        
        if filters.min_views:
            query = query.filter(Post.view_count >= filters.min_views)
        
        if filters.min_likes:
            query = query.filter(Post.like_count >= filters.min_likes)
        
        if filters.tag_ids:
            # Posts that have ANY of the specified tags
            query = query.join(post_tags).filter(
                post_tags.c.tag_id.in_(filters.tag_ids)
            ).distinct()
        
        if filters.search_query:
            # Simple text search across title and content
            search_filter = or_(
                Post.title.ilike(f"%{filters.search_query}%"),
                Post.content.ilike(f"%{filters.search_query}%"),
                Post.excerpt.ilike(f"%{filters.search_query}%")
            )
            query = query.filter(search_filter)
        
        return query
    
    def _apply_post_sorting(self, query, sort_by: str):
        """Apply sorting to post query."""
        sort_options = {
            'created_at_desc': desc(Post.created_at),
            'created_at_asc': asc(Post.created_at),
            'published_at_desc': desc(Post.published_at),
            'published_at_asc': asc(Post.published_at),
            'title_asc': asc(Post.title),
            'title_desc': desc(Post.title),
            'view_count_desc': desc(Post.view_count),
            'like_count_desc': desc(Post.like_count),
            'comment_count_desc': desc(Post.comment_count),
            'engagement_desc': desc(Post.like_count + Post.comment_count * 2),
        }
        
        if sort_by in sort_options:
            return query.order_by(sort_options[sort_by])
        
        # Default sorting
        return query.order_by(desc(Post.published_at))
    
    def record_view(self, db: Session, post_id: int, user_id: Optional[int] = None,
                   ip_address: str = None, user_agent: str = None,
                   referrer: str = None) -> None:
        """Record a post view for analytics."""
        # Check if this user/IP already viewed recently (prevent spam)
        recent_cutoff = datetime.utcnow() - timedelta(hours=1)
        
        existing_view = None
        if user_id:
            existing_view = db.query(PostView).filter(
                and_(
                    PostView.post_id == post_id,
                    PostView.user_id == user_id,
                    PostView.created_at >= recent_cutoff
                )
            ).first()
        elif ip_address:
            existing_view = db.query(PostView).filter(
                and_(
                    PostView.post_id == post_id,
                    PostView.ip_address == ip_address,
                    PostView.created_at >= recent_cutoff
                )
            ).first()
        
        if not existing_view:
            # Create new view record
            view = PostView(
                post_id=post_id,
                user_id=user_id,
                ip_address=ip_address,
                user_agent=user_agent,
                referrer=referrer
            )
            db.add(view)
            
            # Update post view count
            db.query(Post).filter(Post.id == post_id).update({
                "view_count": Post.view_count + 1
            })
            
            db.commit()
    
    def toggle_like(self, db: Session, post_id: int, user_id: int) -> Dict[str, Any]:
        """Toggle like status for a post."""
        existing_like = db.query(PostLike).filter(
            and_(
                PostLike.post_id == post_id,
                PostLike.user_id == user_id
            )
        ).first()
        
        if existing_like:
            # Unlike
            db.delete(existing_like)
            db.query(Post).filter(Post.id == post_id).update({
                "like_count": Post.like_count - 1
            })
            action = "unliked"
        else:
            # Like
            like = PostLike(post_id=post_id, user_id=user_id)
            db.add(like)
            db.query(Post).filter(Post.id == post_id).update({
                "like_count": Post.like_count + 1
            })
            action = "liked"
        
        db.commit()
        
        # Get updated like count
        updated_post = db.query(Post).filter(Post.id == post_id).first()
        
        return {
            "action": action,
            "like_count": updated_post.like_count if updated_post else 0,
            "user_liked": action == "liked"
        }
    
    def _build_change_summary(self, old_title: str, old_content: str, new_post: Post) -> str:
        """Build a summary of changes for revision tracking."""
        changes = []
        
        if old_title != new_post.title:
            changes.append(f"Title changed from '{old_title}' to '{new_post.title}'")
        
        if old_content != new_post.content:
            old_word_count = len(old_content.split())
            new_word_count = new_post.word_count
            word_diff = new_word_count - old_word_count
            
            if word_diff > 0:
                changes.append(f"Content expanded by {word_diff} words")
            elif word_diff < 0:
                changes.append(f"Content reduced by {abs(word_diff)} words")
            else:
                changes.append("Content modified")
        
        return "; ".join(changes) if changes else "Minor updates"


class CommentCRUD(CRUDWithSoftDelete[Comment, CommentCreate, CommentUpdate]):
    """Comment CRUD operations with threading and moderation."""
    
    def create_comment(self, db: Session, obj_in: CommentCreate, author_id: int) -> Comment:
        """Create comment with thread validation."""
        # Validate parent comment exists and belongs to same post
        if obj_in.parent_id:
            parent = db.query(Comment).filter(Comment.id == obj_in.parent_id).first()
            if not parent or parent.post_id != obj_in.post_id:
                raise ValueError("Invalid parent comment")
            
            # Check threading depth (limit to 5 levels)
            depth = self._calculate_comment_depth(db, parent)
            if depth >= 5:
                raise ValueError("Maximum comment thread depth exceeded")
        
        # Create comment
        comment_data = obj_in.dict()
        comment_data['author_id'] = author_id
        
        comment = Comment(**comment_data)
        db.add(comment)
        db.flush()
        
        # Update post comment count
        db.query(Post).filter(Post.id == obj_in.post_id).update({
            "comment_count": Post.comment_count + 1
        })
        
        db.commit()
        db.refresh(comment)
        return comment
    
    def get_post_comments_threaded(self, db: Session, post_id: int, 
                                  include_pending: bool = False) -> List[Comment]:
        """Get post comments in threaded structure."""
        # Base query
        query = db.query(Comment).options(
            joinedload(Comment.author)
        ).filter(Comment.post_id == post_id)
        
        if not include_pending:
            query = query.filter(Comment.status == CommentStatus.APPROVED)
        
        # Get all comments for the post
        all_comments = query.order_by(Comment.created_at).all()
        
        # Build threaded structure
        comments_dict = {comment.id: comment for comment in all_comments}
        root_comments = []
        
        for comment in all_comments:
            if comment.parent_id is None:
                root_comments.append(comment)
            else:
                parent = comments_dict.get(comment.parent_id)
                if parent:
                    if not hasattr(parent, 'threaded_replies'):
                        parent.threaded_replies = []
                    parent.threaded_replies.append(comment)
        
        return root_comments
    
    def moderate_comment(self, db: Session, comment_id: int, status: CommentStatus,
                        moderator_id: int, reason: str = None) -> Optional[Comment]:
        """Moderate comment with audit trail."""
        comment = db.query(Comment).filter(Comment.id == comment_id).first()
        if not comment:
            return None
        
        old_status = comment.status
        comment.status = status
        comment.moderated_by_id = moderator_id
        comment.moderated_at = datetime.utcnow()
        comment.moderation_reason = reason
        
        # Update post comment count if approval status changed
        if old_status != CommentStatus.APPROVED and status == CommentStatus.APPROVED:
            # Comment was approved
            db.query(Post).filter(Post.id == comment.post_id).update({
                "comment_count": Post.comment_count + 1
            })
        elif old_status == CommentStatus.APPROVED and status != CommentStatus.APPROVED:
            # Comment was disapproved
            db.query(Post).filter(Post.id == comment.post_id).update({
                "comment_count": Post.comment_count - 1
            })
        
        db.commit()
        db.refresh(comment)
        return comment
    
    def get_comments_for_moderation(self, db: Session, 
                                   pagination: PaginationParams) -> Tuple[List[Comment], int]:
        """Get comments pending moderation."""
        query = db.query(Comment).options(
            joinedload(Comment.author),
            joinedload(Comment.post)
        ).filter(
            Comment.status == CommentStatus.PENDING
        ).order_by(desc(Comment.created_at))
        
        total = query.count()
        comments = query.offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return comments, total
    
    def get_user_comment_stats(self, db: Session, user_id: int) -> Dict[str, Any]:
        """Get comment statistics for a user."""
        stats = db.query(
            func.count(Comment.id).label('total_comments'),
            func.count(case(
                (Comment.status == CommentStatus.APPROVED, 1)
            )).label('approved_comments'),
            func.count(case(
                (Comment.status == CommentStatus.REJECTED, 1)
            )).label('rejected_comments'),
            func.sum(Comment.like_count).label('total_likes')
        ).filter(Comment.author_id == user_id).first()
        
        return {
            "total_comments": stats.total_comments or 0,
            "approved_comments": stats.approved_comments or 0,
            "rejected_comments": stats.rejected_comments or 0,
            "total_likes_received": stats.total_likes or 0,
            "approval_rate": (
                (stats.approved_comments / stats.total_comments * 100)
                if stats.total_comments > 0 else 0
            )
        }
    
    def _calculate_comment_depth(self, db: Session, comment: Comment) -> int:
        """Calculate the depth of a comment in the thread."""
        depth = 0
        current = comment
        
        while current.parent_id:
            depth += 1
            current = db.query(Comment).filter(Comment.id == current.parent_id).first()
            if not current:
                break
            
            # Safety check to prevent infinite loops
            if depth > 10:
                break
        
        return depth


# Content Management Utilities
class ContentManager:
    """Advanced content management utilities."""
    
    @staticmethod
    def schedule_post_publication(db: Session, post_id: int, 
                                 scheduled_time: datetime) -> bool:
        """Schedule a post for future publication."""
        post = db.query(Post).filter(Post.id == post_id).first()
        if not post:
            return False
        
        post.status = PostStatus.SCHEDULED
        post.scheduled_for = scheduled_time
        db.commit()
        return True
    
    @staticmethod
    def get_scheduled_posts(db: Session) -> List[Post]:
        """Get posts scheduled for publication."""
        now = datetime.utcnow()
        return db.query(Post).filter(
            and_(
                Post.status == PostStatus.SCHEDULED,
                Post.scheduled_for <= now
            )
        ).all()
    
    @staticmethod
    def publish_scheduled_posts(db: Session) -> int:
        """Publish all posts scheduled for now or earlier."""
        scheduled_posts = ContentManager.get_scheduled_posts(db)
        published_count = 0
        
        for post in scheduled_posts:
            post.status = PostStatus.PUBLISHED
            post.published_at = datetime.utcnow()
            post.scheduled_for = None
            published_count += 1
        
        db.commit()
        return published_count
    
    @staticmethod
    def get_content_statistics(db: Session) -> Dict[str, Any]:
        """Get comprehensive content statistics."""
        # Post statistics
        post_stats = db.query(
            func.count(Post.id).label('total_posts'),
            func.count(case((Post.status == PostStatus.PUBLISHED, 1))).label('published'),
            func.count(case((Post.status == PostStatus.DRAFT, 1))).label('drafts'),
            func.count(case((Post.status == PostStatus.SCHEDULED, 1))).label('scheduled'),
            func.sum(Post.view_count).label('total_views'),
            func.sum(Post.like_count).label('total_likes'),
            func.sum(Post.comment_count).label('total_comments')
        ).filter(Post.is_deleted == False).first()
        
        # Comment moderation stats
        comment_stats = db.query(
            func.count(Comment.id).label('total_comments'),
            func.count(case((Comment.status == CommentStatus.APPROVED, 1))).label('approved'),
            func.count(case((Comment.status == CommentStatus.PENDING, 1))).label('pending'),
            func.count(case((Comment.status == CommentStatus.REJECTED, 1))).label('rejected'),
            func.count(case((Comment.is_spam == True, 1))).label('spam')
        ).filter(Comment.is_deleted == False).first()
        
        # Top categories
        top_categories = db.query(
            Category.name,
            Category.posts_count
        ).filter(
            Category.is_deleted == False
        ).order_by(desc(Category.posts_count)).limit(10).all()
        
        return {
            "posts": {
                "total": post_stats.total_posts or 0,
                "published": post_stats.published or 0,
                "drafts": post_stats.drafts or 0,
                "scheduled": post_stats.scheduled or 0,
                "total_views": post_stats.total_views or 0,
                "total_likes": post_stats.total_likes or 0,
                "total_comments": post_stats.total_comments or 0
            },
            "comments": {
                "total": comment_stats.total_comments or 0,
                "approved": comment_stats.approved or 0,
                "pending_moderation": comment_stats.pending or 0,
                "rejected": comment_stats.rejected or 0,
                "spam": comment_stats.spam or 0
            },
            "top_categories": [
                {"name": cat.name, "posts_count": cat.posts_count}
                for cat in top_categories
            ]
        }


# Create CRUD instances
category_crud = CategoryCRUD(Category)
tag_crud = TagCRUD(Tag)
post_crud = PostCRUD(Post)
comment_crud = CommentCRUD(Comment)
content_manager = ContentManager()
```

### 3.6 FastAPI Endpoints Implementation

**app/api/deps.py** - API Dependencies:
```python
"""
FastAPI dependencies for authentication, authorization, and database access.
Includes JWT token validation, role-based access control, and request parsing.
"""

from typing import Generator, Optional

from fastapi import Depends, HTTPException, status, Query
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.orm import Session

from app.database import get_db
from app.core.security import JWTManager, AuthenticationService
from app.models.user import User, UserRole, UserStatus
from app.crud.user import user_crud
from app.schemas.common import PaginationParams, FilterParams

# Security scheme
security = HTTPBearer()


def get_current_user(
    db: Session = Depends(get_db),
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> User:
    """Get current authenticated user from JWT token."""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        # Verify JWT token
        payload = JWTManager.verify_token(credentials.credentials)
        if payload is None:
            raise credentials_exception
        
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except Exception:
        raise credentials_exception
    
    # Get user from database
    user = user_crud.get_by_username(db, username=username)
    if user is None:
        raise credentials_exception
    
    # Check user status
    if user.status != UserStatus.ACTIVE:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="User account is not active"
        )
    
    return user


def get_current_active_user(
    current_user: User = Depends(get_current_user)
) -> User:
    """Get current active user (additional validation)."""
    if not current_user.is_active:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Inactive user"
        )
    return current_user


def get_current_admin(
    current_user: User = Depends(get_current_active_user)
) -> User:
    """Require admin role."""
    if current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )
    return current_user


def get_current_author_or_above(
    current_user: User = Depends(get_current_active_user)
) -> User:
    """Require author role or above."""
    if not current_user.is_author_or_above:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Author permissions required"
        )
    return current_user


def get_current_moderator_or_above(
    current_user: User = Depends(get_current_active_user)
) -> User:
    """Require moderator role or above."""
    if current_user.role not in [UserRole.ADMIN, UserRole.MODERATOR]:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Moderator permissions required"
        )
    return current_user


def get_pagination_params(
    page: int = Query(1, ge=1, description="Page number"),
    size: int = Query(20, ge=1, le=100, description="Page size")
) -> PaginationParams:
    """Get pagination parameters from query string."""
    return PaginationParams(page=page, size=size)


def get_optional_user(
    db: Session = Depends(get_db),
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(HTTPBearer(auto_error=False))
) -> Optional[User]:
    """Get current user if authenticated, otherwise None."""
    if not credentials:
        return None
    
    try:
        payload = JWTManager.verify_token(credentials.credentials)
        if payload is None:
            return None
        
        username: str = payload.get("sub")
        if username is None:
            return None
        
        user = user_crud.get_by_username(db, username=username)
        if user and user.status == UserStatus.ACTIVE and user.is_active:
            return user
    except Exception:
        pass
    
    return None
```

**app/api/v1/auth.py** - Authentication Endpoints:
```python
"""
Authentication endpoints for user login, registration, and token management.
Demonstrates JWT authentication, password reset, and email verification flows.
"""

from datetime import timedelta

from fastapi import APIRouter, Depends, HTTPException, status, Request
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session

from app.database import get_db
from app.core.security import AuthenticationService, JWTManager, SecurityManager
from app.crud.user import user_crud
from app.crud.session import session_crud
from app.schemas.user import (
    UserCreate, UserLogin, TokenResponse, TokenRefresh,
    PasswordResetRequest, PasswordReset, EmailVerification,
    UserResponse
)
from app.schemas.common import APIResponse
from app.models.user import User

router = APIRouter(prefix="/auth", tags=["authentication"])


@router.post("/register", response_model=APIResponse[UserResponse])
async def register_user(
    user_data: UserCreate,
    request: Request,
    db: Session = Depends(get_db)
):
    """
    Register a new user account.
    
    - Creates new user with email verification requirement
    - Sends verification email (in production)
    - Returns user data without sensitive information
    """
    # Check if user already exists
    existing_user = user_crud.get_by_email(db, email=user_data.email)
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="User with this email already exists"
        )
    
    existing_username = user_crud.get_by_username(db, username=user_data.username)
    if existing_username:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username already taken"
        )
    
    # Create user
    auth_service = AuthenticationService(db)
    try:
        user = auth_service.register_user(
            username=user_data.username,
            email=user_data.email,
            full_name=user_data.full_name,
            password=user_data.password
        )
        
        # TODO: Send verification email in production
        # email_service.send_verification_email(user)
        
        return APIResponse(
            success=True,
            message="User registered successfully. Please check your email for verification.",
            data=UserResponse.from_orm(user)
        )
    
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.post("/login", response_model=TokenResponse)
async def login_user(
    form_data: OAuth2PasswordRequestForm = Depends(),
    request: Request,
    db: Session = Depends(get_db)
):
    """
    User login with username/email and password.
    
    - Authenticates user credentials
    - Creates JWT access and refresh tokens
    - Records login session for security tracking
    - Returns tokens and user information
    """
    auth_service = AuthenticationService(db)
    
    # Get client IP address
    client_ip = request.client.host
    
    # Authenticate user
    user = auth_service.authenticate_user(
        username_or_email=form_data.username,
        password=form_data.password,
        ip_address=client_ip
    )
    
    if not user:
        # Log failed login attempt
        logger.warning(f"Failed login attempt for {form_data.username} from {client_ip}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username/email or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Create tokens
    tokens = auth_service.create_user_tokens(user)
    
    # Create session record
    session_crud.create_session(
        db=db,
        user_id=user.id,
        ip_address=client_ip,
        user_agent=request.headers.get("user-agent"),
        session_token=tokens["access_token"][:50]  # Truncated for storage
    )
    
    return TokenResponse(
        access_token=tokens["access_token"],
        refresh_token=tokens["refresh_token"],
        token_type="bearer",
        expires_in=3600,  # 1 hour
        user=UserResponse.from_orm(user)
    )


@router.post("/refresh", response_model=APIResponse[TokenResponse])
async def refresh_token(
    refresh_data: TokenRefresh,
    db: Session = Depends(get_db)
):
    """
    Refresh access token using refresh token.
    
    - Validates refresh token
    - Creates new access token
    - Optionally rotates refresh token
    """
    try:
        new_access_token = JWTManager.refresh_access_token(refresh_data.refresh_token)
        
        if not new_access_token:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid refresh token"
            )
        
        # Get user info from token
        payload = JWTManager.verify_token(new_access_token)
        username = payload.get("sub")
        user = user_crud.get_by_username(db, username=username)
        
        return APIResponse(
            success=True,
            data=TokenResponse(
                access_token=new_access_token,
                refresh_token=refresh_data.refresh_token,  # Keep same refresh token
                token_type="bearer",
                expires_in=3600,
                user=UserResponse.from_orm(user)
            )
        )
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not refresh token"
        )


@router.post("/logout")
async def logout_user(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    User logout - invalidate current session.
    
    - Invalidates current user session
    - Logs logout activity
    """
    # Invalidate all active sessions for the user
    session_crud.deactivate_user_sessions(db, user_id=current_user.id)
    
    return APIResponse(
        success=True,
        message="Successfully logged out"
    )


@router.post("/password-reset-request")
async def request_password_reset(
    reset_request: PasswordResetRequest,
    db: Session = Depends(get_db)
):
    """
    Request password reset via email.
    
    - Validates email exists
    - Generates secure reset token
    - Sends reset email (in production)
    """
    auth_service = AuthenticationService(db)
    
    reset_token = auth_service.initiate_password_reset(reset_request.email)
    
    if reset_token:
        # TODO: Send password reset email in production
        # email_service.send_password_reset_email(user, reset_token)
        pass
    
    # Always return success to prevent email enumeration
    return APIResponse(
        success=True,
        message="If the email exists, a password reset link has been sent."
    )


@router.post("/password-reset")
async def reset_password(
    reset_data: PasswordReset,
    db: Session = Depends(get_db)
):
    """
    Reset user password with token.
    
    - Validates reset token
    - Updates password with new hash
    - Invalidates all user sessions
    """
    auth_service = AuthenticationService(db)
    
    success = auth_service.reset_password(
        token=reset_data.token,
        new_password=reset_data.new_password
    )
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid or expired reset token"
        )
    
    return APIResponse(
        success=True,
        message="Password reset successfully"
    )


@router.post("/verify-email")
async def verify_email(
    verification_data: EmailVerification,
    db: Session = Depends(get_db)
):
    """
    Verify user email address.
    
    - Validates verification token
    - Activates user account
    - Updates user status
    """
    auth_service = AuthenticationService(db)
    
    success = auth_service.verify_email(verification_data.token)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid or expired verification token"
        )
    
    return APIResponse(
        success=True,
        message="Email verified successfully"
    )


@router.get("/me", response_model=APIResponse[UserResponse])
async def get_current_user_profile(
    current_user: User = Depends(get_current_user)
):
    """
    Get current user's profile information.
    
    - Returns authenticated user's profile
    - Includes role and status information
    """
    return APIResponse(
        success=True,
        data=UserResponse.from_orm(current_user)
    )
```

**app/api/v1/users.py** - User Management Endpoints:
```python
"""
User management endpoints for profile operations, social features, and admin functions.
Demonstrates role-based access control and complex user operations.
"""

from typing import List

from fastapi import APIRouter, Depends, HTTPException, status, Query, Request
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import (
    get_current_user, get_current_admin, get_current_moderator_or_above,
    get_pagination_params
)
from app.crud.user import user_crud
from app.models.user import User, UserRole
from app.schemas.user import (
    UserResponse, UserUpdate, UserProfileUpdate, UserPasswordChange,
    UserFilterParams, UserFollowersResponse, UserFollowingResponse,
    FollowUserRequest, UserAdminUpdate, UserStatistics,
    UsersPaginatedResponse, UserProfileResponse
)
from app.schemas.common import APIResponse, PaginationParams

router = APIRouter(prefix="/users", tags=["users"])


@router.get("/", response_model=UsersPaginatedResponse)
async def get_users(
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params),
    role: Optional[UserRole] = Query(None, description="Filter by role"),
    search: Optional[str] = Query(None, description="Search users"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    current_user: User = Depends(get_current_user)
):
    """
    Get paginated list of users with filtering.
    
    - Public endpoint with basic user information
    - Supports filtering by role, status, and search
    - Pagination with configurable page sizes
    """
    filters = UserFilterParams(
        role=role,
        is_active=is_active,
        search_query=search
    )
    
    users, total = user_crud.get_multi_filtered(db, filters=filters, pagination=pagination)
    
    return UsersPaginatedResponse(
        items=[UserResponse.from_orm(user) for user in users],
        total=total,
        page=pagination.page,
        size=pagination.size,
        pages=(total + pagination.size - 1) // pagination.size
    )


@router.get("/profile/{user_id}", response_model=APIResponse[UserProfileResponse])
async def get_user_profile(
    user_id: int,
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_user)
):
    """
    Get detailed user profile by ID.
    
    - Returns public profile information
    - Includes social stats (followers, following)
    - Shows relationship with current user if authenticated
    """
    user = user_crud.get(db, id=user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    # Get user with social data
    profile_data = user_crud.get_user_profile(db, user_id)
    
    # Add relationship info if current user is authenticated
    if current_user:
        profile_data["is_following"] = current_user.is_following(user)
        profile_data["is_followed_by"] = user.is_following(current_user)
    
    return APIResponse(
        success=True,
        data=UserProfileResponse(**profile_data)
    )


@router.put("/profile", response_model=APIResponse[UserResponse])
async def update_user_profile(
    profile_data: UserProfileUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update current user's profile information.
    
    - Updates profile fields (bio, avatar, website, etc.)
    - Validates profile data
    - Returns updated user information
    """
    updated_user = user_crud.update(db, db_obj=current_user, obj_in=profile_data)
    
    return APIResponse(
        success=True,
        message="Profile updated successfully",
        data=UserResponse.from_orm(updated_user)
    )


@router.put("/settings", response_model=APIResponse[UserResponse])
async def update_user_settings(
    settings_data: UserUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update user account settings.
    
    - Updates preferences (timezone, language, etc.)
    - Updates posting limits and permissions
    - Admin-only fields are protected
    """
    # Remove admin-only fields if user is not admin
    if current_user.role != UserRole.ADMIN:
        settings_data = settings_data.dict(exclude={'role', 'status', 'max_posts_per_day'})
    
    updated_user = user_crud.update(db, db_obj=current_user, obj_in=settings_data)
    
    return APIResponse(
        success=True,
        message="Settings updated successfully",
        data=UserResponse.from_orm(updated_user)
    )


@router.post("/change-password")
async def change_password(
    password_data: UserPasswordChange,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Change user password.
    
    - Validates current password
    - Updates to new password
    - Invalidates all existing sessions
    """
    # Verify current password
    if not current_user.verify_password(password_data.current_password):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Current password is incorrect"
        )
    
    # Update password
    success = user_crud.change_password(
        db, 
        user_id=current_user.id, 
        new_password=password_data.new_password
    )
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Failed to update password"
        )
    
    return APIResponse(
        success=True,
        message="Password changed successfully. Please log in again."
    )


# Social Features
@router.post("/follow", response_model=APIResponse[dict])
async def follow_user(
    follow_data: FollowUserRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Follow another user.
    
    - Creates follow relationship
    - Updates follower counts
    - Prevents self-following
    """
    if follow_data.user_id == current_user.id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Cannot follow yourself"
        )
    
    target_user = user_crud.get(db, id=follow_data.user_id)
    if not target_user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    success = user_crud.follow_user(db, follower_id=current_user.id, followed_id=follow_data.user_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Already following this user or follow failed"
        )
    
    return APIResponse(
        success=True,
        message=f"Now following {target_user.username}",
        data={"user_id": follow_data.user_id, "action": "followed"}
    )


@router.delete("/follow/{user_id}")
async def unfollow_user(
    user_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Unfollow a user.
    
    - Removes follow relationship
    - Updates follower counts
    """
    success = user_crud.unfollow_user(db, follower_id=current_user.id, followed_id=user_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Not following this user or unfollow failed"
        )
    
    return APIResponse(
        success=True,
        message="Unfollowed successfully",
        data={"user_id": user_id, "action": "unfollowed"}
    )


@router.get("/{user_id}/followers", response_model=UserFollowersResponse)
async def get_user_followers(
    user_id: int,
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params)
):
    """
    Get user's followers with pagination.
    
    - Returns list of users following the specified user
    - Includes follow dates and mutual connections
    """
    followers, total = user_crud.get_user_followers(db, user_id=user_id, pagination=pagination)
    
    return UserFollowersResponse(
        followers=[UserResponse.from_orm(user) for user in followers],
        total_count=total,
        page=pagination.page,
        size=pagination.size
    )


@router.get("/{user_id}/following", response_model=UserFollowingResponse)
async def get_user_following(
    user_id: int,
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params)
):
    """
    Get users that the specified user is following.
    
    - Returns list of users being followed
    - Includes follow dates and activity stats
    """
    following, total = user_crud.get_user_following(db, user_id=user_id, pagination=pagination)
    
    return UserFollowingResponse(
        following=[UserResponse.from_orm(user) for user in following],
        total_count=total,
        page=pagination.page,
        size=pagination.size
    )


# Admin Endpoints
@router.put("/admin/{user_id}", response_model=APIResponse[UserResponse])
async def admin_update_user(
    user_id: int,
    admin_data: UserAdminUpdate,
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Admin-only user updates.
    
    - Update user roles, status, and limits
    - Full admin control over user accounts
    - Audit trail for admin actions
    """
    user = user_crud.get(db, id=user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    # Log admin action
    logger.info(f"Admin {current_admin.username} updating user {user.username}")
    
    updated_user = user_crud.admin_update_user(
        db, 
        user_id=user_id, 
        update_data=admin_data,
        admin_id=current_admin.id
    )
    
    return APIResponse(
        success=True,
        message="User updated by admin",
        data=UserResponse.from_orm(updated_user)
    )


@router.delete("/admin/{user_id}")
async def admin_delete_user(
    user_id: int,
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Admin-only user deletion (soft delete).
    
    - Soft deletes user account
    - Preserves content with anonymization
    - Creates audit trail
    """
    user = user_crud.get(db, id=user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    if user.role == UserRole.ADMIN and current_admin.id != user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Cannot delete another admin user"
        )
    
    success = user_crud.admin_delete_user(db, user_id=user_id, admin_id=current_admin.id)
    
    return APIResponse(
        success=True,
        message=f"User {user.username} has been deleted"
    )


@router.get("/statistics", response_model=APIResponse[UserStatistics])
async def get_user_statistics(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    days: int = Query(30, ge=1, le=365, description="Number of days for statistics")
):
    """
    Get user statistics for admin dashboard.
    
    - Registration trends and analytics
    - User activity metrics
    - Role distribution
    """
    stats = user_crud.get_user_statistics(db, days=days)
    
    return APIResponse(
        success=True,
        data=UserStatistics(**stats)
    )
```

**app/api/v1/blog.py** - Blog Management Endpoints:
```python
"""
Blog management endpoints for posts, comments, categories, and tags.
Demonstrates content management, moderation, and advanced querying.
"""

from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException, status, Query, Request
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import (
    get_current_user, get_current_author_or_above, get_current_moderator_or_above,
    get_pagination_params, get_optional_user
)
from app.crud.blog import post_crud, comment_crud, category_crud, tag_crud, content_manager
from app.models.user import User
from app.schemas.blog import (
    PostCreate, PostUpdate, PostResponse, PostSummaryResponse,
    CommentCreate, CommentUpdate, CommentResponse, CommentModerationUpdate,
    CategoryCreate, CategoryUpdate, CategoryResponse,
    TagCreate, TagUpdate, TagResponse,
    PostFilterParams, CommentFilterParams, PostAnalytics, BlogStatistics,
    PostsPaginatedResponse, CommentsPaginatedResponse
)
from app.schemas.common import APIResponse, PaginationParams

router = APIRouter(prefix="/blog", tags=["blog"])


# Post Endpoints
@router.get("/posts", response_model=PostsPaginatedResponse)
async def get_posts(
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params),
    category_id: Optional[int] = Query(None, gt=0),
    author_id: Optional[int] = Query(None, gt=0),
    tags: Optional[str] = Query(None, description="Comma-separated tag slugs"),
    search: Optional[str] = Query(None, max_length=200),
    featured_only: Optional[bool] = Query(None),
    sort_by: str = Query("published_at_desc", description="Sort posts by field"),
    current_user: Optional[User] = Depends(get_optional_user)
):
    """
    Get published posts with advanced filtering and search.
    
    - Public endpoint showing published posts
    - Full-text search across title and content
    - Filter by category, author, tags, featured status
    - Multiple sorting options
    """
    # Parse tag slugs
    tag_ids = []
    if tags:
        tag_slugs = [tag.strip() for tag in tags.split(',') if tag.strip()]
        if tag_slugs:
            tags_objs = tag_crud.get_by_slugs(db, slugs=tag_slugs)
            tag_ids = [tag.id for tag in tags_objs]
    
    filters = PostFilterParams(
        category_id=category_id,
        author_id=author_id,
        tag_ids=tag_ids if tag_ids else None,
        search_query=search,
        is_featured=featured_only,
        sort_by=sort_by
    )
    
    posts, total = post_crud.get_published_posts(db, filters=filters, pagination=pagination)
    
    return PostsPaginatedResponse(
        items=[PostSummaryResponse.from_orm(post) for post in posts],
        total=total,
        page=pagination.page,
        size=pagination.size,
        pages=(total + pagination.size - 1) // pagination.size
    )


@router.get("/posts/{post_id}", response_model=APIResponse[PostResponse])
async def get_post(
    post_id: int,
    request: Request,
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_user)
):
    """
    Get single post with full details.
    
    - Returns complete post information
    - Records view for analytics
    - Includes related posts suggestions
    - Shows user-specific data if authenticated
    """
    post = post_crud.get_with_relationships(db, post_id=post_id, include_comments=False)
    if not post or post.status != PostStatus.PUBLISHED:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Post not found"
        )
    
    # Record view for analytics
    client_ip = request.client.host
    user_agent = request.headers.get("user-agent")
    referrer = request.headers.get("referer")
    
    post_crud.record_view(
        db,
        post_id=post_id,
        user_id=current_user.id if current_user else None,
        ip_address=client_ip,
        user_agent=user_agent,
        referrer=referrer
    )
    
    # Get related posts
    related_posts = post_crud.get_related_posts(db, post_id=post_id)
    
    post_data = PostResponse.from_orm(post)
    post_data.related_posts = [PostSummaryResponse.from_orm(p) for p in related_posts]
    
    # Add user-specific data if authenticated
    if current_user:
        post_data.user_has_liked = post_crud.user_has_liked(db, post_id=post_id, user_id=current_user.id)
        post_data.can_edit = post.can_be_edited_by(current_user)
    
    return APIResponse(
        success=True,
        data=post_data
    )


@router.post("/posts", response_model=APIResponse[PostResponse])
async def create_post(
    post_data: PostCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_author_or_above)
):
    """
    Create a new blog post.
    
    - Requires author role or above
    - Supports draft and scheduled publishing
    - Associates tags and categories
    - Creates revision history
    """
    # Check daily posting limit
    if not current_user.can_post_today():
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=f"Daily posting limit ({current_user.max_posts_per_day}) exceeded"
        )
    
    # Validate category exists
    if post_data.category_id:
        category = category_crud.get(db, id=post_data.category_id)
        if not category:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid category ID"
            )
    
    try:
        post = post_crud.create_with_tags(db, obj_in=post_data, author_id=current_user.id)
        
        return APIResponse(
            success=True,
            message="Post created successfully",
            data=PostResponse.from_orm(post)
        )
    
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.put("/posts/{post_id}", response_model=APIResponse[PostResponse])
async def update_post(
    post_id: int,
    post_data: PostUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update an existing post.
    
    - Author can edit their own posts
    - Admins can edit any post
    - Creates revision history for content changes
    - Handles tag updates
    """
    post = post_crud.get(db, id=post_id)
    if not post:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Post not found"
        )
    
    # Check permissions
    if not post.can_be_edited_by(current_user):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not authorized to edit this post"
        )
    
    updated_post = post_crud.update_with_tags(
        db, 
        post_id=post_id, 
        obj_in=post_data, 
        updated_by_id=current_user.id
    )
    
    return APIResponse(
        success=True,
        message="Post updated successfully",
        data=PostResponse.from_orm(updated_post)
    )


@router.post("/posts/{post_id}/publish", response_model=APIResponse[PostResponse])
async def publish_post(
    post_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_author_or_above)
):
    """
    Publish a draft or scheduled post.
    
    - Changes status to published
    - Sets publication timestamp
    - Updates category and author counters
    """
    post = post_crud.get(db, id=post_id)
    if not post:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Post not found"
        )
    
    if not post.can_be_edited_by(current_user):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not authorized to publish this post"
        )
    
    published_post = post_crud.publish_post(db, post_id=post_id, publisher_id=current_user.id)
    
    return APIResponse(
        success=True,
        message="Post published successfully",
        data=PostResponse.from_orm(published_post)
    )


@router.post("/posts/{post_id}/like")
async def toggle_post_like(
    post_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Toggle like status for a post.
    
    - Adds or removes like for authenticated user
    - Updates like count on post
    - Returns updated like status
    """
    post = post_crud.get(db, id=post_id)
    if not post or post.status != PostStatus.PUBLISHED:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Post not found"
        )
    
    result = post_crud.toggle_like(db, post_id=post_id, user_id=current_user.id)
    
    return APIResponse(
        success=True,
        message=f"Post {result['action']}",
        data=result
    )


@router.get("/posts/{post_id}/analytics", response_model=APIResponse[PostAnalytics])
async def get_post_analytics(
    post_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    days: int = Query(30, ge=1, le=365)
):
    """
    Get detailed analytics for a post.
    
    - Author can view their own post analytics
    - Admins can view all post analytics
    - Includes views, engagement, and geographic data
    """
    post = post_crud.get(db, id=post_id)
    if not post:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Post not found"
        )
    
    # Check permissions
    if post.author_id != current_user.id and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not authorized to view analytics for this post"
        )
    
    analytics = post_crud.get_post_analytics(db, post_id=post_id, days=days)
    
    return APIResponse(
        success=True,
        data=PostAnalytics(**analytics)
    )


# Comment Endpoints
@router.get("/posts/{post_id}/comments", response_model=CommentsPaginatedResponse)
async def get_post_comments(
    post_id: int,
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params),
    include_pending: bool = Query(False, description="Include pending comments (moderators only)"),
    current_user: Optional[User] = Depends(get_optional_user)
):
    """
    Get comments for a post in threaded structure.
    
    - Returns approved comments for public
    - Moderators can see pending comments
    - Threaded structure with replies
    """
    # Check if user can see pending comments
    if include_pending and (not current_user or current_user.role not in [UserRole.ADMIN, UserRole.MODERATOR]):
        include_pending = False
    
    comments = comment_crud.get_post_comments_threaded(
        db, 
        post_id=post_id, 
        include_pending=include_pending
    )
    
    return CommentsPaginatedResponse(
        items=[CommentResponse.from_orm(comment) for comment in comments],
        total=len(comments),
        page=1,  # Threading doesn't use pagination
        size=len(comments),
        pages=1
    )


@router.post("/posts/{post_id}/comments", response_model=APIResponse[CommentResponse])
async def create_comment(
    post_id: int,
    comment_data: CommentCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create a new comment on a post.
    
    - Validates post exists and allows comments
    - Supports threaded replies
    - Auto-moderation for trusted users
    """
    # Validate post exists and allows comments
    post = post_crud.get(db, id=post_id)
    if not post or post.status != PostStatus.PUBLISHED:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Post not found"
        )
    
    if not post.allow_comments:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Comments are disabled for this post"
        )
    
    # Force correct post_id
    comment_data.post_id = post_id
    
    try:
        comment = comment_crud.create_comment(
            db, 
            obj_in=comment_data, 
            author_id=current_user.id
        )
        
        return APIResponse(
            success=True,
            message="Comment created successfully",
            data=CommentResponse.from_orm(comment)
        )
    
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.put("/comments/{comment_id}", response_model=APIResponse[CommentResponse])
async def update_comment(
    comment_id: int,
    comment_data: CommentUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update a comment (author only, within time limit).
    
    - Only author can edit their comments
    - Time limit for editing (e.g., 15 minutes)
    - Re-triggers moderation if content changed significantly
    """
    comment = comment_crud.get(db, id=comment_id)
    if not comment:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Comment not found"
        )
    
    # Check permissions
    if comment.author_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not authorized to edit this comment"
        )
    
    # Check time limit (15 minutes)
    time_limit = timedelta(minutes=15)
    if datetime.utcnow() - comment.created_at > time_limit:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Comment editing time limit exceeded"
        )
    
    updated_comment = comment_crud.update(db, db_obj=comment, obj_in=comment_data)
    
    return APIResponse(
        success=True,
        message="Comment updated successfully",
        data=CommentResponse.from_orm(updated_comment)
    )


@router.post("/comments/{comment_id}/moderate", response_model=APIResponse[CommentResponse])
async def moderate_comment(
    comment_id: int,
    moderation_data: CommentModerationUpdate,
    db: Session = Depends(get_db),
    current_moderator: User = Depends(get_current_moderator_or_above)
):
    """
    Moderate a comment (approve/reject/spam).
    
    - Moderator and admin access only
    - Updates comment status with audit trail
    - Affects post comment counts
    """
    moderated_comment = comment_crud.moderate_comment(
        db,
        comment_id=comment_id,
        status=moderation_data.status,
        moderator_id=current_moderator.id,
        reason=moderation_data.moderation_reason
    )
    
    if not moderated_comment:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Comment not found"
        )
    
    return APIResponse(
        success=True,
        message=f"Comment {moderation_data.status.value}",
        data=CommentResponse.from_orm(moderated_comment)
    )


# Category Endpoints
@router.get("/categories", response_model=APIResponse[List[CategoryResponse]])
async def get_categories(
    db: Session = Depends(get_db),
    include_tree: bool = Query(False, description="Return hierarchical tree structure")
):
    """
    Get blog categories.
    
    - Returns flat list or hierarchical tree
    - Includes post counts and visibility status
    - Public endpoint
    """
    if include_tree:
        categories_tree = category_crud.get_category_tree(db)
        return APIResponse(
            success=True,
            data=categories_tree
        )
    else:
        categories = category_crud.get_root_categories(db)
        return APIResponse(
            success=True,
            data=[CategoryResponse.from_orm(cat) for cat in categories]
        )


@router.post("/categories", response_model=APIResponse[CategoryResponse])
async def create_category(
    category_data: CategoryCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_moderator_or_above)
):
    """
    Create a new category.
    
    - Moderator access required
    - Validates parent category exists
    - Auto-generates slug from name
    """
    # Validate parent category if provided
    if category_data.parent_id:
        parent = category_crud.get(db, id=category_data.parent_id)
        if not parent:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid parent category ID"
            )
    
    try:
        category = category_crud.create(db, obj_in=category_data)
        
        return APIResponse(
            success=True,
            message="Category created successfully",
            data=CategoryResponse.from_orm(category)
        )
    
    except IntegrityError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Category name or slug already exists"
        )


# Tag Endpoints
@router.get("/tags", response_model=APIResponse[List[TagResponse]])
async def get_tags(
    db: Session = Depends(get_db),
    popular: bool = Query(False, description="Get popular tags only"),
    search: Optional[str] = Query(None, description="Search tags"),
    limit: int = Query(50, ge=1, le=100)
):
    """
    Get blog tags.
    
    - Returns all tags or popular/trending tags
    - Supports tag search
    - Includes usage statistics
    """
    if search:
        tags = tag_crud.search_tags(db, query=search, limit=limit)
    elif popular:
        tags = tag_crud.get_popular_tags(db, limit=limit)
    else:
        tags = tag_crud.get_multi(db, limit=limit)
    
    return APIResponse(
        success=True,
        data=[TagResponse.from_orm(tag) for tag in tags]
    )


@router.post("/tags", response_model=APIResponse[TagResponse])
async def create_tag(
    tag_data: TagCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_author_or_above)
):
    """
    Create a new tag.
    
    - Author access required
    - Auto-generates slug from name
    - Validates color format
    """
    try:
        tag = tag_crud.create(db, obj_in=tag_data)
        
        return APIResponse(
            success=True,
            message="Tag created successfully",
            data=TagResponse.from_orm(tag)
        )
    
    except IntegrityError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Tag name or slug already exists"
        )


# Analytics Endpoints
@router.get("/statistics", response_model=APIResponse[BlogStatistics])
async def get_blog_statistics(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_moderator_or_above)
):
    """
    Get comprehensive blog statistics.
    
    - Moderator access required
    - Content statistics and analytics
    - Top performing content
    """
    stats = content_manager.get_content_statistics(db)
    
    return APIResponse(
        success=True,
        data=BlogStatistics(**stats)
    )


@router.get("/search", response_model=PostsPaginatedResponse)
async def search_posts(
    query: str = Query(..., min_length=3, description="Search query"),
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params)
):
    """
    Full-text search across blog posts.
    
    - PostgreSQL full-text search
    - Searches title, content, and excerpt
    - Ranked by relevance
    """
    posts, total = post_crud.search_posts(db, search_query=query, pagination=pagination)
    
    return PostsPaginatedResponse(
        items=[PostSummaryResponse.from_orm(post) for post in posts],
        total=total,
        page=pagination.page,
        size=pagination.size,
        pages=(total + pagination.size - 1) // pagination.size
    )


# Moderation Endpoints
@router.get("/moderation/comments", response_model=CommentsPaginatedResponse)
async def get_comments_for_moderation(
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params),
    current_moderator: User = Depends(get_current_moderator_or_above)
):
    """
    Get comments pending moderation.
    
    - Moderator access required
    - Returns comments needing review
    - Ordered by creation date
    """
    comments, total = comment_crud.get_comments_for_moderation(db, pagination=pagination)
    
    return CommentsPaginatedResponse(
        items=[CommentResponse.from_orm(comment) for comment in comments],
        total=total,
        page=pagination.page,
        size=pagination.size,
        pages=(total + pagination.size - 1) // pagination.size
    )
```


### 3.7 Database Migrations with Alembic

Database migrations are essential for managing schema changes in production environments. Alembic provides version control for database schemas.

#### 3.7.1 Alembic Setup and Configuration

**alembic.ini** - Basic Configuration:
```ini
[alembic]
script_location = alembic
file_template = %%(year)d%%(month).2d%%(day).2d_%%(rev)s_%%(slug)s
sqlalchemy.url = postgresql://user:pass@localhost/db

[loggers]
keys = root,sqlalchemy,alembic

[handlers] 
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console

[logger_alembic]  
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
formatter = generic

[formatter_generic]
format = %%(levelname)-5.5s [%%(name)s] %%(message)s
```

### 3.7 Database Migrations with Alembic

Database migrations are essential for managing schema changes in production environments. Alembic provides version control for database schemas, allowing you to track and apply changes safely.

#### 3.7.1 Alembic Setup and Configuration

**alembic.ini** - Configuration File:
```ini
[alembic]
# Path to migration scripts
script_location = alembic

# Template for migration filenames with timestamp
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

# Timezone for timestamps
timezone = UTC

# Database URL (overridden by env.py)
sqlalchemy.url = postgresql://user:pass@localhost/db

# Post-write hooks for code formatting
[post_write_hooks]
hooks = black
black.type = console_scripts
black.entrypoint = black
black.options = -l 88 REVISION_SCRIPT_FILENAME

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**Basic Setup Commands:**
```bash
# Initialize Alembic in your project (run once)
alembic init alembic

# Check Alembic configuration
alembic check

# Create your first migration
alembic revision --autogenerate -m "Initial database schema"

# Apply migrations
alembic upgrade head

# Check current migration status
alembic current
```

#### 3.7.2 Environment Configuration

**alembic/env.py** - Complete Environment Setup:
```python
"""
Enhanced Alembic environment configuration with proper model discovery,
custom filtering, and comprehensive migration handling.
"""

import logging
import sys
import os
from logging.config import fileConfig

from sqlalchemy import engine_from_config, pool
from alembic import context

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# Import all models to register with metadata
from app.models.base import Base
from app.models.user import User, UserSession, UserPermission
from app.models.blog import Post, Comment, Category, Tag, PostLike, PostView, PostRevision
from app.config import get_settings

# Alembic configuration object
config = context.config

# Setup logging from config file
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Get database URL from application settings
settings = get_settings()
config.set_main_option("sqlalchemy.url", str(settings.database.database_url))

# Target metadata for autogenerate
target_metadata = Base.metadata

logger = logging.getLogger('alembic')


def include_object(object, name, type_, reflected, compare_to):
    """
    Filter function to control which database objects are included in migrations.
    
    This function helps exclude system tables, temporary objects, and other
    items that shouldn't be managed by our migrations.
    """
    # Exclude system tables
    system_tables = {
        'spatial_ref_sys',      # PostGIS system table
        'alembic_version',      # Alembic version table
        'pg_stat_statements',   # PostgreSQL stats
        'information_schema'    # PostgreSQL schema info
    }
    
    if type_ == "table" and name in system_tables:
        logger.debug(f"Excluding system table: {name}")
        return False
    
    # Exclude certain auto-generated indexes
    if type_ == "index":
        # PostgreSQL auto-generated indexes
        if name.startswith(('pg_', '_pg_')):
            return False
        # Unique constraint indexes (will be recreated with constraint)
        if name.endswith('_key'):
            return False
    
    # Include everything else
    return True


def process_revision_directives(context, revision, directives):
    """
    Custom processing of migration directives.
    
    This function allows us to customize how migrations are generated,
    including preventing empty migrations and adding custom logic.
    """
    if getattr(config.cmd_opts, 'autogenerate', False):
        script = directives[0]
        
        # Prevent empty migrations
        if script.upgrade_ops.is_empty():
            directives[:] = []
            logger.info('No schema changes detected - skipping migration generation')
            return
        
        # Log migration details
        logger.info(f'Generating migration: {script.message}')
        
        # Could add custom logic here:
        # - Validate migration safety
        # - Add custom operations
        # - Modify generated operations


def run_migrations_offline() -> None:
    """
    Run migrations in 'offline' mode.
    
    Offline mode generates migration SQL without connecting to database.
    Useful for generating SQL scripts for manual review or execution.
    """
    url = config.get_main_option("sqlalchemy.url")
    
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        
        # Enhanced comparison options
        compare_type=True,              # Detect column type changes
        compare_server_default=True,    # Detect default value changes
        include_schemas=True,           # Include schema information
        
        # Custom functions
        include_object=include_object,
        process_revision_directives=process_revision_directives,
        
        # Migration options
        render_as_batch=False,          # Don't use batch mode by default
        transaction_per_migration=True  # Wrap each migration in transaction
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """
    Run migrations in 'online' mode.
    
    Online mode connects to the database and executes migrations directly.
    This is the standard mode for development and production deployments.
    """
    
    # Create engine with special configuration for migrations
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,  # Disable connection pooling for migrations
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            
            # Enhanced comparison options  
            compare_type=True,
            compare_server_default=True,
            include_schemas=True,
            
            # Custom functions
            include_object=include_object,
            process_revision_directives=process_revision_directives,
            
            # Migration execution options
            render_as_batch=False,
            transaction_per_migration=True,
            transactional_ddl=True,        # Use transactions for DDL
            
            # Additional safety options
            as_sql=False,                   # Execute directly, don't just print SQL
            tag=None                        # No special tagging
        )

        with context.begin_transaction():
            context.run_migrations()


# Execute appropriate migration mode
if context.is_offline_mode():
    logger.info("Running migrations in offline mode")
    run_migrations_offline()
else:
    logger.info("Running migrations in online mode") 
    run_migrations_online()
```

**Key Configuration Features:**

1. **Model Auto-Discovery**: Automatically imports all models to ensure they're registered with metadata
2. **Dynamic Database URL**: Uses application settings instead of hardcoded values
3. **Object Filtering**: Excludes system tables and unwanted objects from migrations
4. **Empty Migration Prevention**: Skips generation when no changes detected
5. **Enhanced Comparison**: Detects type changes, defaults, and schema differences
6. **Transaction Safety**: Wraps migrations in transactions for rollback capability

#### 3.7.3 Migration Creation and Management

**Generating Initial Migration:**
```bash
# Create initial migration from existing models
alembic revision --autogenerate -m "Initial schema - users and blog system"

# The generated migration will include all tables defined in our models
```

**Sample Generated Migration (alembic/versions/001_initial_schema.py):**
```python
"""
Initial schema - users and blog system

Revision ID: abc123def456
Revises: 
Create Date: 2024-01-15 10:30:00.000000
"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
import uuid


# Revision identifiers
revision = 'abc123def456'
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    """
    Create initial database schema with all tables and relationships.
    """
    
    # Create UUID extension for PostgreSQL
    op.execute('CREATE EXTENSION IF NOT EXISTS "uuid-ossp"')
    
    # Create ENUM types
    op.execute("CREATE TYPE user_status AS ENUM ('active', 'inactive', 'suspended', 'banned')")
    op.execute("CREATE TYPE user_role AS ENUM ('admin', 'moderator', 'author', 'user')")
    op.execute("CREATE TYPE post_status AS ENUM ('draft', 'published', 'archived')")
    
    # Users table with comprehensive fields
    op.create_table(
        'users',
        sa.Column('id', postgresql.UUID(as_uuid=True), 
                 server_default=sa.text('uuid_generate_v4()'), 
                 nullable=False),
        sa.Column('username', sa.String(length=50), nullable=False),
        sa.Column('email', sa.String(length=255), nullable=False),
        sa.Column('password_hash', sa.String(length=255), nullable=False),
        sa.Column('first_name', sa.String(length=100), nullable=True),
        sa.Column('last_name', sa.String(length=100), nullable=True),
        sa.Column('bio', sa.Text(), nullable=True),
        sa.Column('avatar_url', sa.String(length=500), nullable=True),
        sa.Column('is_verified', sa.Boolean(), 
                 server_default=sa.text('false'), nullable=False),
        sa.Column('status', postgresql.ENUM('active', 'inactive', 'suspended', 'banned', 
                 name='user_status'), 
                 server_default=sa.text("'active'"), nullable=False),
        sa.Column('role', postgresql.ENUM('admin', 'moderator', 'author', 'user', 
                 name='user_role'),
                 server_default=sa.text("'user'"), nullable=False),
        sa.Column('last_login_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('email_verified_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), 
                 server_default=sa.text('CURRENT_TIMESTAMP'), nullable=False),
        sa.Column('updated_at', sa.DateTime(timezone=True), 
                 server_default=sa.text('CURRENT_TIMESTAMP'), nullable=False),
        sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create indexes for users
    op.create_index('ix_users_username', 'users', ['username'], unique=True)
    op.create_index('ix_users_email', 'users', ['email'], unique=True)
    op.create_index('ix_users_status', 'users', ['status'])
    op.create_index('ix_users_role', 'users', ['role'])
    op.create_index('ix_users_created_at', 'users', ['created_at'])
    
    # User sessions table
    op.create_table(
        'user_sessions',
        sa.Column('id', postgresql.UUID(as_uuid=True), 
                 server_default=sa.text('uuid_generate_v4()'), 
                 nullable=False),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('access_token', sa.String(length=500), nullable=False),
        sa.Column('refresh_token', sa.String(length=500), nullable=False),
        sa.Column('expires_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('ip_address', sa.String(length=45), nullable=True),
        sa.Column('user_agent', sa.Text(), nullable=True),
        sa.Column('is_revoked', sa.Boolean(), 
                 server_default=sa.text('false'), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), 
                 server_default=sa.text('CURRENT_TIMESTAMP'), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], 
                               name='fk_user_sessions_user_id', 
                               ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Continue with other tables...
    # (Categories, Tags, Posts, Comments, etc.)


def downgrade() -> None:
    """
    Drop all tables and types created in upgrade.
    """
    # Drop tables in reverse dependency order
    op.drop_table('user_sessions')
    op.drop_index('ix_users_created_at', table_name='users')
    op.drop_index('ix_users_role', table_name='users') 
    op.drop_index('ix_users_status', table_name='users')
    op.drop_index('ix_users_email', table_name='users')
    op.drop_index('ix_users_username', table_name='users')
    op.drop_table('users')
    
    # Drop ENUM types
    op.execute('DROP TYPE IF EXISTS post_status')
    op.execute('DROP TYPE IF EXISTS user_role')
    op.execute('DROP TYPE IF EXISTS user_status')
```

**Creating Model Change Migrations:**

When you modify models, create targeted migrations:

```bash
# After modifying User model to add new field
alembic revision --autogenerate -m "Add user profile fields"

# After modifying Post model relationships
alembic revision --autogenerate -m "Update post-category relationships"

# For data migrations (no model changes)
alembic revision -m "Migrate user roles to new system"
```

**Common Migration Patterns:**

1. **Adding New Column with Default:**
```python
def upgrade() -> None:
    op.add_column('users', 
                  sa.Column('timezone', sa.String(50), 
                           server_default='UTC', nullable=False))

def downgrade() -> None:
    op.drop_column('users', 'timezone')
```

2. **Modifying Column Type:**
```python
def upgrade() -> None:
    # Change username length limit
    op.alter_column('users', 'username', 
                   type_=sa.String(100),  # Was String(50)
                   existing_type=sa.String(50))

def downgrade() -> None:
    op.alter_column('users', 'username',
                   type_=sa.String(50),
                   existing_type=sa.String(100))
```

3. **Adding New Index:**
```python
def upgrade() -> None:
    op.create_index('ix_posts_published_created', 'posts', 
                   ['status', 'created_at'],
                   postgresql_where=sa.text("status = 'published'"))

def downgrade() -> None:
    op.drop_index('ix_posts_published_created', table_name='posts')
```

#### 3.7.4 Migration Management Commands

**Essential Migration Commands:**

```bash
# Check current database revision
alembic current

# Show migration history
alembic history --verbose

# Show pending migrations
alembic show

# Upgrade to latest revision
alembic upgrade head

# Upgrade to specific revision
alembic upgrade abc123def456

# Downgrade by one revision
alembic downgrade -1

# Downgrade to specific revision
alembic downgrade abc123def456

# Show SQL without executing (dry run)
alembic upgrade head --sql

# Stamp database without running migrations
alembic stamp head
```

**Advanced Migration Management Script:**

Create **scripts/migrate.py** for enhanced migration management:

```python
"""
Advanced migration management script with safety checks and rollback capabilities.
"""

import asyncio
import logging
import subprocess
import sys
from pathlib import Path
from typing import List, Optional

import typer
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

from app.config import get_settings

app = typer.Typer()
logger = logging.getLogger(__name__)


class MigrationManager:
    """Enhanced migration management with safety features."""
    
    def __init__(self):
        self.settings = get_settings()
        self.engine = create_engine(str(self.settings.database.database_url))
    
    def run_alembic_command(self, command: List[str]) -> tuple[int, str, str]:
        """Run alembic command and capture output."""
        try:
            result = subprocess.run(
                ['alembic'] + command,
                capture_output=True,
                text=True,
                check=False
            )
            return result.returncode, result.stdout, result.stderr
        except Exception as e:
            return 1, '', str(e)
    
    def get_current_revision(self) -> Optional[str]:
        """Get current database revision."""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT version_num FROM alembic_version"))
                row = result.fetchone()
                return row[0] if row else None
        except SQLAlchemyError:
            return None
    
    def backup_database(self, backup_name: str) -> bool:
        """Create database backup before migration."""
        try:
            db_config = self.settings.database
            cmd = [
                'pg_dump',
                f'--host={db_config.host}',
                f'--port={db_config.port}',
                f'--username={db_config.username}',
                f'--dbname={db_config.database}',
                '--no-password',
                '--format=custom',
                '--compress=9',
                f'--file={backup_name}'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            return result.returncode == 0
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            return False


@app.command()
def status():
    """Show current migration status."""
    manager = MigrationManager()
    
    # Get current revision
    current = manager.get_current_revision()
    typer.echo(f"Current revision: {current or 'Not found'}")
    
    # Show Alembic status
    code, stdout, stderr = manager.run_alembic_command(['current', '--verbose'])
    if stdout:
        typer.echo(stdout)
    if stderr:
        typer.echo(f"Warning: {stderr}", err=True)


@app.command()
def migrate(
    message: str = typer.Option(..., "--message", "-m", help="Migration message"),
    backup: bool = typer.Option(True, help="Create backup before migration"),
    dry_run: bool = typer.Option(False, help="Show SQL without executing")
):
    """Create and apply new migration."""
    manager = MigrationManager()
    
    # Create backup if requested
    if backup and not dry_run:
        backup_name = f"backup_before_migration_{message.replace(' ', '_')}.sql"
        typer.echo(f"Creating backup: {backup_name}")
        
        if not manager.backup_database(backup_name):
            typer.echo("❌ Backup failed! Aborting migration.", err=True)
            raise typer.Exit(1)
        
        typer.echo("✅ Backup created successfully")
    
    # Generate migration
    typer.echo("Generating migration...")
    code, stdout, stderr = manager.run_alembic_command([
        'revision', '--autogenerate', '-m', message
    ])
    
    if code != 0:
        typer.echo(f"❌ Migration generation failed: {stderr}", err=True)
        raise typer.Exit(1)
    
    typer.echo("✅ Migration generated successfully")
    typer.echo(stdout)
    
    # Apply migration
    if dry_run:
        typer.echo("Dry run - showing SQL that would be executed:")
        code, stdout, stderr = manager.run_alembic_command(['upgrade', 'head', '--sql'])
        typer.echo(stdout)
    else:
        typer.echo("Applying migration...")
        code, stdout, stderr = manager.run_alembic_command(['upgrade', 'head'])
        
        if code == 0:
            typer.echo("✅ Migration applied successfully")
            typer.echo(stdout)
        else:
            typer.echo(f"❌ Migration failed: {stderr}", err=True)
            raise typer.Exit(1)


@app.command()
def rollback(
    steps: int = typer.Option(1, help="Number of steps to rollback"),
    backup: bool = typer.Option(True, help="Create backup before rollback"),
    confirm: bool = typer.Option(False, help="Skip confirmation prompt")
):
    """Rollback migrations."""
    manager = MigrationManager()
    
    if not confirm:
        proceed = typer.confirm(
            f"Are you sure you want to rollback {steps} migration(s)?"
        )
        if not proceed:
            typer.echo("Rollback cancelled")
            raise typer.Exit()
    
    # Create backup if requested
    if backup:
        backup_name = f"backup_before_rollback_{steps}_steps.sql"
        typer.echo(f"Creating backup: {backup_name}")
        
        if not manager.backup_database(backup_name):
            typer.echo("❌ Backup failed! Aborting rollback.", err=True)
            raise typer.Exit(1)
    
    # Perform rollback
    rollback_target = f"-{steps}"
    code, stdout, stderr = manager.run_alembic_command(['downgrade', rollback_target])
    
    if code == 0:
        typer.echo(f"✅ Successfully rolled back {steps} migration(s)")
        typer.echo(stdout)
    else:
        typer.echo(f"❌ Rollback failed: {stderr}", err=True)
        raise typer.Exit(1)


@app.command()
def history(verbose: bool = typer.Option(False, help="Show verbose history")):
    """Show migration history."""
    manager = MigrationManager()
    
    command = ['history']
    if verbose:
        command.append('--verbose')
    
    code, stdout, stderr = manager.run_alembic_command(command)
    typer.echo(stdout)


if __name__ == "__main__":
    app()
```

**Usage Examples:**

```bash
# Create and apply migration with backup
python scripts/migrate.py migrate --message "Add user profile enhancements"

# Dry run to see SQL
python scripts/migrate.py migrate --message "Update indexes" --dry-run

# Rollback last migration
python scripts/migrate.py rollback --steps 1

# Check migration status
python scripts/migrate.py status

# Show migration history
python scripts/migrate.py history --verbose
```

**Production Migration Checklist:**

1. **Pre-Migration Safety:**
   - Create database backup
   - Test migration on staging environment
   - Check for long-running operations
   - Verify rollback procedures

2. **Migration Execution:**
   - Use maintenance window if needed
   - Monitor database performance
   - Keep application logs available
   - Have rollback plan ready

3. **Post-Migration Validation:**
   - Verify data integrity
   - Test application functionality
   - Monitor for performance issues
   - Document any manual steps

#### 3.7.5 Advanced Migration Patterns and Data Migrations

**Data Migration Example - Migrating User Roles:**

When you need to modify data, not just schema, create data migrations:

```python
"""
Data migration: Convert old role system to new permission-based system

Revision ID: def456ghi789
Revises: abc123def456
Create Date: 2024-01-16 14:20:00.000000
"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.sql import table, column


# Revision identifiers
revision = 'def456ghi789'
down_revision = 'abc123def456'
branch_labels = None
depends_on = None


def upgrade() -> None:
    """
    Migrate from simple role field to permission-based system.
    """
    
    # First, add new permissions table
    op.create_table(
        'user_permissions',
        sa.Column('id', sa.Integer, primary_key=True),
        sa.Column('user_id', sa.Integer, sa.ForeignKey('users.id'), nullable=False),
        sa.Column('permission', sa.String(100), nullable=False),
        sa.Column('granted_by', sa.Integer, sa.ForeignKey('users.id'), nullable=True),
        sa.Column('granted_at', sa.DateTime, server_default=sa.func.now()),
        sa.UniqueConstraint('user_id', 'permission')
    )
    
    # Define table structures for data migration
    users_table = table('users',
        column('id', sa.Integer),
        column('role', sa.String),
        column('created_at', sa.DateTime)
    )
    
    permissions_table = table('user_permissions',
        column('user_id', sa.Integer),
        column('permission', sa.String),
        column('granted_at', sa.DateTime)
    )
    
    # Create connection for data operations
    connection = op.get_bind()
    
    # Migrate existing roles to permissions
    role_permission_mapping = {
        'admin': ['read', 'write', 'delete', 'moderate', 'admin'],
        'moderator': ['read', 'write', 'moderate'],
        'author': ['read', 'write'],
        'user': ['read']
    }
    
    # Fetch existing users with their roles
    users = connection.execute(
        sa.select(users_table.c.id, users_table.c.role, users_table.c.created_at)
    ).fetchall()
    
    # Insert permissions for each user based on their role
    permissions_to_insert = []
    for user_id, role, created_at in users:
        if role in role_permission_mapping:
            for permission in role_permission_mapping[role]:
                permissions_to_insert.append({
                    'user_id': user_id,
                    'permission': permission,
                    'granted_at': created_at
                })
    
    # Bulk insert permissions
    if permissions_to_insert:
        op.bulk_insert(permissions_table, permissions_to_insert)
    
    # Add indexes for performance
    op.create_index('ix_user_permissions_user_id', 'user_permissions', ['user_id'])
    op.create_index('ix_user_permissions_permission', 'user_permissions', ['permission'])


def downgrade() -> None:
    """
    Reverse the permission migration - restore simple roles.
    """
    # Note: This is destructive - we lose permission granularity
    
    # Get the connection
    connection = op.get_bind()
    
    # Define table structures
    users_table = table('users',
        column('id', sa.Integer),
        column('role', sa.String)
    )
    
    permissions_table = table('user_permissions',
        column('user_id', sa.Integer),
        column('permission', sa.String)
    )
    
    # Try to restore roles based on highest permission level
    # This is a best-effort restoration
    permission_role_mapping = {
        'admin': 'admin',
        'moderate': 'moderator', 
        'write': 'author',
        'read': 'user'
    }
    
    # Get users and their highest permission level
    highest_permissions = connection.execute(
        text("""
            SELECT DISTINCT user_id,
                CASE 
                    WHEN 'admin' = ANY(array_agg(permission)) THEN 'admin'
                    WHEN 'moderate' = ANY(array_agg(permission)) THEN 'moderator'
                    WHEN 'write' = ANY(array_agg(permission)) THEN 'author'
                    ELSE 'user'
                END as role
            FROM user_permissions 
            GROUP BY user_id
        """)
    ).fetchall()
    
    # Update user roles
    for user_id, role in highest_permissions:
        connection.execute(
            users_table.update()
            .where(users_table.c.id == user_id)
            .values(role=role)
        )
    
    # Drop the permissions table
    op.drop_index('ix_user_permissions_permission')
    op.drop_index('ix_user_permissions_user_id')
    op.drop_table('user_permissions')
```

**PostgreSQL-Specific Migration Features:**

```python
"""
PostgreSQL-specific advanced migration patterns.

Revision ID: ghi789jkl012
Revises: def456ghi789
Create Date: 2024-01-17 09:15:00.000000
"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql


def upgrade() -> None:
    """Add PostgreSQL-specific features and optimizations."""
    
    # 1. Create custom composite types
    op.execute("""
        CREATE TYPE address_type AS (
            street VARCHAR(200),
            city VARCHAR(100),
            state VARCHAR(50),
            country VARCHAR(50),
            postal_code VARCHAR(20)
        )
    """)
    
    # 2. Add JSONB column with GIN index
    op.add_column('posts', 
        sa.Column('metadata_jsonb', postgresql.JSONB, nullable=True)
    )
    op.create_index(
        'ix_posts_metadata_gin',
        'posts',
        ['metadata_jsonb'],
        postgresql_using='gin'
    )
    
    # 3. Create full-text search with custom configuration
    op.execute("""
        CREATE TEXT SEARCH CONFIGURATION blog_search (COPY = english);
        ALTER TEXT SEARCH CONFIGURATION blog_search
        ALTER MAPPING FOR hword, hword_part, word WITH unaccent, simple;
    """)
    
    # 4. Add generated columns for full-text search
    op.execute("""
        ALTER TABLE posts 
        ADD COLUMN search_vector tsvector 
        GENERATED ALWAYS AS (
            setweight(to_tsvector('blog_search', coalesce(title, '')), 'A') ||
            setweight(to_tsvector('blog_search', coalesce(excerpt, '')), 'B') ||
            setweight(to_tsvector('blog_search', coalesce(content, '')), 'D')
        ) STORED
    """)
    
    # 5. Create GIN index for full-text search
    op.create_index(
        'ix_posts_search_vector',
        'posts',
        ['search_vector'],
        postgresql_using='gin'
    )
    
    # 6. Create partial indexes for performance
    op.create_index(
        'ix_posts_published_recent',
        'posts',
        ['created_at'],
        postgresql_where=sa.text("status = 'published' AND created_at > CURRENT_DATE - INTERVAL '30 days'")
    )
    
    # 7. Add database functions for analytics
    op.execute("""
        CREATE OR REPLACE FUNCTION calculate_engagement_score(
            likes_count INTEGER,
            comments_count INTEGER,
            views_count INTEGER,
            created_at TIMESTAMP
        ) RETURNS NUMERIC AS $$
        DECLARE
            days_old INTEGER;
            base_score NUMERIC;
        BEGIN
            days_old := EXTRACT(DAY FROM NOW() - created_at);
            base_score := (likes_count * 3) + (comments_count * 5) + (views_count * 0.1);
            
            -- Apply time decay
            IF days_old > 0 THEN
                base_score := base_score / (1 + (days_old * 0.1));
            END IF;
            
            RETURN ROUND(base_score, 2);
        END;
        $$ LANGUAGE plpgsql IMMUTABLE;
    """)
    
    # 8. Create materialized view for analytics
    op.execute("""
        CREATE MATERIALIZED VIEW post_analytics AS
        SELECT 
            p.id,
            p.title,
            p.author_id,
            u.username as author_name,
            p.status,
            p.created_at,
            COALESCE(likes.count, 0) as likes_count,
            COALESCE(comments.count, 0) as comments_count,
            COALESCE(views.count, 0) as views_count,
            calculate_engagement_score(
                COALESCE(likes.count, 0),
                COALESCE(comments.count, 0), 
                COALESCE(views.count, 0),
                p.created_at
            ) as engagement_score
        FROM posts p
        JOIN users u ON p.author_id = u.id
        LEFT JOIN (
            SELECT post_id, COUNT(*) as count 
            FROM post_likes 
            WHERE deleted_at IS NULL 
            GROUP BY post_id
        ) likes ON p.id = likes.post_id
        LEFT JOIN (
            SELECT post_id, COUNT(*) as count 
            FROM comments 
            WHERE deleted_at IS NULL AND status = 'published'
            GROUP BY post_id
        ) comments ON p.id = comments.post_id
        LEFT JOIN (
            SELECT post_id, COUNT(*) as count 
            FROM post_views 
            GROUP BY post_id
        ) views ON p.id = views.post_id
        WHERE p.deleted_at IS NULL;
    """)
    
    # 9. Create unique index for materialized view
    op.create_index(
        'ix_post_analytics_id',
        'post_analytics', 
        ['id'],
        unique=True
    )
    
    # 10. Add triggers for automatic updates
    op.execute("""
        -- Function to refresh materialized view
        CREATE OR REPLACE FUNCTION refresh_post_analytics()
        RETURNS TRIGGER AS $$
        BEGIN
            REFRESH MATERIALIZED VIEW CONCURRENTLY post_analytics;
            RETURN NULL;
        END;
        $$ LANGUAGE plpgsql;
        
        -- Triggers to refresh analytics on data changes
        CREATE TRIGGER trigger_refresh_analytics_posts
        AFTER INSERT OR UPDATE OR DELETE ON posts
        FOR EACH STATEMENT EXECUTE FUNCTION refresh_post_analytics();
        
        CREATE TRIGGER trigger_refresh_analytics_likes  
        AFTER INSERT OR UPDATE OR DELETE ON post_likes
        FOR EACH STATEMENT EXECUTE FUNCTION refresh_post_analytics();
        
        CREATE TRIGGER trigger_refresh_analytics_comments
        AFTER INSERT OR UPDATE OR DELETE ON comments  
        FOR EACH STATEMENT EXECUTE FUNCTION refresh_post_analytics();
    """)


def downgrade() -> None:
    """Remove PostgreSQL-specific features."""
    
    # Drop triggers first
    op.execute("DROP TRIGGER IF EXISTS trigger_refresh_analytics_comments ON comments")
    op.execute("DROP TRIGGER IF EXISTS trigger_refresh_analytics_likes ON post_likes")
    op.execute("DROP TRIGGER IF EXISTS trigger_refresh_analytics_posts ON posts")
    op.execute("DROP FUNCTION IF EXISTS refresh_post_analytics()")
    
    # Drop materialized view and indexes
    op.drop_index('ix_post_analytics_id')
    op.execute("DROP MATERIALIZED VIEW IF EXISTS post_analytics")
    
    # Drop custom functions
    op.execute("DROP FUNCTION IF EXISTS calculate_engagement_score")
    
    # Drop indexes
    op.drop_index('ix_posts_published_recent', table_name='posts')
    op.drop_index('ix_posts_search_vector', table_name='posts')
    op.drop_index('ix_posts_metadata_gin', table_name='posts')
    
    # Remove generated column
    op.execute("ALTER TABLE posts DROP COLUMN IF EXISTS search_vector")
    
    # Drop text search configuration
    op.execute("DROP TEXT SEARCH CONFIGURATION IF EXISTS blog_search")
    
    # Remove JSONB column
    op.drop_column('posts', 'metadata_jsonb')
    
    # Drop composite type
    op.execute("DROP TYPE IF EXISTS address_type")
```

**Custom Migration Operations Class:**

Create **alembic/custom_operations.py** for reusable migration patterns:

```python
"""
Custom Alembic operations for common PostgreSQL patterns.
"""

from alembic.operations import Operations
from alembic.operations.base import BatchOperations
from alembic import op
import sqlalchemy as sa


@Operations.register_operation("create_enum_safe")
class CreateEnumSafeOp:
    """Create enum type safely (if not exists)."""
    
    def __init__(self, name: str, values: list):
        self.name = name
        self.values = values
    
    @classmethod
    def create_enum_safe(cls, operations, name: str, values: list):
        """Create enum type if it doesn't exist."""
        return cls(name, values)
    
    def reverse(self):
        return DropEnumSafeOp(self.name)
    
    def to_diff_tuple(self):
        return ("create_enum_safe", (self.name, self.values))


@Operations.register_operation("drop_enum_safe") 
class DropEnumSafeOp:
    """Drop enum type safely."""
    
    def __init__(self, name: str):
        self.name = name
    
    @classmethod
    def drop_enum_safe(cls, operations, name: str):
        """Drop enum type safely."""
        return cls(name)
    
    def reverse(self):
        # Note: Cannot reverse without knowing original values
        raise NotImplementedError("Cannot auto-reverse enum drop")


@Operations.implementation_for(CreateEnumSafeOp)
def create_enum_safe(operations, operation):
    """Implementation for safe enum creation."""
    values_str = "', '".join(operation.values)
    operations.execute(
        f"CREATE TYPE {operation.name} AS ENUM ('{values_str}')"
        f" -- IF NOT EXISTS not supported in enum, using exception handling"
    )


@Operations.implementation_for(DropEnumSafeOp)
def drop_enum_safe(operations, operation):
    """Implementation for safe enum drop."""
    operations.execute(f"DROP TYPE IF EXISTS {operation.name}")


# Add custom operation to create full-text search setup
@Operations.register_operation("create_fulltext_search")
class CreateFullTextSearchOp:
    """Create full-text search configuration."""
    
    def __init__(self, table_name: str, column_name: str, 
                 search_columns: dict, config_name: str = 'simple'):
        self.table_name = table_name
        self.column_name = column_name
        self.search_columns = search_columns  # {'title': 'A', 'content': 'D'}
        self.config_name = config_name
    
    @classmethod
    def create_fulltext_search(cls, operations, table_name: str, 
                              column_name: str, search_columns: dict, 
                              config_name: str = 'simple'):
        return cls(table_name, column_name, search_columns, config_name)


@Operations.implementation_for(CreateFullTextSearchOp)
def create_fulltext_search(operations, operation):
    """Implementation for full-text search setup."""
    
    # Build the search vector expression
    vector_parts = []
    for col, weight in operation.search_columns.items():
        vector_parts.append(
            f"setweight(to_tsvector('{operation.config_name}', coalesce({col}, '')), '{weight}')"
        )
    
    vector_expr = " || ".join(vector_parts)
    
    # Add the search vector column
    operations.add_column(
        operation.table_name,
        sa.Column(operation.column_name, sa.dialects.postgresql.TSVECTOR)
    )
    
    # Create the expression as generated column (PostgreSQL 12+)
    operations.execute(f"""
        ALTER TABLE {operation.table_name} 
        ALTER COLUMN {operation.column_name}
        SET DEFAULT ({vector_expr})
    """)
    
    # Update existing rows
    operations.execute(f"""
        UPDATE {operation.table_name}
        SET {operation.column_name} = {vector_expr}
    """)
    
    # Create GIN index
    operations.create_index(
        f'ix_{operation.table_name}_{operation.column_name}',
        operation.table_name,
        [operation.column_name],
        postgresql_using='gin'
    )
```

**Using Custom Operations in Migrations:**

```python
"""
Example migration using custom operations

Revision ID: jkl012mno345
Revises: ghi789jkl012
"""

from alembic import op
import sqlalchemy as sa

# Import custom operations
import alembic.custom_operations


def upgrade() -> None:
    """Use custom operations for cleaner migrations."""
    
    # Safe enum creation
    op.create_enum_safe('notification_type', [
        'comment', 'like', 'follow', 'mention', 'system'
    ])
    
    # Create notifications table
    op.create_table(
        'notifications',
        sa.Column('id', sa.Integer, primary_key=True),
        sa.Column('user_id', sa.Integer, sa.ForeignKey('users.id')),
        sa.Column('type', sa.Enum('comment', 'like', 'follow', 'mention', 'system', 
                                 name='notification_type')),
        sa.Column('title', sa.String(200), nullable=False),
        sa.Column('message', sa.Text(), nullable=False),
        sa.Column('is_read', sa.Boolean(), default=False),
        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now())
    )
    
    # Setup full-text search for notifications
    op.create_fulltext_search(
        table_name='notifications',
        column_name='search_vector',
        search_columns={'title': 'A', 'message': 'B'},
        config_name='simple'
    )


def downgrade() -> None:
    """Remove notifications system."""
    op.drop_table('notifications')
    op.drop_enum_safe('notification_type')
```

**Migration Testing and Validation:**

Create **tests/test_migrations.py** for migration testing:

```python
"""
Migration testing to ensure schema changes work correctly.
"""

import pytest
from alembic import command
from alembic.config import Config
from alembic.runtime.migration import MigrationContext
from alembic.script import ScriptDirectory
from sqlalchemy import create_engine, MetaData, inspect

from app.config import get_settings


class TestMigrations:
    """Test migration functionality."""
    
    @pytest.fixture
    def alembic_config(self):
        """Create Alembic configuration for testing."""
        config = Config("alembic.ini")
        return config
    
    @pytest.fixture  
    def migration_engine(self):
        """Create test database engine for migrations."""
        settings = get_settings()
        engine = create_engine(str(settings.test_database_url))
        return engine
    
    def test_migration_up_and_down(self, alembic_config, migration_engine):
        """Test that migrations can be applied and rolled back."""
        
        with migration_engine.connect() as connection:
            context = MigrationContext.configure(connection)
            script = ScriptDirectory.from_config(alembic_config)
            
            # Get all revisions
            revisions = list(script.walk_revisions())
            
            # Test upgrade to each revision
            for rev in reversed(revisions):
                command.upgrade(alembic_config, rev.revision)
                
                # Verify schema state
                inspector = inspect(connection)
                tables = inspector.get_table_names()
                assert len(tables) > 0
                
            # Test downgrade from each revision
            for rev in revisions[:-1]:  # Don't downgrade from base
                command.downgrade(alembic_config, rev.down_revision)
    
    def test_schema_consistency(self, alembic_config, migration_engine):
        """Test that final migration schema matches model metadata."""
        
        # Apply all migrations
        command.upgrade(alembic_config, "head")
        
        # Compare with model metadata
        from app.models.base import Base
        model_metadata = Base.metadata
        
        with migration_engine.connect() as connection:
            inspector = inspect(connection)
            
            # Check all model tables exist
            model_tables = set(model_metadata.tables.keys())
            db_tables = set(inspector.get_table_names())
            
            assert model_tables.issubset(db_tables), \
                f"Missing tables: {model_tables - db_tables}"
    
    def test_data_migration_integrity(self, alembic_config, migration_engine):
        """Test that data migrations preserve data integrity."""
        
        # This would include specific tests for data migrations
        # Testing role conversion, permission assignments, etc.
        pass
```

This completes Section 3.7 - Database Migrations with Alembic! The section now covers:

✅ **3.7.1**: Basic setup and configuration
✅ **3.7.2**: Environment configuration with model discovery  
✅ **3.7.3**: Migration creation and common patterns
✅ **3.7.4**: Management commands and safety tools
✅ **3.7.5**: Advanced patterns including data migrations, PostgreSQL features, custom operations, and testing

The migration section demonstrates production-ready database management including data migrations, custom PostgreSQL features like full-text search, materialized views, triggers, and comprehensive testing strategies.

---

## Part 4: E-commerce System with Complex Relationships

This section demonstrates advanced SQLAlchemy concepts through a complete e-commerce system, including polymorphic inheritance, complex business logic, inventory management, and payment processing.

### 4.1 E-commerce Database Models

**app/models/ecommerce.py** - Complete E-commerce System:

```python
"""
Complete e-commerce system models demonstrating advanced SQLAlchemy concepts:
- Polymorphic inheritance for products and payments
- Complex business logic with inventory management  
- Advanced relationships and constraints
- Integration with existing user system
"""

import enum
from decimal import Decimal
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any

from sqlalchemy import (
    Column, Integer, String, Text, DateTime, Boolean, Numeric, 
    ForeignKey, Table, UniqueConstraint, CheckConstraint, Index,
    JSON, func, select, case, and_, or_
)
from sqlalchemy.orm import relationship, validates, column_property, backref
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.ext.declarative import declared_attr
from sqlalchemy.dialects.postgresql import UUID, JSONB, INET
from sqlalchemy.sql import expression

from app.models.base import Base
from app.models.user import User


# Enums for e-commerce
class OrderStatus(enum.Enum):
    """Order status enumeration."""
    PENDING = "pending"
    CONFIRMED = "confirmed"
    PROCESSING = "processing"
    SHIPPED = "shipped"
    DELIVERED = "delivered"
    CANCELLED = "cancelled"
    REFUNDED = "refunded"


class PaymentStatus(enum.Enum):
    """Payment status enumeration."""
    PENDING = "pending"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    REFUNDED = "refunded"


class PaymentMethod(enum.Enum):
    """Payment method enumeration."""
    CREDIT_CARD = "credit_card"
    DEBIT_CARD = "debit_card"
    PAYPAL = "paypal"
    BANK_TRANSFER = "bank_transfer"
    CRYPTOCURRENCY = "cryptocurrency"
    CASH_ON_DELIVERY = "cash_on_delivery"


class ProductType(enum.Enum):
    """Product type enumeration for polymorphic inheritance."""
    PHYSICAL = "physical"
    DIGITAL = "digital"
    SERVICE = "service"
    SUBSCRIPTION = "subscription"


class ShippingStatus(enum.Enum):
    """Shipping status enumeration."""
    PENDING = "pending"
    PROCESSING = "processing"
    IN_TRANSIT = "in_transit"
    OUT_FOR_DELIVERY = "out_for_delivery"
    DELIVERED = "delivered"
    FAILED_DELIVERY = "failed_delivery"
    RETURNED = "returned"


# Association tables for many-to-many relationships
product_categories = Table(
    'product_categories',
    Base.metadata,
    Column('product_id', Integer, ForeignKey('products.id'), primary_key=True),
    Column('category_id', Integer, ForeignKey('categories.id'), primary_key=True),
    UniqueConstraint('product_id', 'category_id', name='unique_product_category')
)

product_tags = Table(
    'product_tags', 
    Base.metadata,
    Column('product_id', Integer, ForeignKey('products.id'), primary_key=True),
    Column('tag_id', Integer, ForeignKey('tags.id'), primary_key=True),
    UniqueConstraint('product_id', 'tag_id', name='unique_product_tag')
)


class Category(Base):
    """Product categories with hierarchical structure."""
    __tablename__ = 'categories'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False)
    slug = Column(String(120), unique=True, nullable=False)
    description = Column(Text)
    parent_id = Column(Integer, ForeignKey('categories.id'), nullable=True)
    
    # SEO and metadata
    meta_title = Column(String(200))
    meta_description = Column(String(500))
    image_url = Column(String(500))
    
    # Display options
    sort_order = Column(Integer, default=0)
    is_active = Column(Boolean, default=True)
    is_featured = Column(Boolean, default=False)
    
    # Hierarchy relationships
    parent = relationship('Category', remote_side=[id], backref='children')
    products = relationship('Product', secondary=product_categories, 
                          back_populates='categories')
    
    # Analytics
    product_count = column_property(
        select(func.count(product_categories.c.product_id))
        .where(product_categories.c.category_id == id)
        .scalar_subquery()
    )
    
    @hybrid_property
    def full_path(self):
        """Get full category path (e.g., Electronics > Computers > Laptops)."""
        if self.parent:
            return f"{self.parent.full_path} > {self.name}"
        return self.name
    
    def __repr__(self):
        return f"<Category(id={self.id}, name='{self.name}')>"


class Brand(Base):
    """Product brands with metadata and relationships."""
    __tablename__ = 'brands'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, unique=True)
    slug = Column(String(120), unique=True, nullable=False)
    description = Column(Text)
    website_url = Column(String(500))
    logo_url = Column(String(500))
    
    # Brand metadata
    country_of_origin = Column(String(100))
    founded_year = Column(Integer)
    is_verified = Column(Boolean, default=False)
    
    # Relationships
    products = relationship('Product', back_populates='brand')
    
    # Analytics
    product_count = column_property(
        select(func.count('*'))
        .select_from('products')
        .where(text('products.brand_id = brands.id'))
        .scalar_subquery()
    )
    
    def __repr__(self):
        return f"<Brand(id={self.id}, name='{self.name}')>"


class Product(Base):
    """
    Base product model with polymorphic inheritance.
    Supports different product types: physical, digital, service, subscription.
    """
    __tablename__ = 'products'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(200), nullable=False)
    slug = Column(String(220), unique=True, nullable=False)
    description = Column(Text)
    short_description = Column(String(500))
    
    # Product identification
    sku = Column(String(100), unique=True, nullable=False)
    barcode = Column(String(50), unique=True, nullable=True)
    brand_id = Column(Integer, ForeignKey('brands.id'), nullable=True)
    
    # Pricing
    base_price = Column(Numeric(10, 2), nullable=False)
    sale_price = Column(Numeric(10, 2), nullable=True)
    cost_price = Column(Numeric(10, 2), nullable=True)  # For profit calculations
    
    # Product status and visibility
    is_active = Column(Boolean, default=True)
    is_featured = Column(Boolean, default=False)
    is_digital = Column(Boolean, default=False)
    requires_shipping = Column(Boolean, default=True)
    
    # SEO and metadata
    meta_title = Column(String(200))
    meta_description = Column(String(500))
    search_keywords = Column(Text)
    
    # Product specifications (JSON)
    specifications = Column(JSONB, default={})
    attributes = Column(JSONB, default={})  # Color, size, weight, etc.
    
    # Inventory tracking
    stock_quantity = Column(Integer, default=0)
    low_stock_threshold = Column(Integer, default=10)
    manage_stock = Column(Boolean, default=True)
    
    # Product metrics
    view_count = Column(Integer, default=0)
    purchase_count = Column(Integer, default=0)
    rating_average = Column(Numeric(3, 2), default=0.0)
    rating_count = Column(Integer, default=0)
    
    # Polymorphic inheritance setup
    product_type = Column(String(50), nullable=False)
    __mapper_args__ = {
        'polymorphic_identity': 'base',
        'polymorphic_on': product_type,
        'with_polymorphic': '*'
    }
    
    # Relationships
    brand = relationship('Brand', back_populates='products')
    categories = relationship('Category', secondary=product_categories, 
                            back_populates='products')
    tags = relationship('Tag', secondary=product_tags)
    
    # Inventory and orders
    inventory_logs = relationship('InventoryLog', back_populates='product')
    order_items = relationship('OrderItem', back_populates='product')
    reviews = relationship('ProductReview', back_populates='product')
    wishlists = relationship('WishlistItem', back_populates='product')
    
    # Computed properties
    @hybrid_property
    def current_price(self):
        """Get current selling price (sale price if available, otherwise base price)."""
        return self.sale_price if self.sale_price else self.base_price
    
    @current_price.expression
    def current_price(cls):
        return case(
            (cls.sale_price.isnot(None), cls.sale_price),
            else_=cls.base_price
        )
    
    @hybrid_property
    def is_on_sale(self):
        """Check if product is currently on sale."""
        return self.sale_price is not None and self.sale_price < self.base_price
    
    @hybrid_property
    def is_in_stock(self):
        """Check if product is in stock."""
        if not self.manage_stock:
            return True
        return self.stock_quantity > 0
    
    @hybrid_property
    def is_low_stock(self):
        """Check if product is low on stock."""
        if not self.manage_stock:
            return False
        return self.stock_quantity <= self.low_stock_threshold
    
    @validates('base_price', 'sale_price', 'cost_price')
    def validate_prices(self, key, value):
        """Validate that prices are positive."""
        if value is not None and value < 0:
            raise ValueError(f"{key} must be positive")
        return value
    
    @validates('stock_quantity')
    def validate_stock(self, key, value):
        """Validate stock quantity."""
        if value < 0:
            raise ValueError("Stock quantity cannot be negative")
        return value
    
    def __repr__(self):
        return f"<Product(id={self.id}, name='{self.name}', type='{self.product_type}')>"


class PhysicalProduct(Product):
    """Physical products that require shipping."""
    __tablename__ = 'physical_products'
    
    id = Column(Integer, ForeignKey('products.id'), primary_key=True)
    
    # Physical attributes
    weight = Column(Numeric(8, 3))  # in kg
    length = Column(Numeric(8, 2))  # in cm
    width = Column(Numeric(8, 2))   # in cm  
    height = Column(Numeric(8, 2))  # in cm
    
    # Shipping information
    shipping_class_id = Column(Integer, ForeignKey('shipping_classes.id'))
    fragile = Column(Boolean, default=False)
    hazardous = Column(Boolean, default=False)
    
    # Warranty and support
    warranty_period_months = Column(Integer, default=12)
    return_period_days = Column(Integer, default=30)
    
    __mapper_args__ = {
        'polymorphic_identity': 'physical'
    }
    
    # Relationships
    shipping_class = relationship('ShippingClass')
    
    @hybrid_property
    def dimensional_weight(self):
        """Calculate dimensional weight for shipping."""
        if all([self.length, self.width, self.height]):
            return (self.length * self.width * self.height) / 5000  # Standard divisor
        return None


class DigitalProduct(Product):
    """Digital products like software, ebooks, courses."""
    __tablename__ = 'digital_products'
    
    id = Column(Integer, ForeignKey('products.id'), primary_key=True)
    
    # Digital delivery
    download_url = Column(String(500))
    download_limit = Column(Integer, default=5)  # Number of allowed downloads
    access_duration_days = Column(Integer)  # How long access is valid
    file_size_mb = Column(Numeric(10, 2))
    
    # License information
    license_type = Column(String(100))  # Single user, multi-user, enterprise
    license_terms = Column(Text)
    
    # Digital content metadata
    content_type = Column(String(50))  # PDF, video, software, etc.
    supported_platforms = Column(JSONB, default=list)  # ['windows', 'mac', 'linux']
    requirements = Column(JSONB, default={})  # System requirements
    
    __mapper_args__ = {
        'polymorphic_identity': 'digital'
    }
    
    # Digital-specific relationships
    downloads = relationship('ProductDownload', back_populates='product')


class ServiceProduct(Product):
    """Service-based products like consultations, installations.""" 
    __tablename__ = 'service_products'
    
    id = Column(Integer, ForeignKey('products.id'), primary_key=True)
    
    # Service details
    duration_minutes = Column(Integer)  # Service duration
    location_type = Column(String(50))  # remote, on_site, in_store
    advance_booking_hours = Column(Integer, default=24)  # Minimum booking notice
    
    # Availability
    max_bookings_per_day = Column(Integer, default=8)
    available_days = Column(JSONB, default=list)  # ['monday', 'tuesday', ...]
    available_hours_start = Column(String(10), default='09:00')  # 24hr format
    available_hours_end = Column(String(10), default='17:00')
    
    # Service provider information
    provider_user_id = Column(Integer, ForeignKey('users.id'), nullable=True)
    qualifications = Column(JSONB, default=list)  # Required qualifications
    
    __mapper_args__ = {
        'polymorphic_identity': 'service'
    }
    
    # Service relationships
    provider = relationship('User', foreign_keys=[provider_user_id])
    bookings = relationship('ServiceBooking', back_populates='service')


class SubscriptionProduct(Product):
    """Subscription-based products with recurring billing."""
    __tablename__ = 'subscription_products'
    
    id = Column(Integer, ForeignKey('products.id'), primary_key=True)
    
    # Subscription details
    billing_period = Column(String(20), nullable=False)  # monthly, quarterly, yearly
    billing_interval = Column(Integer, default=1)  # Every N periods
    trial_period_days = Column(Integer, default=0)  # Free trial duration
    
    # Subscription limits
    max_users = Column(Integer)  # For team subscriptions
    max_storage_gb = Column(Integer)  # Storage limits
    feature_limits = Column(JSONB, default={})  # Feature-specific limits
    
    # Setup and cancellation
    setup_fee = Column(Numeric(10, 2), default=0)
    cancellation_period_days = Column(Integer, default=30)  # Notice period
    auto_renewal = Column(Boolean, default=True)
    
    __mapper_args__ = {
        'polymorphic_identity': 'subscription'
    }
    
    # Subscription relationships
    subscriptions = relationship('UserSubscription', back_populates='product')


# Shopping cart and orders
class Cart(Base):
    """User shopping cart for temporary item storage."""
    __tablename__ = 'carts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    session_id = Column(String(255), nullable=True)  # For anonymous users
    
    # Cart metadata
    currency = Column(String(3), default='USD')
    expires_at = Column(DateTime, nullable=True)
    
    # Relationships
    user = relationship('User')
    items = relationship('CartItem', back_populates='cart', cascade='all, delete-orphan')
    
    # Computed properties
    @hybrid_property
    def total_items(self):
        return sum(item.quantity for item in self.items)
    
    @hybrid_property
    def total_price(self):
        return sum(item.total_price for item in self.items)


class CartItem(Base):
    """Individual items in shopping cart."""
    __tablename__ = 'cart_items'
    
    id = Column(Integer, primary_key=True)
    cart_id = Column(Integer, ForeignKey('carts.id'), nullable=False)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    
    quantity = Column(Integer, nullable=False, default=1)
    unit_price = Column(Numeric(10, 2), nullable=False)  # Price at time of adding
    
    # Product customization
    product_options = Column(JSONB, default={})  # Size, color, etc.
    custom_notes = Column(Text)
    
    # Constraints
    __table_args__ = (
        CheckConstraint('quantity > 0', name='positive_quantity'),
        CheckConstraint('unit_price >= 0', name='non_negative_price'),
        UniqueConstraint('cart_id', 'product_id', 'product_options', 
                        name='unique_cart_product_options')
    )
    
    # Relationships
    cart = relationship('Cart', back_populates='items')
    product = relationship('Product')
    
    @hybrid_property
    def total_price(self):
        return self.quantity * self.unit_price

class Order(Base):
    """
    Complete order management with status tracking and business logic.
    Demonstrates transaction management and complex business workflows.
    """
    __tablename__ = 'orders'
    
    id = Column(Integer, primary_key=True)
    order_number = Column(String(50), unique=True, nullable=False)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Order status and workflow
    status = Column(SQLEnum(OrderStatus), default=OrderStatus.PENDING, nullable=False)
    placed_at = Column(DateTime, server_default=func.now(), nullable=False)
    confirmed_at = Column(DateTime, nullable=True)
    shipped_at = Column(DateTime, nullable=True)
    delivered_at = Column(DateTime, nullable=True)
    cancelled_at = Column(DateTime, nullable=True)
    
    # Pricing and totals
    subtotal = Column(Numeric(10, 2), nullable=False)
    tax_amount = Column(Numeric(10, 2), default=0)
    shipping_cost = Column(Numeric(10, 2), default=0)
    discount_amount = Column(Numeric(10, 2), default=0)
    total_amount = Column(Numeric(10, 2), nullable=False)
    currency = Column(String(3), default='USD')
    
    # Customer information (snapshot at order time)
    customer_email = Column(String(255), nullable=False)
    customer_phone = Column(String(20))
    billing_address = Column(JSONB, nullable=False)
    shipping_address = Column(JSONB, nullable=True)
    
    # Order metadata
    notes = Column(Text)
    internal_notes = Column(Text)  # Staff-only notes
    source = Column(String(50), default='web')  # web, mobile, api, admin
    ip_address = Column(INET)
    user_agent = Column(Text)
    
    # Fulfillment tracking
    tracking_number = Column(String(100))
    shipping_carrier = Column(String(100))
    estimated_delivery = Column(DateTime)
    
    # Constraints
    __table_args__ = (
        CheckConstraint('subtotal >= 0', name='positive_subtotal'),
        CheckConstraint('total_amount >= 0', name='positive_total'),
        CheckConstraint('tax_amount >= 0', name='non_negative_tax'),
        CheckConstraint('shipping_cost >= 0', name='non_negative_shipping'),
        Index('ix_orders_status_placed', 'status', 'placed_at'),
        Index('ix_orders_user_status', 'user_id', 'status'),
    )
    
    # Relationships
    user = relationship('User')
    items = relationship('OrderItem', back_populates='order', cascade='all, delete-orphan')
    payments = relationship('Payment', back_populates='order')
    shipments = relationship('Shipment', back_populates='order')
    refunds = relationship('Refund', back_populates='order')
    
    # Business logic methods
    @hybrid_property
    def is_paid(self):
        """Check if order is fully paid."""
        paid_amount = sum(p.amount for p in self.payments if p.status == PaymentStatus.COMPLETED)
        return paid_amount >= self.total_amount
    
    @hybrid_property
    def outstanding_amount(self):
        """Calculate remaining amount to be paid."""
        paid_amount = sum(p.amount for p in self.payments if p.status == PaymentStatus.COMPLETED)
        return max(0, self.total_amount - paid_amount)
    
    @hybrid_property
    def can_be_cancelled(self):
        """Check if order can be cancelled."""
        cancellable_statuses = {OrderStatus.PENDING, OrderStatus.CONFIRMED}
        return self.status in cancellable_statuses
    
    @validates('total_amount')
    def validate_total(self, key, value):
        """Validate that total amount matches calculated total."""
        if hasattr(self, 'subtotal') and hasattr(self, 'tax_amount') and hasattr(self, 'shipping_cost'):
            calculated_total = (self.subtotal or 0) + (self.tax_amount or 0) + (self.shipping_cost or 0) - (self.discount_amount or 0)
            if abs(value - calculated_total) > 0.01:  # Allow for rounding
                raise ValueError("Total amount doesn't match calculated total")
        return value
    
    def calculate_totals(self):
        """Recalculate order totals from items."""
        self.subtotal = sum(item.total_price for item in self.items)
        # Tax and shipping would be calculated based on business rules
        self.total_amount = self.subtotal + self.tax_amount + self.shipping_cost - self.discount_amount
    
    def __repr__(self):
        return f"<Order(id={self.id}, number='{self.order_number}', status='{self.status.value}')>"


class OrderItem(Base):
    """Individual items within an order."""
    __tablename__ = 'order_items'
    
    id = Column(Integer, primary_key=True)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=False)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    
    # Item details (snapshot at order time)
    product_name = Column(String(200), nullable=False)  # Product name at time of order
    product_sku = Column(String(100), nullable=False)   # SKU at time of order
    quantity = Column(Integer, nullable=False)
    unit_price = Column(Numeric(10, 2), nullable=False)
    total_price = Column(Numeric(10, 2), nullable=False)
    
    # Product customization at order time
    product_options = Column(JSONB, default={})
    product_snapshot = Column(JSONB, default={})  # Full product data at order time
    
    # Fulfillment tracking
    quantity_shipped = Column(Integer, default=0)
    quantity_delivered = Column(Integer, default=0)
    quantity_returned = Column(Integer, default=0)
    
    # Constraints
    __table_args__ = (
        CheckConstraint('quantity > 0', name='positive_item_quantity'),
        CheckConstraint('unit_price >= 0', name='non_negative_item_price'),
        CheckConstraint('quantity_shipped <= quantity', name='shipped_not_exceeding_ordered'),
        CheckConstraint('quantity_delivered <= quantity_shipped', name='delivered_not_exceeding_shipped'),
    )
    
    # Relationships
    order = relationship('Order', back_populates='items')
    product = relationship('Product')
    
    @hybrid_property
    def quantity_pending_shipment(self):
        """Calculate quantity still pending shipment."""
        return self.quantity - self.quantity_shipped
    
    @hybrid_property
    def is_fully_shipped(self):
        """Check if item is fully shipped."""
        return self.quantity_shipped >= self.quantity
    
    def __repr__(self):
        return f"<OrderItem(id={self.id}, product='{self.product_name}', qty={self.quantity})>"
```

### 4.2 Order and Payment Processing

**Continue app/models/ecommerce.py** - Payment and Transaction Models:

```python

# Payment processing with polymorphic inheritance
class Payment(Base):
    """
    Base payment model with polymorphic inheritance for different payment methods.
    Handles payment processing, refunds, and transaction tracking.
    """
    __tablename__ = 'payments'
    
    id = Column(Integer, primary_key=True)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=False)
    
    # Payment identification
    payment_number = Column(String(50), unique=True, nullable=False)
    external_transaction_id = Column(String(200))  # From payment provider
    payment_method = Column(SQLEnum(PaymentMethod), nullable=False)
    payment_provider = Column(String(100))  # stripe, paypal, etc.
    
    # Payment amounts
    amount = Column(Numeric(10, 2), nullable=False)
    currency = Column(String(3), default='USD')
    exchange_rate = Column(Numeric(10, 6), default=1.0)  # For multi-currency
    
    # Payment status and timing
    status = Column(SQLEnum(PaymentStatus), default=PaymentStatus.PENDING)
    initiated_at = Column(DateTime, server_default=func.now())
    completed_at = Column(DateTime, nullable=True)
    failed_at = Column(DateTime, nullable=True)
    
    # Payment details
    payment_details = Column(JSONB, default={})  # Method-specific data
    failure_reason = Column(Text)
    gateway_response = Column(JSONB, default={})  # Full gateway response
    
    # Risk assessment
    risk_score = Column(Numeric(5, 2))  # 0-100 risk score
    fraud_check_passed = Column(Boolean)
    
    # Polymorphic inheritance
    payment_type = Column(String(50), nullable=False)
    __mapper_args__ = {
        'polymorphic_identity': 'base',
        'polymorphic_on': payment_type,
        'with_polymorphic': '*'
    }
    
    # Relationships
    order = relationship('Order', back_populates='payments')
    refunds = relationship('Refund', back_populates='payment')
    
    # Constraints
    __table_args__ = (
        CheckConstraint('amount > 0', name='positive_payment_amount'),
        CheckConstraint('risk_score >= 0 AND risk_score <= 100', name='valid_risk_score'),
        Index('ix_payments_status_method', 'status', 'payment_method'),
        Index('ix_payments_order_status', 'order_id', 'status'),
    )
    
    @hybrid_property
    def is_successful(self):
        """Check if payment was successful."""
        return self.status == PaymentStatus.COMPLETED
    
    @hybrid_property
    def is_refundable(self):
        """Check if payment can be refunded."""
        if self.status != PaymentStatus.COMPLETED:
            return False
        # Add business logic for refund eligibility
        refunded_amount = sum(r.amount for r in self.refunds)
        return refunded_amount < self.amount
    
    def __repr__(self):
        return f"<Payment(id={self.id}, amount={self.amount}, status='{self.status.value}')>"


class CreditCardPayment(Payment):
    """Credit card payment details."""
    __tablename__ = 'credit_card_payments'
    
    id = Column(Integer, ForeignKey('payments.id'), primary_key=True)
    
    # Card details (PCI compliant - no sensitive data)
    card_brand = Column(String(50))  # Visa, MasterCard, etc.
    last_four_digits = Column(String(4))
    expiry_month = Column(Integer)
    expiry_year = Column(Integer)
    
    # Transaction details
    authorization_code = Column(String(50))
    cvv_result = Column(String(10))
    avs_result = Column(String(10))  # Address Verification System
    
    __mapper_args__ = {
        'polymorphic_identity': 'credit_card'
    }


class PayPalPayment(Payment):
    """PayPal payment details."""
    __tablename__ = 'paypal_payments'
    
    id = Column(Integer, ForeignKey('payments.id'), primary_key=True)
    
    # PayPal-specific details
    paypal_transaction_id = Column(String(200))
    paypal_payer_id = Column(String(100))
    paypal_email = Column(String(255))
    
    __mapper_args__ = {
        'polymorphic_identity': 'paypal'
    }


class BankTransferPayment(Payment):
    """Bank transfer payment details."""
    __tablename__ = 'bank_transfer_payments'
    
    id = Column(Integer, ForeignKey('payments.id'), primary_key=True)
    
    # Bank details
    bank_name = Column(String(200))
    account_number_masked = Column(String(20))  # Last 4 digits only
    routing_number = Column(String(20))
    transfer_reference = Column(String(100))
    
    __mapper_args__ = {
        'polymorphic_identity': 'bank_transfer'
    }


class Refund(Base):
    """Payment refunds with tracking and approval workflow."""
    __tablename__ = 'refunds'
    
    id = Column(Integer, primary_key=True)
    payment_id = Column(Integer, ForeignKey('payments.id'), nullable=False)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=False)
    
    # Refund details
    refund_number = Column(String(50), unique=True, nullable=False)
    amount = Column(Numeric(10, 2), nullable=False)
    reason = Column(String(500))
    notes = Column(Text)
    
    # Approval workflow
    requested_by_user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    approved_by_user_id = Column(Integer, ForeignKey('users.id'), nullable=True)
    status = Column(String(50), default='pending')  # pending, approved, processed, rejected
    
    # Timing
    requested_at = Column(DateTime, server_default=func.now())
    approved_at = Column(DateTime, nullable=True)
    processed_at = Column(DateTime, nullable=True)
    
    # External processing
    external_refund_id = Column(String(200))  # From payment processor
    processor_response = Column(JSONB, default={})
    
    # Constraints
    __table_args__ = (
        CheckConstraint('amount > 0', name='positive_refund_amount'),
    )
    
    # Relationships
    payment = relationship('Payment', back_populates='refunds')
    order = relationship('Order', back_populates='refunds')
    requested_by = relationship('User', foreign_keys=[requested_by_user_id])
    approved_by = relationship('User', foreign_keys=[approved_by_user_id])
    
    def __repr__(self):
        return f"<Refund(id={self.id}, amount={self.amount}, status='{self.status}')>"


class Address(Base):
    """Customer addresses for billing and shipping."""
    __tablename__ = 'addresses'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Address details
    label = Column(String(100))  # Home, Work, etc.
    first_name = Column(String(100), nullable=False)
    last_name = Column(String(100), nullable=False)
    company = Column(String(200))
    address_line_1 = Column(String(200), nullable=False)
    address_line_2 = Column(String(200))
    city = Column(String(100), nullable=False)
    state_province = Column(String(100))
    postal_code = Column(String(20), nullable=False)
    country = Column(String(100), nullable=False)
    
    # Contact information
    phone = Column(String(20))
    
    # Address flags
    is_default_billing = Column(Boolean, default=False)
    is_default_shipping = Column(Boolean, default=False)
    is_verified = Column(Boolean, default=False)
    
    # Geocoding (for shipping calculations)
    latitude = Column(Numeric(10, 8))
    longitude = Column(Numeric(11, 8))
    
    # Relationships
    user = relationship('User')
    
    @hybrid_property
    def full_name(self):
        """Get full name for address."""
        return f"{self.first_name} {self.last_name}"
    
    @hybrid_property
    def formatted_address(self):
        """Get formatted address string."""
        lines = [self.address_line_1]
        if self.address_line_2:
            lines.append(self.address_line_2)
        lines.append(f"{self.city}, {self.state_province} {self.postal_code}")
        lines.append(self.country)
        return '\n'.join(lines)
    
    def __repr__(self):
        return f"<Address(id={self.id}, label='{self.label}', city='{self.city}')>"


class ShippingClass(Base):
    """Shipping classes for different product types."""
    __tablename__ = 'shipping_classes'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, unique=True)
    description = Column(Text)
    
    # Shipping costs
    base_cost = Column(Numeric(8, 2), default=0)
    cost_per_kg = Column(Numeric(8, 2), default=0)
    free_shipping_threshold = Column(Numeric(10, 2))  # Free shipping above this amount
    
    # Delivery estimates
    min_delivery_days = Column(Integer, default=1)
    max_delivery_days = Column(Integer, default=7)
    
    # Restrictions
    restricted_countries = Column(JSONB, default=list)  # Countries where shipping not allowed
    
    # Relationships
    products = relationship('PhysicalProduct', back_populates='shipping_class')
    
    def calculate_shipping_cost(self, weight: float, destination_country: str, order_value: float) -> Decimal:
        """Calculate shipping cost for given parameters."""
        if destination_country in (self.restricted_countries or []):
            raise ValueError(f"Shipping not available to {destination_country}")
        
        # Free shipping if threshold met
        if self.free_shipping_threshold and order_value >= self.free_shipping_threshold:
            return Decimal('0.00')
        
        # Calculate cost
        cost = self.base_cost + (Decimal(str(weight)) * self.cost_per_kg)
        return cost
    
    def __repr__(self):
        return f"<ShippingClass(id={self.id}, name='{self.name}')>"


class Shipment(Base):
    """Order shipment tracking."""
    __tablename__ = 'shipments'
    
    id = Column(Integer, primary_key=True)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=False)
    
    # Shipment details
    tracking_number = Column(String(100), unique=True, nullable=False)
    carrier = Column(String(100), nullable=False)
    service_type = Column(String(100))  # Standard, Express, Overnight
    
    # Status and timing
    status = Column(SQLEnum(ShippingStatus), default=ShippingStatus.PENDING)
    shipped_at = Column(DateTime)
    estimated_delivery = Column(DateTime)
    actual_delivery = Column(DateTime)
    
    # Package details
    weight_kg = Column(Numeric(8, 3))
    dimensions = Column(JSONB, default={})  # {length, width, height}
    package_count = Column(Integer, default=1)
    
    # Shipping cost
    shipping_cost = Column(Numeric(8, 2))
    insurance_cost = Column(Numeric(8, 2), default=0)
    
    # Delivery details
    delivery_signature = Column(String(200))
    delivery_notes = Column(Text)
    delivery_photo_url = Column(String(500))
    
    # Relationships
    order = relationship('Order', back_populates='shipments')
    tracking_events = relationship('ShipmentTracking', back_populates='shipment')
    
    @hybrid_property
    def is_delivered(self):
        """Check if shipment is delivered."""
        return self.status == ShippingStatus.DELIVERED
    
    def __repr__(self):
        return f"<Shipment(id={self.id}, tracking='{self.tracking_number}', status='{self.status.value}')>"


class ShipmentTracking(Base):
    """Individual tracking events for shipments."""
    __tablename__ = 'shipment_tracking'
    
    id = Column(Integer, primary_key=True)
    shipment_id = Column(Integer, ForeignKey('shipments.id'), nullable=False)
    
    # Tracking event details
    status = Column(SQLEnum(ShippingStatus), nullable=False)
    location = Column(String(200))
    description = Column(Text)
    event_time = Column(DateTime, nullable=False)
    
    # Provider data
    raw_data = Column(JSONB, default={})  # Raw tracking data from carrier
    
    # Relationships
    shipment = relationship('Shipment', back_populates='tracking_events')
    
    def __repr__(self):
        return f"<ShipmentTracking(shipment_id={self.shipment_id}, status='{self.status.value}')>"


class InventoryLog(Base):
    """Inventory movement tracking for stock management."""
    __tablename__ = 'inventory_logs'
    
    id = Column(Integer, primary_key=True)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    
    # Movement details
    movement_type = Column(String(50), nullable=False)  # purchase, sale, adjustment, return
    quantity_change = Column(Integer, nullable=False)  # Positive = increase, negative = decrease
    previous_quantity = Column(Integer, nullable=False)
    new_quantity = Column(Integer, nullable=False)
    
    # Reference information
    reference_type = Column(String(50))  # order, return, manual, supplier
    reference_id = Column(Integer)  # ID of the reference object
    
    # Metadata
    reason = Column(String(500))
    notes = Column(Text)
    performed_by_user_id = Column(Integer, ForeignKey('users.id'))
    cost_per_unit = Column(Numeric(10, 2))  # Cost for purchases
    
    # Constraints
    __table_args__ = (
        CheckConstraint('new_quantity >= 0', name='non_negative_inventory'),
        Index('ix_inventory_logs_product_date', 'product_id', 'created_at'),
    )
    
    # Relationships
    product = relationship('Product', back_populates='inventory_logs')
    performed_by = relationship('User')
    
    def __repr__(self):
        return f"<InventoryLog(product_id={self.product_id}, change={self.quantity_change})>"


# Product reviews and ratings
class ProductReview(Base):
    """Product reviews and ratings from customers.""" 
    __tablename__ = 'product_reviews'
    
    id = Column(Integer, primary_key=True)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=True)  # Verified purchase
    
    # Review content
    title = Column(String(200))
    content = Column(Text, nullable=False)
    rating = Column(Integer, nullable=False)  # 1-5 stars
    
    # Review metadata
    is_verified_purchase = Column(Boolean, default=False)
    is_featured = Column(Boolean, default=False)
    
    # Moderation
    status = Column(String(50), default='pending')  # pending, approved, rejected
    moderated_by_user_id = Column(Integer, ForeignKey('users.id'), nullable=True)
    moderated_at = Column(DateTime, nullable=True)
    moderation_reason = Column(Text)
    
    # Helpfulness tracking
    helpful_votes = Column(Integer, default=0)
    total_votes = Column(Integer, default=0)
    
    # Constraints
    __table_args__ = (
        CheckConstraint('rating >= 1 AND rating <= 5', name='valid_rating'),
        CheckConstraint('helpful_votes <= total_votes', name='helpful_not_exceeding_total'),
        UniqueConstraint('product_id', 'user_id', 'order_id', name='unique_review_per_order'),
        Index('ix_product_reviews_rating_status', 'product_id', 'rating', 'status'),
    )
    
    # Relationships
    product = relationship('Product', back_populates='reviews')
    user = relationship('User')
    order = relationship('Order')
    moderated_by = relationship('User', foreign_keys=[moderated_by_user_id])
    
    @hybrid_property
    def helpfulness_ratio(self):
        """Calculate helpfulness ratio."""
        if self.total_votes == 0:
            return 0
        return self.helpful_votes / self.total_votes
    
    def __repr__(self):
        return f"<ProductReview(id={self.id}, product_id={self.product_id}, rating={self.rating})>"


class Wishlist(Base):
    """User wishlists for saved products."""
    __tablename__ = 'wishlists'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False, default='My Wishlist')
    description = Column(Text)
    is_public = Column(Boolean, default=False)
    is_default = Column(Boolean, default=True)
    
    # Relationships
    user = relationship('User')
    items = relationship('WishlistItem', back_populates='wishlist', cascade='all, delete-orphan')
    
    @hybrid_property
    def total_items(self):
        """Count total items in wishlist."""
        return len(self.items)
    
    @hybrid_property
    def total_value(self):
        """Calculate total value of wishlist."""
        return sum(item.product.current_price for item in self.items)
    
    def __repr__(self):
        return f"<Wishlist(id={self.id}, name='{self.name}', user_id={self.user_id})>"


class WishlistItem(Base):
    """Individual items in a wishlist."""
    __tablename__ = 'wishlist_items'
    
    id = Column(Integer, primary_key=True)
    wishlist_id = Column(Integer, ForeignKey('wishlists.id'), nullable=False)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    
    # Item preferences
    preferred_options = Column(JSONB, default={})  # Preferred size, color, etc.
    notes = Column(Text)
    priority = Column(Integer, default=1)  # 1=high, 2=medium, 3=low
    
    # Price tracking
    price_when_added = Column(Numeric(10, 2))
    notify_on_sale = Column(Boolean, default=False)
    target_price = Column(Numeric(10, 2))  # Notify when price drops below this
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('wishlist_id', 'product_id', name='unique_wishlist_product'),
        CheckConstraint('priority IN (1, 2, 3)', name='valid_priority'),
    )
    
    # Relationships
    wishlist = relationship('Wishlist', back_populates='items')
    product = relationship('Product', back_populates='wishlists')
    
    @hybrid_property
    def price_dropped(self):
        """Check if price has dropped since adding to wishlist."""
        if not self.price_when_added:
            return False
        return self.product.current_price < self.price_when_added
    
    def __repr__(self):
        return f"<WishlistItem(id={self.id}, product_id={self.product_id})>"
```

### 4.3 Inventory Management and Business Logic

**Continue app/models/ecommerce.py** - Inventory and Business Rules:

```python

class Supplier(Base):
    """Product suppliers and vendor management."""
    __tablename__ = 'suppliers'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(200), nullable=False)
    contact_person = Column(String(200))
    email = Column(String(255))
    phone = Column(String(20))
    
    # Address information
    address = Column(JSONB)  # Full address as JSON
    
    # Business details
    tax_id = Column(String(50))
    payment_terms = Column(String(100))  # Net 30, COD, etc.
    currency = Column(String(3), default='USD')
    
    # Supplier rating and performance
    rating = Column(Numeric(3, 2), default=0.0)
    on_time_delivery_rate = Column(Numeric(5, 2), default=100.0)  # Percentage
    quality_rating = Column(Numeric(3, 2), default=0.0)
    
    # Status
    is_active = Column(Boolean, default=True)
    is_preferred = Column(Boolean, default=False)
    
    # Relationships
    purchase_orders = relationship('PurchaseOrder', back_populates='supplier')
    product_suppliers = relationship('ProductSupplier', back_populates='supplier')
    
    def __repr__(self):
        return f"<Supplier(id={self.id}, name='{self.name}')>"


class ProductSupplier(Base):
    """Many-to-many relationship between products and suppliers with pricing."""
    __tablename__ = 'product_suppliers'
    
    id = Column(Integer, primary_key=True)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    supplier_id = Column(Integer, ForeignKey('suppliers.id'), nullable=False)
    
    # Supplier-specific product details
    supplier_sku = Column(String(100))  # Supplier's SKU for this product
    supplier_name = Column(String(200))  # Supplier's name for this product
    cost_price = Column(Numeric(10, 2), nullable=False)
    minimum_order_quantity = Column(Integer, default=1)
    
    # Lead times
    lead_time_days = Column(Integer, default=7)
    
    # Status
    is_preferred = Column(Boolean, default=False)
    is_active = Column(Boolean, default=True)
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('product_id', 'supplier_id', name='unique_product_supplier'),
        CheckConstraint('cost_price > 0', name='positive_supplier_cost'),
        CheckConstraint('minimum_order_quantity > 0', name='positive_min_order'),
    )
    
    # Relationships
    product = relationship('Product')
    supplier = relationship('Supplier', back_populates='product_suppliers')
    
    def __repr__(self):
        return f"<ProductSupplier(product_id={self.product_id}, supplier_id={self.supplier_id})>"


class PurchaseOrder(Base):
    """Purchase orders for inventory replenishment.""" 
    __tablename__ = 'purchase_orders'
    
    id = Column(Integer, primary_key=True)
    po_number = Column(String(50), unique=True, nullable=False)
    supplier_id = Column(Integer, ForeignKey('suppliers.id'), nullable=False)
    
    # Order details
    status = Column(String(50), default='pending')  # pending, sent, confirmed, received, cancelled
    total_amount = Column(Numeric(12, 2), nullable=False)
    currency = Column(String(3), default='USD')
    
    # Timing
    order_date = Column(DateTime, server_default=func.now())
    expected_delivery = Column(DateTime)
    actual_delivery = Column(DateTime)
    
    # Payment terms
    payment_terms = Column(String(100))
    payment_due_date = Column(DateTime)
    
    # Notes and attachments
    notes = Column(Text)
    attachment_urls = Column(JSONB, default=list)
    
    # Created by
    created_by_user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Relationships
    supplier = relationship('Supplier', back_populates='purchase_orders')
    created_by = relationship('User')
    items = relationship('PurchaseOrderItem', back_populates='purchase_order')
    
    def __repr__(self):
        return f"<PurchaseOrder(id={self.id}, po_number='{self.po_number}', status='{self.status}')>"


class PurchaseOrderItem(Base):
    """Items within a purchase order."""
    __tablename__ = 'purchase_order_items'
    
    id = Column(Integer, primary_key=True)
    purchase_order_id = Column(Integer, ForeignKey('purchase_orders.id'), nullable=False)
    product_id = Column(Integer, ForeignKey('products.id'), nullable=False)
    
    # Order details
    quantity_ordered = Column(Integer, nullable=False)
    quantity_received = Column(Integer, default=0)
    unit_cost = Column(Numeric(10, 2), nullable=False)
    total_cost = Column(Numeric(10, 2), nullable=False)
    
    # Quality control
    quantity_accepted = Column(Integer, default=0)
    quantity_rejected = Column(Integer, default=0)
    rejection_reason = Column(Text)
    
    # Constraints
    __table_args__ = (
        CheckConstraint('quantity_ordered > 0', name='positive_ordered_quantity'),
        CheckConstraint('quantity_received <= quantity_ordered', name='received_not_exceeding_ordered'),
        CheckConstraint('unit_cost > 0', name='positive_unit_cost'),
    )
    
    # Relationships
    purchase_order = relationship('PurchaseOrder', back_populates='items')
    product = relationship('Product')
    
    @hybrid_property
    def quantity_pending(self):
        """Calculate quantity still pending receipt."""
        return self.quantity_ordered - self.quantity_received
    
    def __repr__(self):
        return f"<PurchaseOrderItem(id={self.id}, product_id={self.product_id}, qty={self.quantity_ordered})>"

# Digital product delivery tracking
class ProductDownload(Base):
    """Track digital product downloads and access."""
    __tablename__ = 'product_downloads'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    product_id = Column(Integer, ForeignKey('digital_products.id'), nullable=False)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=False)
    
    # Download tracking
    download_token = Column(String(255), unique=True, nullable=False)
    download_count = Column(Integer, default=0)
    max_downloads = Column(Integer, default=5)
    
    # Access control
    expires_at = Column(DateTime)
    first_accessed_at = Column(DateTime)
    last_accessed_at = Column(DateTime)
    
    # Download metadata
    file_size_bytes = Column(Integer)
    download_ip = Column(INET)
    user_agent = Column(Text)
    
    # Relationships
    user = relationship('User')
    product = relationship('DigitalProduct', back_populates='downloads')
    order = relationship('Order')
    
    @hybrid_property
    def downloads_remaining(self):
        """Calculate remaining download attempts."""
        return max(0, self.max_downloads - self.download_count)
    
    @hybrid_property
    def is_expired(self):
        """Check if download access has expired."""
        if self.expires_at:
            return datetime.utcnow() > self.expires_at
        return False
    
    def __repr__(self):
        return f"<ProductDownload(id={self.id}, user_id={self.user_id}, downloads={self.download_count})>"


# Service booking system
class ServiceBooking(Base):
    """Service bookings for service-type products."""
    __tablename__ = 'service_bookings'
    
    id = Column(Integer, primary_key=True)
    service_id = Column(Integer, ForeignKey('service_products.id'), nullable=False)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=True)
    
    # Booking details
    booking_reference = Column(String(50), unique=True, nullable=False)
    scheduled_date = Column(DateTime, nullable=False)
    duration_minutes = Column(Integer, nullable=False)
    
    # Status tracking
    status = Column(String(50), default='pending')  # pending, confirmed, in_progress, completed, cancelled
    confirmed_at = Column(DateTime)
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    cancelled_at = Column(DateTime)
    
    # Service delivery
    location_type = Column(String(50))  # remote, on_site, in_store
    location_details = Column(JSONB, default={})  # Address or meeting details
    
    # Service provider assignment
    assigned_provider_id = Column(Integer, ForeignKey('users.id'), nullable=True)
    
    # Customer information
    customer_requirements = Column(Text)
    special_instructions = Column(Text)
    
    # Service completion
    service_notes = Column(Text)  # Provider notes after service
    customer_rating = Column(Integer)  # 1-5 rating from customer
    provider_rating = Column(Integer)  # 1-5 rating from provider
    
    # Relationships
    service = relationship('ServiceProduct', back_populates='bookings')
    user = relationship('User', foreign_keys=[user_id])
    assigned_provider = relationship('User', foreign_keys=[assigned_provider_id])
    order = relationship('Order')
    
    def __repr__(self):
        return f"<ServiceBooking(id={self.id}, reference='{self.booking_reference}', status='{self.status}')>"


# Subscription management
class UserSubscription(Base):
    """User subscriptions for subscription products."""
    __tablename__ = 'user_subscriptions'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    product_id = Column(Integer, ForeignKey('subscription_products.id'), nullable=False)
    
    # Subscription details
    subscription_reference = Column(String(50), unique=True, nullable=False)
    status = Column(String(50), default='active')  # active, paused, cancelled, expired
    
    # Billing information
    started_at = Column(DateTime, server_default=func.now())
    trial_ends_at = Column(DateTime)
    next_billing_date = Column(DateTime, nullable=False)
    cancelled_at = Column(DateTime)
    expires_at = Column(DateTime)
    
    # Billing history
    billing_cycles_completed = Column(Integer, default=0)
    total_amount_paid = Column(Numeric(12, 2), default=0)
    
    # Usage tracking for limited subscriptions
    current_users = Column(Integer, default=1)
    storage_used_gb = Column(Numeric(10, 2), default=0)
    feature_usage = Column(JSONB, default={})
    
    # Cancellation details
    cancellation_reason = Column(Text)
    cancelled_by_user_id = Column(Integer, ForeignKey('users.id'))
    
    # Relationships
    user = relationship('User')
    product = relationship('SubscriptionProduct', back_populates='subscriptions')
    cancelled_by = relationship('User', foreign_keys=[cancelled_by_user_id])
    billing_history = relationship('SubscriptionBilling', back_populates='subscription')
    
    @hybrid_property
    def is_in_trial(self):
        """Check if subscription is in trial period."""
        if not self.trial_ends_at:
            return False
        return datetime.utcnow() < self.trial_ends_at
    
    @hybrid_property
    def days_until_billing(self):
        """Calculate days until next billing."""
        if not self.next_billing_date:
            return None
        delta = self.next_billing_date - datetime.utcnow()
        return delta.days if delta.days > 0 else 0
    
    def __repr__(self):
        return f"<UserSubscription(id={self.id}, user_id={self.user_id}, status='{self.status}')>"


class SubscriptionBilling(Base):
    """Billing history for subscriptions."""
    __tablename__ = 'subscription_billing'
    
    id = Column(Integer, primary_key=True)
    subscription_id = Column(Integer, ForeignKey('user_subscriptions.id'), nullable=False)
    
    # Billing period
    billing_period_start = Column(DateTime, nullable=False)
    billing_period_end = Column(DateTime, nullable=False)
    amount = Column(Numeric(10, 2), nullable=False)
    currency = Column(String(3), default='USD')
    
    # Payment processing
    payment_id = Column(Integer, ForeignKey('payments.id'), nullable=True)
    status = Column(String(50), default='pending')  # pending, paid, failed, refunded
    due_date = Column(DateTime, nullable=False)
    paid_at = Column(DateTime)
    
    # Usage-based billing (for metered subscriptions)
    usage_data = Column(JSONB, default={})
    
    # Relationships
    subscription = relationship('UserSubscription', back_populates='billing_history')
    payment = relationship('Payment')
    
    def __repr__(self):
        return f"<SubscriptionBilling(id={self.id}, amount={self.amount}, status='{self.status}')>"


# Promotional and discount system
class Coupon(Base):
    """Discount coupons and promotional codes."""
    __tablename__ = 'coupons'
    
    id = Column(Integer, primary_key=True)
    code = Column(String(50), unique=True, nullable=False)
    name = Column(String(200), nullable=False)
    description = Column(Text)
    
    # Discount configuration
    discount_type = Column(String(20), nullable=False)  # percentage, fixed_amount, free_shipping
    discount_value = Column(Numeric(10, 2), nullable=False)
    max_discount_amount = Column(Numeric(10, 2))  # Cap for percentage discounts
    
    # Usage restrictions
    min_order_amount = Column(Numeric(10, 2))
    max_uses = Column(Integer)  # Total usage limit
    max_uses_per_user = Column(Integer, default=1)
    first_order_only = Column(Boolean, default=False)
    
    # Validity period
    valid_from = Column(DateTime, server_default=func.now())
    valid_until = Column(DateTime)
    
    # Product restrictions
    applicable_product_ids = Column(JSONB, default=list)  # Empty = all products
    applicable_category_ids = Column(JSONB, default=list)
    excluded_product_ids = Column(JSONB, default=list)
    
    # Status
    is_active = Column(Boolean, default=True)
    
    # Usage tracking
    total_uses = Column(Integer, default=0)
    total_discount_given = Column(Numeric(12, 2), default=0)
    
    # Relationships
    order_coupons = relationship('OrderCoupon', back_populates='coupon')
    
    @hybrid_property
    def is_valid(self):
        """Check if coupon is currently valid."""
        now = datetime.utcnow()
        return (
            self.is_active and
            self.valid_from <= now and
            (not self.valid_until or self.valid_until >= now) and
            (not self.max_uses or self.total_uses < self.max_uses)
        )
    
    @hybrid_property
    def usage_percentage(self):
        """Calculate usage percentage."""
        if not self.max_uses:
            return 0
        return (self.total_uses / self.max_uses) * 100
    
    def calculate_discount(self, order_amount: Decimal, user_id: int) -> Decimal:
        """Calculate discount amount for given order."""
        # Implement discount calculation logic
        if not self.is_valid:
            return Decimal('0')
        
        if self.min_order_amount and order_amount < self.min_order_amount:
            return Decimal('0')
        
        if self.discount_type == 'percentage':
            discount = order_amount * (self.discount_value / 100)
            if self.max_discount_amount:
                discount = min(discount, self.max_discount_amount)
            return discount
        elif self.discount_type == 'fixed_amount':
            return min(self.discount_value, order_amount)
        
        return Decimal('0')
    
    def __repr__(self):
        return f"<Coupon(id={self.id}, code='{self.code}', type='{self.discount_type}')>"


class OrderCoupon(Base):
    """Applied coupons to orders (many-to-many with additional data)."""
    __tablename__ = 'order_coupons'
    
    id = Column(Integer, primary_key=True)
    order_id = Column(Integer, ForeignKey('orders.id'), nullable=False)
    coupon_id = Column(Integer, ForeignKey('coupons.id'), nullable=False)
    
    # Applied discount details
    discount_amount = Column(Numeric(10, 2), nullable=False)
    applied_at = Column(DateTime, server_default=func.now())
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('order_id', 'coupon_id', name='unique_order_coupon'),
        CheckConstraint('discount_amount >= 0', name='non_negative_discount'),
    )
    
    # Relationships
    order = relationship('Order')
    coupon = relationship('Coupon', back_populates='order_coupons')
    
    def __repr__(self):
        return f"<OrderCoupon(order_id={self.order_id}, discount={self.discount_amount})>"
```

### 4.3 Advanced Business Logic and Inventory Management

**app/services/inventory.py** - Inventory Management Service:

```python
"""
Advanced inventory management service with business logic.
Handles stock tracking, reorder points, and automated procurement.
"""

import logging
from datetime import datetime, timedelta
from decimal import Decimal
from typing import List, Dict, Optional, Tuple

from sqlalchemy import and_, or_, func
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from app.models.ecommerce import (
    Product, PhysicalProduct, InventoryLog, PurchaseOrder, 
    PurchaseOrderItem, Supplier, ProductSupplier
)
from app.models.user import User

logger = logging.getLogger(__name__)


class InventoryManager:
    """Advanced inventory management with automated reordering."""
    
    def __init__(self, db: Session):
        self.db = db
    
    def adjust_stock(
        self, 
        product_id: int, 
        quantity_change: int, 
        reason: str,
        reference_type: str = 'manual',
        reference_id: Optional[int] = None,
        performed_by_user_id: Optional[int] = None,
        cost_per_unit: Optional[Decimal] = None
    ) -> InventoryLog:
        """
        Adjust product stock with full audit trail.
        
        Args:
            product_id: Product to adjust
            quantity_change: Positive for increase, negative for decrease
            reason: Reason for adjustment
            reference_type: Type of operation (sale, purchase, return, adjustment)
            reference_id: ID of the reference object (order, purchase_order, etc.)
            performed_by_user_id: User performing the adjustment
            cost_per_unit: Cost per unit for accounting
        
        Returns:
            InventoryLog: The created inventory log entry
        """
        
        # Get current product with lock to prevent race conditions
        product = self.db.query(Product).filter(
            Product.id == product_id
        ).with_for_update().first()
        
        if not product:
            raise ValueError(f"Product {product_id} not found")
        
        if not product.manage_stock and quantity_change < 0:
            # Allow stock reduction even if stock management is disabled
            # This handles digital products or services
            pass
        elif product.stock_quantity + quantity_change < 0:
            raise ValueError(
                f"Insufficient stock. Available: {product.stock_quantity}, "
                f"Requested: {abs(quantity_change)}"
            )
        
        # Record previous quantity
        previous_quantity = product.stock_quantity
        
        # Update stock quantity
        product.stock_quantity += quantity_change
        new_quantity = product.stock_quantity
        
        # Create inventory log
        inventory_log = InventoryLog(
            product_id=product_id,
            movement_type=reference_type,
            quantity_change=quantity_change,
            previous_quantity=previous_quantity,
            new_quantity=new_quantity,
            reason=reason,
            reference_type=reference_type,
            reference_id=reference_id,
            performed_by_user_id=performed_by_user_id,
            cost_per_unit=cost_per_unit
        )
        
        self.db.add(inventory_log)
        
        # Check for low stock and trigger reorder if needed
        if product.is_low_stock and quantity_change < 0:
            self._check_reorder_point(product)
        
        logger.info(f"Stock adjusted for product {product_id}: {quantity_change} units. New stock: {new_quantity}")
        
        return inventory_log
    
    def reserve_stock(self, product_id: int, quantity: int, reference_id: int) -> bool:
        """
        Reserve stock for an order (soft reservation).
        This prevents overselling while order is being processed.
        """
        # Implementation would track reserved quantities
        # Could use a separate table or JSON field on product
        pass
    
    def release_stock_reservation(self, product_id: int, quantity: int, reference_id: int) -> bool:
        """Release previously reserved stock."""
        pass
    
    def get_stock_movements(
        self, 
        product_id: Optional[int] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        movement_types: Optional[List[str]] = None
    ) -> List[InventoryLog]:
        """Get stock movement history with filtering."""
        
        query = self.db.query(InventoryLog)
        
        if product_id:
            query = query.filter(InventoryLog.product_id == product_id)
        
        if start_date:
            query = query.filter(InventoryLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(InventoryLog.created_at <= end_date)
        
        if movement_types:
            query = query.filter(InventoryLog.movement_type.in_(movement_types))
        
        return query.order_by(InventoryLog.created_at.desc()).all()
    
    def get_low_stock_products(self) -> List[Product]:
        """Get all products that are low on stock."""
        return self.db.query(Product).filter(
            and_(
                Product.manage_stock == True,
                Product.stock_quantity <= Product.low_stock_threshold,
                Product.is_active == True,
                Product.deleted_at.is_(None)
            )
        ).all()
    
    def _check_reorder_point(self, product: Product):
        """Check if product needs reordering and create purchase order if needed."""
        
        # Get preferred supplier
        preferred_supplier = self.db.query(ProductSupplier).filter(
            and_(
                ProductSupplier.product_id == product.id,
                ProductSupplier.is_preferred == True,
                ProductSupplier.is_active == True
            )
        ).first()
        
        if not preferred_supplier:
            logger.warning(f"No preferred supplier found for product {product.id}")
            return
        
        # Calculate reorder quantity (simple reorder point model)
        reorder_quantity = max(
            preferred_supplier.minimum_order_quantity,
            product.low_stock_threshold * 2  # Order enough for 2x threshold
        )
        
        # Check if there's already a pending purchase order
        existing_po = self.db.query(PurchaseOrder).join(PurchaseOrderItem).filter(
            and_(
                PurchaseOrderItem.product_id == product.id,
                PurchaseOrder.status.in_(['pending', 'sent', 'confirmed'])
            )
        ).first()
        
        if existing_po:
            logger.info(f"Purchase order already exists for product {product.id}")
            return
        
        # Create automatic purchase order (would need admin approval in real system)
        logger.info(f"Product {product.id} needs reordering. Recommended quantity: {reorder_quantity}")
        
        # In a real system, this might:
        # 1. Create a draft purchase order for admin review
        # 2. Send notifications to purchasing team
        # 3. Automatically order if within certain thresholds
    
    def get_inventory_valuation(self) -> Dict[str, Decimal]:
        """Calculate total inventory valuation using different methods."""
        
        # Get all products with stock
        products_with_stock = self.db.query(Product).filter(
            and_(
                Product.stock_quantity > 0,
                Product.manage_stock == True
            )
        ).all()
        
        fifo_value = Decimal('0')
        lifo_value = Decimal('0') 
        average_cost_value = Decimal('0')
        current_cost_value = Decimal('0')
        
        for product in products_with_stock:
            # Get recent inventory movements
            recent_purchases = self.db.query(InventoryLog).filter(
                and_(
                    InventoryLog.product_id == product.id,
                    InventoryLog.movement_type == 'purchase',
                    InventoryLog.cost_per_unit.isnot(None)
                )
            ).order_by(InventoryLog.created_at.desc()).limit(10).all()
            
            if recent_purchases:
                # Calculate different valuation methods
                avg_cost = sum(log.cost_per_unit for log in recent_purchases) / len(recent_purchases)
                latest_cost = recent_purchases[0].cost_per_unit
                
                average_cost_value += avg_cost * product.stock_quantity
                current_cost_value += latest_cost * product.stock_quantity
            
            # Use current cost price if available
            if product.cost_price:
                current_cost_value += product.cost_price * product.stock_quantity
        
        return {
            'current_cost': current_cost_value,
            'average_cost': average_cost_value,
            'total_units': sum(p.stock_quantity for p in products_with_stock)
        }


class OrderProcessor:
    """Advanced order processing with business logic."""
    
    def __init__(self, db: Session):
        self.db = db
        self.inventory_manager = InventoryManager(db)
    
    def create_order_from_cart(self, cart: 'Cart', shipping_address: Dict, billing_address: Dict) -> 'Order':
        """
        Convert shopping cart to order with full validation.
        
        This method demonstrates complex business logic and transaction management.
        """
        
        if not cart.items:
            raise ValueError("Cannot create order from empty cart")
        
        # Validate stock availability
        for item in cart.items:
            if item.product.manage_stock and item.quantity > item.product.stock_quantity:
                raise ValueError(
                    f"Insufficient stock for {item.product.name}. "
                    f"Available: {item.product.stock_quantity}, Requested: {item.quantity}"
                )
        
        # Generate order number
        order_number = self._generate_order_number()
        
        # Calculate order totals
        subtotal = sum(item.total_price for item in cart.items)
        tax_amount = self._calculate_tax(subtotal, shipping_address)
        shipping_cost = self._calculate_shipping(cart.items, shipping_address)
        total_amount = subtotal + tax_amount + shipping_cost
        
        # Create order
        order = Order(
            order_number=order_number,
            user_id=cart.user_id,
            status=OrderStatus.PENDING,
            subtotal=subtotal,
            tax_amount=tax_amount,
            shipping_cost=shipping_cost,
            total_amount=total_amount,
            currency=cart.currency,
            customer_email=cart.user.email,
            billing_address=billing_address,
            shipping_address=shipping_address
        )
        
        self.db.add(order)
        self.db.flush()  # Get order ID
        
        # Create order items
        for cart_item in cart.items:
            order_item = OrderItem(
                order_id=order.id,
                product_id=cart_item.product_id,
                product_name=cart_item.product.name,
                product_sku=cart_item.product.sku,
                quantity=cart_item.quantity,
                unit_price=cart_item.unit_price,
                total_price=cart_item.total_price,
                product_options=cart_item.product_options,
                product_snapshot=self._create_product_snapshot(cart_item.product)
            )
            self.db.add(order_item)
        
        return order
    
    def _generate_order_number(self) -> str:
        """Generate unique order number."""
        import secrets
        import string
        
        # Format: ORD-YYYYMMDD-XXXXX
        date_part = datetime.now().strftime('%Y%m%d')
        random_part = ''.join(secrets.choice(string.digits) for _ in range(5))
        return f"ORD-{date_part}-{random_part}"
    
    def _calculate_tax(self, subtotal: Decimal, address: Dict) -> Decimal:
        """Calculate tax based on shipping address."""
        # Simplified tax calculation - real implementation would use tax service
        tax_rate = Decimal('0.08')  # 8% default tax rate
        return subtotal * tax_rate
    
    def _calculate_shipping(self, cart_items: List['CartItem'], address: Dict) -> Decimal:
        """Calculate shipping cost for order."""
        # Group items by shipping class
        shipping_cost = Decimal('0')
        
        # Simple shipping calculation
        for item in cart_items:
            if item.product.requires_shipping:
                if isinstance(item.product, PhysicalProduct):
                    # Use shipping class if available
                    if item.product.shipping_class:
                        cost = item.product.shipping_class.calculate_shipping_cost(
                            item.product.weight or 1.0,
                            address.get('country', 'US'),
                            float(item.total_price)
                        )
                        shipping_cost += cost
                    else:
                        # Default shipping cost
                        shipping_cost += Decimal('5.99')
        
        return shipping_cost
    
    def _create_product_snapshot(self, product: Product) -> Dict:
        """Create snapshot of product data at order time."""
        return {
            'name': product.name,
            'sku': product.sku,
            'description': product.short_description,
            'specifications': product.specifications,
            'brand': product.brand.name if product.brand else None,
            'categories': [cat.name for cat in product.categories],
            'snapshot_created_at': datetime.utcnow().isoformat()
        }
```

## Section 4.4: E-commerce CRUD Operations

Building comprehensive CRUD operations for the e-commerce system with advanced querying patterns and business logic integration.

**app/crud/ecommerce.py** - E-commerce CRUD Operations:

```python
"""
Comprehensive CRUD operations for e-commerce system.
Demonstrates advanced SQLAlchemy querying, polymorphic operations,
and business logic integration.
"""

from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime, timedelta
from decimal import Decimal

from sqlalchemy import and_, or_, func, case, distinct, text
from sqlalchemy.orm import Session, joinedload, selectinload, contains_eager
from sqlalchemy.exc import IntegrityError

from app.crud.base import BaseCRUD
from app.models.ecommerce import (
    Product, PhysicalProduct, DigitalProduct, ServiceProduct, SubscriptionProduct,
    Category, Brand, Cart, CartItem, Order, OrderItem, Payment,
    CreditCardPayment, PayPalPayment, BankTransferPayment,
    InventoryLog, Coupon, OrderCoupon, UserSubscription,
    ProductDownload, ServiceBooking
)


class ProductCRUD(BaseCRUD[Product]):
    """CRUD operations for products with polymorphic support."""
    
    def __init__(self, db: Session):
        super().__init__(Product, db)
    
    def get_products_by_type(self, product_type: str, **filters) -> List[Product]:
        """Get products by polymorphic type."""
        
        # Map product types to actual classes
        type_mapping = {
            'physical': PhysicalProduct,
            'digital': DigitalProduct,
            'service': ServiceProduct,
            'subscription': SubscriptionProduct
        }
        
        if product_type not in type_mapping:
            raise ValueError(f"Invalid product type: {product_type}")
        
        model_class = type_mapping[product_type]
        
        query = self.db.query(model_class).filter(model_class.is_active == True)
        
        # Apply additional filters
        for key, value in filters.items():
            if hasattr(model_class, key):
                query = query.filter(getattr(model_class, key) == value)
        
        return query.all()
    
    def search_products(
        self,
        search_term: str,
        category_ids: Optional[List[int]] = None,
        brand_ids: Optional[List[int]] = None,
        min_price: Optional[Decimal] = None,
        max_price: Optional[Decimal] = None,
        in_stock_only: bool = False,
        sort_by: str = 'relevance'
    ) -> List[Product]:
        """
        Advanced product search with multiple filters.
        Demonstrates complex querying with PostgreSQL full-text search.
        """
        
        # Base query with eager loading for performance
        query = self.db.query(Product).options(
            joinedload(Product.brand),
            selectinload(Product.categories),
            joinedload(Product.primary_image)
        ).filter(
            and_(
                Product.is_active == True,
                Product.deleted_at.is_(None)
            )
        )
        
        # Full-text search using PostgreSQL tsvector
        if search_term:
            # Create search vector combining multiple fields
            search_query = func.plainto_tsquery('english', search_term)
            
            # Search in multiple fields with different weights
            query = query.filter(
                or_(
                    func.to_tsvector('english', Product.name).match(search_query),
                    func.to_tsvector('english', Product.description).match(search_query),
                    func.to_tsvector('english', Product.short_description).match(search_query),
                    Product.sku.ilike(f'%{search_term}%')
                )
            )
        
        # Category filtering (many-to-many relationship)
        if category_ids:
            query = query.join(Product.categories).filter(
                Category.id.in_(category_ids)
            )
        
        # Brand filtering
        if brand_ids:
            query = query.filter(Product.brand_id.in_(brand_ids))
        
        # Price range filtering
        if min_price is not None:
            query = query.filter(Product.sale_price >= min_price)
        
        if max_price is not None:
            query = query.filter(Product.sale_price <= max_price)
        
        # Stock availability filtering
        if in_stock_only:
            query = query.filter(
                or_(
                    Product.manage_stock == False,
                    Product.stock_quantity > 0
                )
            )
        
        # Sorting options
        if sort_by == 'price_asc':
            query = query.order_by(Product.sale_price.asc())
        elif sort_by == 'price_desc':
            query = query.order_by(Product.sale_price.desc())
        elif sort_by == 'name':
            query = query.order_by(Product.name.asc())
        elif sort_by == 'newest':
            query = query.order_by(Product.created_at.desc())
        elif sort_by == 'popularity':
            # Join with order items to sort by sales
            query = query.outerjoin(OrderItem).group_by(Product.id).order_by(
                func.count(OrderItem.id).desc()
            )
        
        return query.all()
    
    def get_product_analytics(self, product_id: int) -> Dict[str, Any]:
        """Get comprehensive analytics for a product."""
        
        # Sales analytics
        sales_stats = self.db.query(
            func.count(OrderItem.id).label('total_orders'),
            func.sum(OrderItem.quantity).label('total_quantity_sold'),
            func.sum(OrderItem.total_price).label('total_revenue'),
            func.avg(OrderItem.unit_price).label('average_unit_price')
        ).filter(OrderItem.product_id == product_id).first()
        
        # Recent sales trend (last 30 days)
        thirty_days_ago = datetime.utcnow() - timedelta(days=30)
        recent_sales = self.db.query(
            func.date_trunc('day', Order.created_at).label('date'),
            func.sum(OrderItem.quantity).label('quantity'),
            func.sum(OrderItem.total_price).label('revenue')
        ).join(OrderItem).filter(
            and_(
                OrderItem.product_id == product_id,
                Order.created_at >= thirty_days_ago
            )
        ).group_by(func.date_trunc('day', Order.created_at)).all()
        
        # Inventory movements
        inventory_stats = self.db.query(
            func.count(InventoryLog.id).label('total_movements'),
            func.sum(
                case(
                    (InventoryLog.quantity_change > 0, InventoryLog.quantity_change),
                    else_=0
                )
            ).label('total_added'),
            func.sum(
                case(
                    (InventoryLog.quantity_change < 0, abs(InventoryLog.quantity_change)),
                    else_=0
                )
            ).label('total_removed')
        ).filter(InventoryLog.product_id == product_id).first()
        
        return {
            'sales': {
                'total_orders': sales_stats.total_orders or 0,
                'total_quantity_sold': sales_stats.total_quantity_sold or 0,
                'total_revenue': sales_stats.total_revenue or Decimal('0'),
                'average_unit_price': sales_stats.average_unit_price or Decimal('0')
            },
            'recent_trend': [
                {
                    'date': item.date.isoformat(),
                    'quantity': item.quantity or 0,
                    'revenue': item.revenue or Decimal('0')
                }
                for item in recent_sales
            ],
            'inventory': {
                'total_movements': inventory_stats.total_movements or 0,
                'total_added': inventory_stats.total_added or 0,
                'total_removed': inventory_stats.total_removed or 0
            }
        }
    
    def get_related_products(self, product_id: int, limit: int = 5) -> List[Product]:
        """
        Get related products using category similarity and sales patterns.
        Demonstrates advanced relationship queries.
        """
        
        product = self.get(product_id)
        if not product:
            return []
        
        # Get products from same categories
        category_related = self.db.query(Product).join(Product.categories).filter(
            and_(
                Category.id.in_([cat.id for cat in product.categories]),
                Product.id != product_id,
                Product.is_active == True,
                Product.deleted_at.is_(None)
            )
        ).distinct()
        
        # Get products frequently bought together (collaborative filtering)
        frequently_bought_together = self.db.query(Product).join(
            OrderItem, Product.id == OrderItem.product_id
        ).join(
            Order, OrderItem.order_id == Order.id
        ).filter(
            and_(
                Order.id.in_(
                    # Subquery to find orders containing our product
                    self.db.query(Order.id).join(OrderItem).filter(
                        OrderItem.product_id == product_id
                    )
                ),
                OrderItem.product_id != product_id,
                Product.is_active == True
            )
        ).group_by(Product.id).order_by(
            func.count(OrderItem.id).desc()
        )
        
        # Combine and limit results
        related_products = []
        
        # Add category-related products first
        for prod in category_related.limit(3):
            related_products.append(prod)
        
        # Add frequently bought together products
        for prod in frequently_bought_together.limit(limit - len(related_products)):
            if prod not in related_products:
                related_products.append(prod)
        
        return related_products[:limit]


class CartCRUD(BaseCRUD[Cart]):
    """CRUD operations for shopping cart management."""
    
    def add_item_to_cart(
        self, 
        cart_id: int, 
        product_id: int, 
        quantity: int,
        product_options: Optional[Dict] = None
    ) -> CartItem:
        """
        Add item to cart with validation and deduplication.
        Demonstrates business logic in CRUD operations.
        """
        
        cart = self.get(cart_id)
        if not cart:
            raise ValueError(f"Cart {cart_id} not found")
        
        # Get product with validation
        product = self.db.query(Product).filter(
            and_(
                Product.id == product_id,
                Product.is_active == True,
                Product.deleted_at.is_(None)
            )
        ).first()
        
        if not product:
            raise ValueError(f"Product {product_id} not found or inactive")
        
        # Validate stock availability
        if product.manage_stock and quantity > product.stock_quantity:
            raise ValueError(
                f"Insufficient stock. Available: {product.stock_quantity}, "
                f"Requested: {quantity}"
            )
        
        # Check for existing item with same options
        existing_item = self.db.query(CartItem).filter(
            and_(
                CartItem.cart_id == cart_id,
                CartItem.product_id == product_id,
                CartItem.product_options == (product_options or {})
            )
        ).first()
        
        if existing_item:
            # Update quantity of existing item
            new_quantity = existing_item.quantity + quantity
            
            # Validate total quantity against stock
            if product.manage_stock and new_quantity > product.stock_quantity:
                raise ValueError(
                    f"Total quantity would exceed stock. "
                    f"Current cart: {existing_item.quantity}, "
                    f"Available: {product.stock_quantity}"
                )
            
            existing_item.quantity = new_quantity
            existing_item.updated_at = datetime.utcnow()
            
            # Recalculate totals
            self._update_cart_item_totals(existing_item)
            return existing_item
        
        else:
            # Create new cart item
            cart_item = CartItem(
                cart_id=cart_id,
                product_id=product_id,
                quantity=quantity,
                product_options=product_options or {}
            )
            
            self.db.add(cart_item)
            self.db.flush()
            
            # Calculate totals
            self._update_cart_item_totals(cart_item)
            
            return cart_item
    
    def update_item_quantity(self, cart_item_id: int, new_quantity: int) -> CartItem:
        """Update cart item quantity with validation."""
        
        cart_item = self.db.query(CartItem).filter(
            CartItem.id == cart_item_id
        ).first()
        
        if not cart_item:
            raise ValueError(f"Cart item {cart_item_id} not found")
        
        if new_quantity <= 0:
            # Remove item if quantity is 0 or negative
            self.db.delete(cart_item)
            return None
        
        # Validate stock availability
        if cart_item.product.manage_stock and new_quantity > cart_item.product.stock_quantity:
            raise ValueError(
                f"Insufficient stock. Available: {cart_item.product.stock_quantity}, "
                f"Requested: {new_quantity}"
            )
        
        cart_item.quantity = new_quantity
        cart_item.updated_at = datetime.utcnow()
        
        # Recalculate totals
        self._update_cart_item_totals(cart_item)
        
        return cart_item
    
    def get_cart_with_totals(self, cart_id: int) -> Optional[Cart]:
        """Get cart with all items and calculated totals."""
        
        cart = self.db.query(Cart).options(
            selectinload(Cart.items).selectinload(CartItem.product)
        ).filter(Cart.id == cart_id).first()
        
        if not cart:
            return None
        
        # Calculate cart totals
        cart.item_count = len(cart.items)
        cart.subtotal = sum(item.total_price for item in cart.items)
        
        return cart
    
    def apply_coupon(self, cart_id: int, coupon_code: str, user_id: int) -> Dict[str, Any]:
        """
        Apply coupon to cart with validation.
        Demonstrates complex business rule validation.
        """
        
        cart = self.get_cart_with_totals(cart_id)
        if not cart:
            raise ValueError(f"Cart {cart_id} not found")
        
        # Get and validate coupon
        coupon = self.db.query(Coupon).filter(
            and_(
                Coupon.code == coupon_code,
                Coupon.is_active == True
            )
        ).first()
        
        if not coupon:
            raise ValueError(f"Invalid coupon code: {coupon_code}")
        
        if not coupon.is_valid:
            raise ValueError("Coupon has expired or reached usage limit")
        
        # Check user-specific restrictions
        if coupon.max_uses_per_user:
            user_usage = self.db.query(func.count(OrderCoupon.id)).join(Order).filter(
                and_(
                    OrderCoupon.coupon_id == coupon.id,
                    Order.user_id == user_id
                )
            ).scalar()
            
            if user_usage >= coupon.max_uses_per_user:
                raise ValueError("Coupon usage limit exceeded for this user")
        
        # Check minimum order amount
        if coupon.min_order_amount and cart.subtotal < coupon.min_order_amount:
            raise ValueError(
                f"Minimum order amount of {coupon.min_order_amount} required"
            )
        
        # Check product restrictions
        if coupon.applicable_product_ids:
            applicable_items = [
                item for item in cart.items 
                if item.product_id in coupon.applicable_product_ids
            ]
            if not applicable_items:
                raise ValueError("Coupon is not applicable to items in cart")
        
        # Calculate discount
        discount_amount = coupon.calculate_discount(cart.subtotal, user_id)
        
        return {
            'coupon_id': coupon.id,
            'discount_amount': discount_amount,
            'coupon_description': coupon.description,
            'new_total': cart.subtotal - discount_amount
        }
    
    def _update_cart_item_totals(self, cart_item: CartItem):
        """Update calculated fields for cart item."""
        
        # Get current product price (might have changed since added to cart)
        current_price = cart_item.product.sale_price
        
        # Use current price or locked price if cart is saved
        cart_item.unit_price = current_price
        cart_item.total_price = cart_item.unit_price * cart_item.quantity
    
    def cleanup_expired_carts(self, days_old: int = 7) -> int:
        """Remove old abandoned carts."""
        
        cutoff_date = datetime.utcnow() - timedelta(days=days_old)
        
        expired_carts = self.db.query(Cart).filter(
            and_(
                Cart.updated_at < cutoff_date,
                Cart.status == 'active'  # Only cleanup active carts
            )
        ).all()
        
        for cart in expired_carts:
            self.db.delete(cart)
        
        return len(expired_carts)


class OrderCRUD(BaseCRUD[Order]):
    """CRUD operations for order management."""
    
    def get_orders_with_details(
        self,
        user_id: Optional[int] = None,
        status: Optional[str] = None,
        date_range: Optional[Tuple[datetime, datetime]] = None
    ) -> List[Order]:
        """Get orders with complete details using optimized loading."""
        
        query = self.db.query(Order).options(
            selectinload(Order.items).selectinload(OrderItem.product),
            selectinload(Order.payments),
            selectinload(Order.shipping_address),
            joinedload(Order.user)
        )
        
        if user_id:
            query = query.filter(Order.user_id == user_id)
        
        if status:
            query = query.filter(Order.status == status)
        
        if date_range:
            start_date, end_date = date_range
            query = query.filter(
                and_(
                    Order.created_at >= start_date,
                    Order.created_at <= end_date
                )
            )
        
        return query.order_by(Order.created_at.desc()).all()
    
    def process_order_payment(self, order_id: int, payment_data: Dict) -> Payment:
        """
        Process payment for an order.
        Demonstrates polymorphic payment processing.
        """
        
        order = self.get(order_id)
        if not order:
            raise ValueError(f"Order {order_id} not found")
        
        if order.status != 'pending':
            raise ValueError(f"Cannot process payment for order with status: {order.status}")
        
        # Create polymorphic payment based on method
        payment_method = payment_data.get('method')
        
        if payment_method == 'credit_card':
            payment = CreditCardPayment(
                order_id=order_id,
                amount=order.total_amount,
                currency=order.currency,
                status='processing',
                payment_method='credit_card',
                card_last_four=payment_data.get('card_last_four'),
                card_brand=payment_data.get('card_brand'),
                card_exp_month=payment_data.get('exp_month'),
                card_exp_year=payment_data.get('exp_year')
            )
        
        elif payment_method == 'paypal':
            payment = PayPalPayment(
                order_id=order_id,
                amount=order.total_amount,
                currency=order.currency,
                status='processing',
                payment_method='paypal',
                paypal_email=payment_data.get('paypal_email'),
                paypal_transaction_id=payment_data.get('transaction_id')
            )
        
        elif payment_method == 'bank_transfer':
            payment = BankTransferPayment(
                order_id=order_id,
                amount=order.total_amount,
                currency=order.currency,
                status='pending',
                payment_method='bank_transfer',
                bank_name=payment_data.get('bank_name'),
                account_holder=payment_data.get('account_holder'),
                routing_number=payment_data.get('routing_number'),
                account_number=payment_data.get('account_number')
            )
        
        else:
            raise ValueError(f"Unsupported payment method: {payment_method}")
        
        self.db.add(payment)
        self.db.flush()
        
        # Simulate payment processing (in real app, would integrate with payment gateway)
        if payment_method in ['credit_card', 'paypal']:
            # Immediate processing simulation
            payment.status = 'completed'
            payment.processed_at = datetime.utcnow()
            payment.gateway_transaction_id = f"TXN_{payment.id}_{datetime.utcnow().timestamp()}"
            
            # Update order status
            order.status = 'confirmed'
            order.confirmed_at = datetime.utcnow()
            
            # Reserve/reduce inventory
            self._process_order_inventory(order)
        
        return payment
    
    def _process_order_inventory(self, order: Order):
        """Process inventory changes for confirmed order."""
        from app.services.inventory import InventoryManager
        
        inventory_manager = InventoryManager(self.db)
        
        for item in order.items:
            if item.product.manage_stock:
                inventory_manager.adjust_stock(
                    product_id=item.product_id,
                    quantity_change=-item.quantity,
                    reason=f"Sale - Order {order.order_number}",
                    reference_type='sale',
                    reference_id=order.id
                )
    
    def get_order_summary_stats(
        self, 
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """Get comprehensive order statistics."""
        
        query = self.db.query(Order)
        
        if start_date:
            query = query.filter(Order.created_at >= start_date)
        
        if end_date:
            query = query.filter(Order.created_at <= end_date)
        
        # Overall statistics
        stats = query.with_entities(
            func.count(Order.id).label('total_orders'),
            func.sum(Order.total_amount).label('total_revenue'),
            func.avg(Order.total_amount).label('average_order_value'),
            func.min(Order.total_amount).label('min_order_value'),
            func.max(Order.total_amount).label('max_order_value')
        ).first()
        
        # Orders by status
        status_breakdown = self.db.query(
            Order.status,
            func.count(Order.id).label('count'),
            func.sum(Order.total_amount).label('revenue')
        ).group_by(Order.status).all()
        
        # Top products by revenue
        top_products = self.db.query(
            Product.name,
            func.sum(OrderItem.quantity).label('total_quantity'),
            func.sum(OrderItem.total_price).label('total_revenue')
        ).join(OrderItem).join(Product).group_by(
            Product.id, Product.name
        ).order_by(
            func.sum(OrderItem.total_price).desc()
        ).limit(10).all()
        
        return {
            'summary': {
                'total_orders': stats.total_orders or 0,
                'total_revenue': stats.total_revenue or Decimal('0'),
                'average_order_value': stats.average_order_value or Decimal('0'),
                'min_order_value': stats.min_order_value or Decimal('0'),
                'max_order_value': stats.max_order_value or Decimal('0')
            },
            'status_breakdown': [
                {
                    'status': item.status,
                    'count': item.count,
                    'revenue': item.revenue or Decimal('0')
                }
                for item in status_breakdown
            ],
            'top_products': [
                {
                    'name': item.name,
                    'quantity_sold': item.total_quantity,
                    'revenue': item.total_revenue
                }
                for item in top_products
            ]
        }


class PaymentCRUD(BaseCRUD[Payment]):
    """CRUD operations for payment processing."""
    
    def get_payments_by_method(self, method: str) -> List[Payment]:
        """Get payments by specific method using polymorphic queries."""
        
        method_mapping = {
            'credit_card': CreditCardPayment,
            'paypal': PayPalPayment,
            'bank_transfer': BankTransferPayment
        }
        
        if method not in method_mapping:
            raise ValueError(f"Invalid payment method: {method}")
        
        payment_class = method_mapping[method]
        
        return self.db.query(payment_class).options(
            joinedload(payment_class.order)
        ).all()
    
    def get_payment_analytics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """Get payment method analytics and success rates."""
        
        query = self.db.query(Payment)
        
        if start_date:
            query = query.filter(Payment.created_at >= start_date)
        
        if end_date:
            query = query.filter(Payment.created_at <= end_date)
        
        # Payment method breakdown
        method_stats = query.with_entities(
            Payment.payment_method,
            func.count(Payment.id).label('total_transactions'),
            func.sum(Payment.amount).label('total_amount'),
            func.count(
                case((Payment.status == 'completed', 1))
            ).label('successful_payments'),
            func.count(
                case((Payment.status == 'failed', 1))
            ).label('failed_payments')
        ).group_by(Payment.payment_method).all()
        
        # Processing time analysis
        processing_times = self.db.query(
            Payment.payment_method,
            func.avg(
                func.extract('epoch', Payment.processed_at - Payment.created_at)
            ).label('avg_processing_time_seconds')
        ).filter(
            Payment.processed_at.isnot(None)
        ).group_by(Payment.payment_method).all()
        
        return {
            'method_breakdown': [
                {
                    'method': stat.payment_method,
                    'total_transactions': stat.total_transactions,
                    'total_amount': stat.total_amount or Decimal('0'),
                    'success_rate': (
                        (stat.successful_payments / stat.total_transactions * 100) 
                        if stat.total_transactions > 0 else 0
                    ),
                    'failure_rate': (
                        (stat.failed_payments / stat.total_transactions * 100)
                        if stat.total_transactions > 0 else 0
                    )
                }
                for stat in method_stats
            ],
            'processing_times': {
                time_stat.payment_method: time_stat.avg_processing_time_seconds
                for time_stat in processing_times
            }
        }


class CouponCRUD(BaseCRUD[Coupon]):
    """CRUD operations for coupon and promotion management."""
    
    def validate_coupon_usage(self, coupon_code: str, user_id: int, order_amount: Decimal) -> Dict[str, Any]:
        """Comprehensive coupon validation."""
        
        coupon = self.db.query(Coupon).filter(
            Coupon.code == coupon_code
        ).first()
        
        if not coupon:
            return {'valid': False, 'error': 'Coupon not found'}
        
        if not coupon.is_valid:
            return {'valid': False, 'error': 'Coupon has expired or is inactive'}
        
        # Check user usage limits
        if coupon.max_uses_per_user:
            user_usage = self.db.query(func.count(OrderCoupon.id)).join(Order).filter(
                and_(
                    OrderCoupon.coupon_id == coupon.id,
                    Order.user_id == user_id
                )
            ).scalar()
            
            if user_usage >= coupon.max_uses_per_user:
                return {'valid': False, 'error': 'Usage limit exceeded'}
        
        # Check first order restriction
        if coupon.first_order_only:
            previous_orders = self.db.query(func.count(Order.id)).filter(
                Order.user_id == user_id
            ).scalar()
            
            if previous_orders > 0:
                return {'valid': False, 'error': 'Coupon only valid for first order'}
        
        # Check minimum order amount
        if coupon.min_order_amount and order_amount < coupon.min_order_amount:
            return {
                'valid': False, 
                'error': f'Minimum order amount of {coupon.min_order_amount} required'
            }
        
        # Calculate discount
        discount_amount = coupon.calculate_discount(order_amount, user_id)
        
        return {
            'valid': True,
            'discount_amount': discount_amount,
            'coupon': coupon
        }
    
    def get_user_eligible_coupons(self, user_id: int, order_amount: Optional[Decimal] = None) -> List[Coupon]:
        """Get coupons that user is eligible to use."""
        
        # Get active coupons
        query = self.db.query(Coupon).filter(
            and_(
                Coupon.is_active == True,
                or_(
                    Coupon.valid_until.is_(None),
                    Coupon.valid_until >= datetime.utcnow()
                ),
                Coupon.valid_from <= datetime.utcnow()
            )
        )
        
        # Filter by order amount if provided
        if order_amount:
            query = query.filter(
                or_(
                    Coupon.min_order_amount.is_(None),
                    Coupon.min_order_amount <= order_amount
                )
            )
        
        eligible_coupons = []
        
        for coupon in query.all():
            # Check user-specific eligibility
            if coupon.max_uses_per_user:
                user_usage = self.db.query(func.count(OrderCoupon.id)).join(Order).filter(
                    and_(
                        OrderCoupon.coupon_id == coupon.id,
                        Order.user_id == user_id
                    )
                ).scalar()
                
                if user_usage >= coupon.max_uses_per_user:
                    continue
            
            # Check first order restriction
            if coupon.first_order_only:
                previous_orders = self.db.query(func.count(Order.id)).filter(
                    Order.user_id == user_id
                ).scalar()
                
                if previous_orders > 0:
                    continue
            
            eligible_coupons.append(coupon)
        
        return eligible_coupons


class SubscriptionCRUD(BaseCRUD[UserSubscription]):
    """CRUD operations for subscription management."""
    
    def create_subscription(
        self,
        user_id: int,
        product_id: int,
        trial_days: Optional[int] = None
    ) -> UserSubscription:
        """Create new subscription with trial period."""
        
        # Validate subscription product
        product = self.db.query(SubscriptionProduct).filter(
            SubscriptionProduct.id == product_id
        ).first()
        
        if not product:
            raise ValueError(f"Subscription product {product_id} not found")
        
        # Check for existing active subscription
        existing = self.db.query(UserSubscription).filter(
            and_(
                UserSubscription.user_id == user_id,
                UserSubscription.product_id == product_id,
                UserSubscription.status == 'active'
            )
        ).first()
        
        if existing:
            raise ValueError("User already has active subscription for this product")
        
        # Generate subscription reference
        import uuid
        reference = f"SUB-{uuid.uuid4().hex[:8].upper()}"
        
        # Calculate billing dates
        now = datetime.utcnow()
        trial_ends_at = now + timedelta(days=trial_days) if trial_days else None
        
        if product.billing_cycle == 'monthly':
            next_billing = (trial_ends_at or now) + timedelta(days=30)
        elif product.billing_cycle == 'quarterly':
            next_billing = (trial_ends_at or now) + timedelta(days=90)
        elif product.billing_cycle == 'yearly':
            next_billing = (trial_ends_at or now) + timedelta(days=365)
        else:
            raise ValueError(f"Unsupported billing cycle: {product.billing_cycle}")
        
        subscription = UserSubscription(
            user_id=user_id,
            product_id=product_id,
            subscription_reference=reference,
            trial_ends_at=trial_ends_at,
            next_billing_date=next_billing
        )
        
        self.db.add(subscription)
        return subscription
    
    def process_subscription_billing(self, subscription_id: int) -> 'SubscriptionBilling':
        """Process recurring billing for subscription."""
        
        subscription = self.get(subscription_id)
        if not subscription:
            raise ValueError(f"Subscription {subscription_id} not found")
        
        if subscription.status != 'active':
            raise ValueError(f"Cannot bill inactive subscription")
        
        # Calculate billing period
        billing_start = subscription.next_billing_date
        
        if subscription.product.billing_cycle == 'monthly':
            billing_end = billing_start + timedelta(days=30)
            next_billing = billing_end
        elif subscription.product.billing_cycle == 'quarterly':
            billing_end = billing_start + timedelta(days=90)
            next_billing = billing_end
        elif subscription.product.billing_cycle == 'yearly':
            billing_end = billing_start + timedelta(days=365)
            next_billing = billing_end
        
        # Create billing record
        from app.models.ecommerce import SubscriptionBilling
        
        billing = SubscriptionBilling(
            subscription_id=subscription_id,
            billing_period_start=billing_start,
            billing_period_end=billing_end,
            amount=subscription.product.price,
            currency=subscription.product.currency,
            due_date=billing_start + timedelta(days=7)  # 7 day grace period
        )
        
        self.db.add(billing)
        
        # Update subscription
        subscription.next_billing_date = next_billing
        subscription.billing_cycles_completed += 1
        
        return billing
    
    def get_expiring_subscriptions(self, days_ahead: int = 7) -> List[UserSubscription]:
        """Get subscriptions expiring within specified days."""
        
        future_date = datetime.utcnow() + timedelta(days=days_ahead)
        
        return self.db.query(UserSubscription).filter(
            and_(
                UserSubscription.status == 'active',
                UserSubscription.next_billing_date <= future_date,
                UserSubscription.next_billing_date >= datetime.utcnow()
            )
        ).options(
            joinedload(UserSubscription.user),
            joinedload(UserSubscription.product)
        ).all()


class InventoryCRUD(BaseCRUD[InventoryLog]):
    """CRUD operations for inventory tracking."""
    
    def get_stock_movements_report(
        self,
        product_ids: Optional[List[int]] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        movement_types: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        Generate comprehensive stock movement report.
        Demonstrates complex reporting queries with aggregations.
        """
        
        # Base query with product information
        query = self.db.query(
            InventoryLog.product_id,
            Product.name.label('product_name'),
            Product.sku.label('product_sku'),
            InventoryLog.movement_type,
            func.sum(InventoryLog.quantity_change).label('total_change'),
            func.count(InventoryLog.id).label('transaction_count'),
            func.min(InventoryLog.created_at).label('first_transaction'),
            func.max(InventoryLog.created_at).label('last_transaction')
        ).join(Product)
        
        # Apply filters
        if product_ids:
            query = query.filter(InventoryLog.product_id.in_(product_ids))
        
        if start_date:
            query = query.filter(InventoryLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(InventoryLog.created_at <= end_date)
        
        if movement_types:
            query = query.filter(InventoryLog.movement_type.in_(movement_types))
        
        # Group by product and movement type
        results = query.group_by(
            InventoryLog.product_id,
            Product.name,
            Product.sku,
            InventoryLog.movement_type
        ).order_by(
            InventoryLog.product_id,
            InventoryLog.movement_type
        ).all()
        
        return [
            {
                'product_id': result.product_id,
                'product_name': result.product_name,
                'product_sku': result.product_sku,
                'movement_type': result.movement_type,
                'total_change': result.total_change,
                'transaction_count': result.transaction_count,
                'first_transaction': result.first_transaction.isoformat(),
                'last_transaction': result.last_transaction.isoformat()
            }
            for result in results
        ]

    def get_abc_analysis(self) -> Dict[str, Any]:
        """
        Perform ABC analysis on inventory (Pareto analysis).
        Demonstrates advanced SQL with window functions and CTEs.
        """
        
        # Create CTE with product sales and rankings
        cte_sales = self.db.query(
            Product.id.label('product_id'),
            Product.name.label('product_name'),
            Product.sku.label('product_sku'),
            func.sum(OrderItem.total_price).label('total_revenue'),
            func.sum(OrderItem.quantity).label('total_quantity'),
            func.count(distinct(Order.id)).label('order_count')
        ).join(OrderItem).join(Order).filter(
            Product.is_active == True
        ).group_by(
            Product.id, Product.name, Product.sku
        ).cte('sales_data')
        
        # Calculate running totals and percentages using window functions
        analysis_query = self.db.query(
            cte_sales.c.product_id,
            cte_sales.c.product_name,
            cte_sales.c.product_sku,
            cte_sales.c.total_revenue,
            cte_sales.c.total_quantity,
            cte_sales.c.order_count,
            (cte_sales.c.total_revenue / 
             func.sum(cte_sales.c.total_revenue).over() * 100
            ).label('revenue_percentage'),
            func.sum(cte_sales.c.total_revenue).over(
                order_by=cte_sales.c.total_revenue.desc()
            ).label('cumulative_revenue'),
            (func.sum(cte_sales.c.total_revenue).over(
                order_by=cte_sales.c.total_revenue.desc()
            ) / func.sum(cte_sales.c.total_revenue).over() * 100
            ).label('cumulative_percentage')
        ).order_by(cte_sales.c.total_revenue.desc())
        
        results = analysis_query.all()
        
        # Classify products into A, B, C categories
        categorized_products = {
            'A': [],  # Top 80% of revenue (usually ~20% of products)
            'B': [],  # Next 15% of revenue
            'C': []   # Remaining 5% of revenue
        }
        
        for result in results:
            if result.cumulative_percentage <= 80:
                category = 'A'
            elif result.cumulative_percentage <= 95:
                category = 'B'
            else:
                category = 'C'
            
            categorized_products[category].append({
                'product_id': result.product_id,
                'product_name': result.product_name,
                'sku': result.product_sku,
                'total_revenue': result.total_revenue,
                'revenue_percentage': round(result.revenue_percentage, 2),
                'cumulative_percentage': round(result.cumulative_percentage, 2)
            })
        
        return categorized_products
    
    def get_slow_moving_stock(self, days_threshold: int = 90) -> List[Dict[str, Any]]:
        """
        Identify slow-moving inventory that hasn't sold recently.
        Important for inventory optimization and cash flow.
        """
        
        cutoff_date = datetime.utcnow() - timedelta(days=days_threshold)
        
        # Subquery for products with recent sales
        recent_sales_subquery = self.db.query(
            distinct(OrderItem.product_id)
        ).join(Order).filter(
            Order.created_at >= cutoff_date
        ).subquery()
        
        # Find products with stock but no recent sales
        slow_movers = self.db.query(
            Product.id,
            Product.name,
            Product.sku,
            Product.stock_quantity,
            Product.cost_price,
            (Product.stock_quantity * Product.cost_price).label('inventory_value'),
            func.max(Order.created_at).label('last_sale_date')
        ).outerjoin(OrderItem).outerjoin(Order).filter(
            and_(
                Product.manage_stock == True,
                Product.stock_quantity > 0,
                ~Product.id.in_(recent_sales_subquery)
            )
        ).group_by(
            Product.id, Product.name, Product.sku, 
            Product.stock_quantity, Product.cost_price
        ).order_by(
            (Product.stock_quantity * Product.cost_price).desc()
        ).all()
        
        return [
            {
                'product_id': item.id,
                'name': item.name,
                'sku': item.sku,
                'stock_quantity': item.stock_quantity,
                'cost_price': item.cost_price,
                'inventory_value': item.inventory_value,
                'last_sale_date': item.last_sale_date.isoformat() if item.last_sale_date else None,
                'days_since_last_sale': (
                    (datetime.utcnow() - item.last_sale_date).days 
                    if item.last_sale_date else None
                )
            }
            for item in slow_movers
        ]


# Advanced analytics and reporting
class EcommerceAnalytics:
    """Advanced analytics for e-commerce data."""
    
    def __init__(self, db: Session):
        self.db = db
    
    def get_customer_lifetime_value(
        self, 
        user_ids: Optional[List[int]] = None,
        include_projections: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Calculate Customer Lifetime Value (CLV) for users.
        Demonstrates advanced analytical queries.
        """
        
        # Base customer data with aggregations
        query = self.db.query(
            User.id.label('user_id'),
            User.email,
            User.created_at.label('customer_since'),
            func.count(distinct(Order.id)).label('total_orders'),
            func.sum(Order.total_amount).label('total_spent'),
            func.avg(Order.total_amount).label('average_order_value'),
            func.min(Order.created_at).label('first_order_date'),
            func.max(Order.created_at).label('last_order_date'),
            func.avg(
                func.extract('days', 
                    func.lead(Order.created_at).over(
                        partition_by=Order.user_id,
                        order_by=Order.created_at
                    ) - Order.created_at
                )
            ).label('average_days_between_orders')
        ).outerjoin(Order).filter(
            Order.status == 'completed'
        ).group_by(User.id, User.email, User.created_at)
        
        if user_ids:
            query = query.filter(User.id.in_(user_ids))
        
        results = query.all()
        
        clv_data = []
        for result in results:
            # Calculate customer metrics
            customer_age_days = (datetime.utcnow() - result.customer_since).days
            
            if result.total_orders and result.total_orders > 1:
                # Calculate purchase frequency (orders per day)
                purchase_frequency = result.total_orders / customer_age_days
                
                # Historical CLV (actual spend)
                historical_clv = result.total_spent or Decimal('0')
                
                # Projected CLV (simple model)
                if include_projections and result.average_days_between_orders:
                    projected_annual_orders = 365 / result.average_days_between_orders
                    projected_annual_clv = projected_annual_orders * result.average_order_value
                else:
                    projected_annual_clv = None
            
            else:
                purchase_frequency = 0
                historical_clv = result.total_spent or Decimal('0')
                projected_annual_clv = None
            
            clv_data.append({
                'user_id': result.user_id,
                'email': result.email,
                'customer_since': result.customer_since.isoformat(),
                'total_orders': result.total_orders or 0,
                'total_spent': historical_clv,
                'average_order_value': result.average_order_value or Decimal('0'),
                'purchase_frequency_per_day': purchase_frequency,
                'days_between_orders': result.average_days_between_orders,
                'historical_clv': historical_clv,
                'projected_annual_clv': projected_annual_clv,
                'last_order_date': result.last_order_date.isoformat() if result.last_order_date else None
            })
        
        return clv_data
    
    def get_cohort_analysis(self, period: str = 'month') -> Dict[str, Any]:
        """
        Perform cohort analysis showing customer retention over time.
        Advanced SQLAlchemy demonstrating date functions and cohort tracking.
        """
        
        if period not in ['month', 'week']:
            raise ValueError("Period must be 'month' or 'week'")
        
        # Define period truncation function
        if period == 'month':
            period_func = func.date_trunc('month', Order.created_at)
        else:
            period_func = func.date_trunc('week', Order.created_at)
        
        # Get first order date for each customer (cohort assignment)
        first_orders = self.db.query(
            Order.user_id,
            func.min(period_func).label('cohort_period')
        ).group_by(Order.user_id).subquery()
        
        # Get all orders with cohort information
        cohort_data = self.db.query(
            first_orders.c.cohort_period,
            period_func.label('order_period'),
            func.count(distinct(Order.user_id)).label('customers'),
            func.sum(Order.total_amount).label('revenue')
        ).join(
            first_orders, Order.user_id == first_orders.c.user_id
        ).group_by(
            first_orders.c.cohort_period,
            period_func
        ).order_by(
            first_orders.c.cohort_period,
            period_func
        ).all()
        
        # Process results into cohort table format
        cohorts = {}
        for row in cohort_data:
            cohort_key = row.cohort_period.isoformat()
            if cohort_key not in cohorts:
                cohorts[cohort_key] = {}
            
            # Calculate period offset
            if period == 'month':
                period_offset = (
                    row.order_period.year - row.cohort_period.year
                ) * 12 + (
                    row.order_period.month - row.cohort_period.month
                )
            else:  # week
                period_offset = (row.order_period - row.cohort_period).days // 7
            
            cohorts[cohort_key][period_offset] = {
                'customers': row.customers,
                'revenue': row.revenue
            }
        
        return {
            'period': period,
            'cohorts': cohorts,
            'analysis_date': datetime.utcnow().isoformat()
        }
    
    def get_product_performance_matrix(self) -> List[Dict[str, Any]]:
        """
        Create product performance matrix (Sales vs. Profit margin).
        Useful for product portfolio optimization.
        """
        
        performance_data = self.db.query(
            Product.id,
            Product.name,
            Product.sku,
            Product.sale_price,
            Product.cost_price,
            ((Product.sale_price - Product.cost_price) / Product.sale_price * 100).label('profit_margin_percent'),
            func.coalesce(func.sum(OrderItem.quantity), 0).label('total_quantity_sold'),
            func.coalesce(func.sum(OrderItem.total_price), 0).label('total_revenue'),
            func.count(distinct(Order.user_id)).label('unique_customers')
        ).outerjoin(OrderItem).outerjoin(Order).filter(
            and_(
                Product.is_active == True,
                Product.cost_price.isnot(None),
                Product.cost_price > 0
            )
        ).group_by(
            Product.id, Product.name, Product.sku, 
            Product.sale_price, Product.cost_price
        ).order_by(
            func.sum(OrderItem.total_price).desc().nullslast()
        ).all()
        
        return [
            {
                'product_id': item.id,
                'name': item.name,
                'sku': item.sku,
                'sale_price': item.sale_price,
                'cost_price': item.cost_price,
                'profit_margin_percent': round(item.profit_margin_percent or 0, 2),
                'total_quantity_sold': item.total_quantity_sold,
                'total_revenue': item.total_revenue,
                'unique_customers': item.unique_customers,
                'performance_category': self._categorize_product_performance(
                    item.total_revenue, item.profit_margin_percent
                )
            }
            for item in performance_data
        ]
    
    def _categorize_product_performance(self, revenue: Decimal, margin: Decimal) -> str:
        """Categorize product based on revenue and margin."""
        
        if revenue > 10000 and margin > 30:
            return 'Star'  # High revenue, high margin
        elif revenue > 10000 and margin <= 30:
            return 'Cash Cow'  # High revenue, low margin
        elif revenue <= 10000 and margin > 30:
            return 'Question Mark'  # Low revenue, high margin
        else:
            return 'Dog'  # Low revenue, low margin
    
    def get_seasonal_trends(self, years_back: int = 2) -> Dict[str, Any]:
        """
        Analyze seasonal sales trends.
        Demonstrates date extraction and seasonal pattern analysis.
        """
        
        cutoff_date = datetime.utcnow() - timedelta(days=years_back * 365)
        
        # Monthly sales data
        monthly_trends = self.db.query(
            func.extract('year', Order.created_at).label('year'),
            func.extract('month', Order.created_at).label('month'),
            func.count(Order.id).label('order_count'),
            func.sum(Order.total_amount).label('total_revenue'),
            func.avg(Order.total_amount).label('average_order_value')
        ).filter(
            and_(
                Order.created_at >= cutoff_date,
                Order.status == 'completed'
            )
        ).group_by(
            func.extract('year', Order.created_at),
            func.extract('month', Order.created_at)
        ).order_by('year', 'month').all()
        
        # Day of week analysis
        dow_trends = self.db.query(
            func.extract('dow', Order.created_at).label('day_of_week'),
            func.count(Order.id).label('order_count'),
            func.sum(Order.total_amount).label('total_revenue'),
            func.avg(Order.total_amount).label('average_order_value')
        ).filter(
            and_(
                Order.created_at >= cutoff_date,
                Order.status == 'completed'
            )
        ).group_by(
            func.extract('dow', Order.created_at)
        ).order_by('day_of_week').all()
        
        # Hour of day analysis
        hour_trends = self.db.query(
            func.extract('hour', Order.created_at).label('hour'),
            func.count(Order.id).label('order_count'),
            func.sum(Order.total_amount).label('total_revenue')
        ).filter(
            and_(
                Order.created_at >= cutoff_date,
                Order.status == 'completed'
            )
        ).group_by(
            func.extract('hour', Order.created_at)
        ).order_by('hour').all()
        
        return {
            'monthly_trends': [
                {
                    'year': int(item.year),
                    'month': int(item.month),
                    'order_count': item.order_count,
                    'total_revenue': item.total_revenue,
                    'average_order_value': item.average_order_value
                }
                for item in monthly_trends
            ],
            'day_of_week_trends': [
                {
                    'day_of_week': int(item.day_of_week),
                    'day_name': ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'][int(item.day_of_week)],
                    'order_count': item.order_count,
                    'total_revenue': item.total_revenue,
                    'average_order_value': item.average_order_value
                }
                for item in dow_trends
            ],
            'hourly_trends': [
                {
                    'hour': int(item.hour),
                    'order_count': item.order_count,
                    'total_revenue': item.total_revenue
                }
                for item in hour_trends
            ]
        }
    
    def get_customer_segmentation(self) -> Dict[str, Any]:
        """
        Segment customers using RFM analysis (Recency, Frequency, Monetary).
        Advanced customer analytics using window functions.
        """
        
        # Calculate RFM scores for each customer
        rfm_query = self.db.query(
            User.id.label('user_id'),
            User.email,
            User.created_at.label('customer_since'),
            
            # Recency: Days since last order
            func.extract('days', 
                datetime.utcnow() - func.max(Order.created_at)
            ).label('recency_days'),
            
            # Frequency: Total number of orders
            func.count(Order.id).label('frequency'),
            
            # Monetary: Total amount spent
            func.sum(Order.total_amount).label('monetary_value')
            
        ).join(Order).filter(
            Order.status == 'completed'
        ).group_by(
            User.id, User.email, User.created_at
        ).subquery()
        
        # Calculate RFM scores using quintiles
        rfm_with_scores = self.db.query(
            rfm_query,
            
            # Recency score (lower recency = higher score)
            func.ntile(5).over(
                order_by=rfm_query.c.recency_days.asc()
            ).label('recency_score'),
            
            # Frequency score  
            func.ntile(5).over(
                order_by=rfm_query.c.frequency.asc()
            ).label('frequency_score'),
            
            # Monetary score
            func.ntile(5).over(
                order_by=rfm_query.c.monetary_value.asc()
            ).label('monetary_score')
            
        ).all()
        
        # Categorize customers based on RFM scores
        segmented_customers = {
            'Champions': [],      # High RFM scores (5,5,5) or (5,4,5), etc.
            'Loyal Customers': [], # High frequency and monetary, medium recency
            'Potential Loyalists': [], # Recent customers with potential
            'New Customers': [],   # Recent but low frequency/monetary
            'At Risk': [],        # Previously valuable but not recent
            'Lost': []           # Low recency, previously valuable
        }
        
        for customer in rfm_with_scores:
            r, f, m = customer.recency_score, customer.frequency_score, customer.monetary_score
            
            # Customer segmentation logic
            if r >= 4 and f >= 4 and m >= 4:
                segment = 'Champions'
            elif r >= 3 and f >= 3 and m >= 3:
                segment = 'Loyal Customers'
            elif r >= 4 and f <= 3:
                segment = 'Potential Loyalists'
            elif r >= 4 and f <= 2:
                segment = 'New Customers'
            elif r <= 2 and (f >= 3 or m >= 3):
                segment = 'At Risk'
            else:
                segment = 'Lost'
            
            segmented_customers[segment].append({
                'user_id': customer.user_id,
                'email': customer.email,
                'customer_since': customer.customer_since.isoformat(),
                'rfm_scores': {'recency': r, 'frequency': f, 'monetary': m},
                'metrics': {
                    'recency_days': int(customer.recency_days) if customer.recency_days else 0,
                    'total_orders': customer.frequency,
                    'total_spent': customer.monetary_value
                }
            })
        
        return segmented_customers
    
    def get_product_recommendation_data(self, user_id: int, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Generate product recommendation data using collaborative filtering.
        Demonstrates advanced relationship queries for recommendation systems.
        """
        
        # Get user's purchase history
        user_products = self.db.query(Product.id).join(OrderItem).join(Order).filter(
            Order.user_id == user_id
        ).subquery()
        
        # Find similar customers (those who bought similar products)
        similar_customers = self.db.query(
            Order.user_id,
            func.count(distinct(OrderItem.product_id)).label('common_products')
        ).join(OrderItem).filter(
            and_(
                OrderItem.product_id.in_(user_products),
                Order.user_id != user_id
            )
        ).group_by(Order.user_id).having(
            func.count(distinct(OrderItem.product_id)) >= 2
        ).order_by(
            func.count(distinct(OrderItem.product_id)).desc()
        ).limit(50).subquery()
        
        # Get products purchased by similar customers but not by target user
        recommendations = self.db.query(
            Product.id,
            Product.name,
            Product.sale_price,
            func.count(distinct(similar_customers.c.user_id)).label('similar_customer_count'),
            func.sum(OrderItem.quantity).label('total_purchased_by_similar'),
            func.avg(OrderItem.unit_price).label('average_price_paid')
        ).join(OrderItem).join(Order).join(
            similar_customers, Order.user_id == similar_customers.c.user_id
        ).filter(
            and_(
                ~Product.id.in_(user_products),
                Product.is_active == True,
                Product.deleted_at.is_(None)
            )
        ).group_by(
            Product.id, Product.name, Product.sale_price
        ).order_by(
            func.count(distinct(similar_customers.c.user_id)).desc(),
            func.sum(OrderItem.quantity).desc()
        ).limit(limit).all()
        
        return [
            {
                'product_id': item.id,
                'name': item.name,
                'sale_price': item.sale_price,
                'recommendation_score': item.similar_customer_count,
                'popularity_among_similar': item.total_purchased_by_similar,
                'average_price_paid': item.average_price_paid
            }
            for item in recommendations
        ]
```

### 4.5 Advanced Query Patterns and Optimization

**app/db/query_patterns.py** - Advanced SQLAlchemy Query Patterns:

```python
"""
Advanced SQLAlchemy query patterns and optimization techniques.
Demonstrates complex queries, performance optimization, and PostgreSQL features.
"""

from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
from decimal import Decimal

from sqlalchemy import (
    and_, or_, not_, func, case, cast, text, literal_column,
    Integer, String, DateTime, select, union_all, exists
)
from sqlalchemy.orm import Session, Query, aliased, joinedload, selectinload
from sqlalchemy.sql import expression
from sqlalchemy.dialects.postgresql import TSVECTOR, aggregate_order_by

from app.models.ecommerce import Product, Order, OrderItem, User, Category, Brand


class AdvancedQueryPatterns:
    """Demonstrates advanced SQLAlchemy query patterns."""
    
    def __init__(self, db: Session):
        self.db = db
    
    def complex_product_search(self, filters: Dict[str, Any]) -> Query:
        """
        Demonstrates complex filtering with dynamic query building.
        Shows best practices for handling optional filters.
        """
        
        # Start with base query
        query = self.db.query(Product).filter(
            and_(
                Product.is_active == True,
                Product.deleted_at.is_(None)
            )
        )
        
        # Dynamic filter building
        if 'search_term' in filters:
            search_term = filters['search_term']
            
            # Full-text search with ranking
            search_vector = func.to_tsvector('english', 
                func.coalesce(Product.name, '') + ' ' +
                func.coalesce(Product.description, '') + ' ' +
                func.coalesce(Product.short_description, '')
            )
            
            search_query = func.plainto_tsquery('english', search_term)
            
            query = query.add_columns(
                func.ts_rank(search_vector, search_query).label('search_rank')
            ).filter(
                search_vector.match(search_query)
            ).order_by(
                func.ts_rank(search_vector, search_query).desc()
            )
        
        # Category filtering with hierarchy support
        if 'category_ids' in filters:
            category_ids = filters['category_ids']
            
            # Include subcategories using recursive CTE
            category_hierarchy = self.db.query(Category.id).filter(
                Category.id.in_(category_ids)
            ).cte(recursive=True)
            
            # Add recursive part (children)
            category_alias = aliased(Category)
            category_hierarchy = category_hierarchy.union_all(
                self.db.query(category_alias.id).filter(
                    category_alias.parent_id == category_hierarchy.c.id
                )
            )
            
            query = query.join(Product.categories).filter(
                Category.id.in_(self.db.query(category_hierarchy.c.id))
            )
        
        # Price range with currency conversion support
        if 'price_range' in filters:
            min_price, max_price = filters['price_range']
            
            # Handle currency conversion (simplified)
            base_currency = filters.get('currency', 'USD')
            
            if base_currency != 'USD':
                # In real implementation, would use exchange rate table
                conversion_rate = filters.get('conversion_rate', 1.0)
                converted_min = min_price / conversion_rate if min_price else None
                converted_max = max_price / conversion_rate if max_price else None
            else:
                converted_min, converted_max = min_price, max_price
            
            if converted_min:
                query = query.filter(Product.sale_price >= converted_min)
            if converted_max:
                query = query.filter(Product.sale_price <= converted_max)
        
        # Availability filtering
        if filters.get('in_stock_only', False):
            query = query.filter(
                or_(
                    Product.manage_stock == False,
                    and_(
                        Product.manage_stock == True,
                        Product.stock_quantity > 0
                    )
                )
            )
        
        # Review rating filtering
        if 'min_rating' in filters:
            min_rating = filters['min_rating']
            
            # Subquery for average ratings
            avg_rating_subquery = self.db.query(
                OrderItem.product_id,
                func.avg(cast(OrderItem.review_rating, Integer)).label('avg_rating')
            ).filter(
                OrderItem.review_rating.isnot(None)
            ).group_by(OrderItem.product_id).subquery()
            
            query = query.outerjoin(
                avg_rating_subquery,
                Product.id == avg_rating_subquery.c.product_id
            ).filter(
                or_(
                    avg_rating_subquery.c.avg_rating >= min_rating,
                    avg_rating_subquery.c.avg_rating.is_(None)  # Include unrated products
                )
            )
        
        # Geographic availability
        if 'shipping_country' in filters:
            country = filters['shipping_country']
            
            # Check product shipping restrictions
            query = query.filter(
                or_(
                    Product.shipping_restrictions.is_(None),
                    ~Product.shipping_restrictions.contains([country])
                )
            )
        
        return query
    
    def advanced_order_analytics(self, analysis_type: str) -> List[Dict[str, Any]]:
        """
        Advanced order analytics with different analysis types.
        Demonstrates complex aggregations and window functions.
        """
        
        if analysis_type == 'customer_journey':
            # Customer journey analysis with order sequence
            customer_journey = self.db.query(
                User.id.label('user_id'),
                User.email,
                Order.id.label('order_id'),
                Order.created_at,
                Order.total_amount,
                func.row_number().over(
                    partition_by=User.id,
                    order_by=Order.created_at
                ).label('order_sequence'),
                func.lag(Order.created_at).over(
                    partition_by=User.id,
                    order_by=Order.created_at
                ).label('previous_order_date'),
                func.lead(Order.created_at).over(
                    partition_by=User.id,
                    order_by=Order.created_at
                ).label('next_order_date')
            ).join(Order).filter(
                Order.status == 'completed'
            ).order_by(User.id, Order.created_at).all()
            
            return [
                {
                    'user_id': item.user_id,
                    'email': item.email,
                    'order_sequence': item.order_sequence,
                    'order_date': item.created_at.isoformat(),
                    'amount': item.total_amount,
                    'days_since_previous': (
                        (item.created_at - item.previous_order_date).days
                        if item.previous_order_date else None
                    ),
                    'days_to_next': (
                        (item.next_order_date - item.created_at).days
                        if item.next_order_date else None
                    )
                }
                for item in customer_journey
            ]
        
        elif analysis_type == 'cohort_retention':
            # Monthly cohort retention analysis
            cohort_sizes = self.db.query(
                func.date_trunc('month', User.created_at).label('cohort_month'),
                func.count(User.id).label('cohort_size')
            ).group_by(
                func.date_trunc('month', User.created_at)
            ).subquery()
            
            # Customer activity by month
            monthly_activity = self.db.query(
                func.date_trunc('month', User.created_at).label('cohort_month'),
                func.date_trunc('month', Order.created_at).label('activity_month'),
                func.count(distinct(Order.user_id)).label('active_customers')
            ).join(Order).filter(
                Order.status == 'completed'
            ).group_by(
                func.date_trunc('month', User.created_at),
                func.date_trunc('month', Order.created_at)
            ).subquery()
            
            # Calculate retention percentages
            retention_data = self.db.query(
                cohort_sizes.c.cohort_month,
                monthly_activity.c.activity_month,
                cohort_sizes.c.cohort_size,
                monthly_activity.c.active_customers,
                (monthly_activity.c.active_customers * 100.0 / cohort_sizes.c.cohort_size).label('retention_rate')
            ).join(
                monthly_activity,
                cohort_sizes.c.cohort_month == monthly_activity.c.cohort_month
            ).order_by(
                cohort_sizes.c.cohort_month,
                monthly_activity.c.activity_month
            ).all()
            
            return [
                {
                    'cohort_month': item.cohort_month.isoformat(),
                    'activity_month': item.activity_month.isoformat(),
                    'cohort_size': item.cohort_size,
                    'active_customers': item.active_customers,
                    'retention_rate': round(item.retention_rate, 2),
                    'period_offset': (
                        (item.activity_month.year - item.cohort_month.year) * 12 +
                        (item.activity_month.month - item.cohort_month.month)
                    )
                }
                for item in retention_data
            ]
    
    def advanced_inventory_queries(self) -> Dict[str, Any]:
        """
        Advanced inventory analysis using CTEs and window functions.
        Demonstrates PostgreSQL-specific features.
        """
        
        # Inventory turnover calculation using CTE
        with self.db.begin():
            # Create temporary function for inventory turnover calculation
            self.db.execute(text("""
                CREATE OR REPLACE FUNCTION calculate_inventory_turnover(
                    p_product_id INTEGER,
                    p_period_days INTEGER DEFAULT 365
                ) RETURNS NUMERIC AS $$
                DECLARE
                    avg_inventory NUMERIC;
                    cogs NUMERIC;  -- Cost of Goods Sold
                BEGIN
                    -- Calculate average inventory over period
                    SELECT AVG(stock_quantity) INTO avg_inventory
                    FROM inventory_logs 
                    WHERE product_id = p_product_id 
                    AND created_at >= NOW() - INTERVAL '%s days' % p_period_days;
                    
                    -- Calculate cost of goods sold
                    SELECT SUM(quantity_change * COALESCE(cost_per_unit, 0)) INTO cogs
                    FROM inventory_logs 
                    WHERE product_id = p_product_id 
                    AND movement_type = 'sale'
                    AND created_at >= NOW() - INTERVAL '%s days' % p_period_days;
                    
                    -- Return turnover ratio
                    IF avg_inventory > 0 THEN
                        RETURN COALESCE(cogs, 0) / avg_inventory;
                    ELSE
                        RETURN 0;
                    END IF;
                END;
                $$ LANGUAGE plpgsql;
            """))
        
        # Advanced inventory analysis query
        inventory_analysis = self.db.query(
            Product.id,
            Product.name,
            Product.sku,
            Product.stock_quantity,
            Product.cost_price,
            Product.sale_price,
            ((Product.sale_price - Product.cost_price) / Product.sale_price * 100).label('margin_percent'),
            
            # Use custom PostgreSQL function
            func.calculate_inventory_turnover(Product.id, 365).label('annual_turnover'),
            
            # Stock value
            (Product.stock_quantity * Product.cost_price).label('stock_value'),
            
            # Sales velocity (units per day over last 90 days)
            func.coalesce(
                self.db.query(
                    func.sum(OrderItem.quantity) / 90.0
                ).join(Order).filter(
                    and_(
                        OrderItem.product_id == Product.id,
                        Order.created_at >= datetime.utcnow() - timedelta(days=90),
                        Order.status == 'completed'
                    )
                ).scalar_subquery(), 0
            ).label('velocity_per_day')
            
        ).filter(
            and_(
                Product.manage_stock == True,
                Product.is_active == True
            )
        ).all()
        
        return [
            {
                'product_id': item.id,
                'name': item.name,
                'sku': item.sku,
                'current_stock': item.stock_quantity,
                'stock_value': item.stock_value,
                'margin_percent': round(item.margin_percent or 0, 2),
                'annual_turnover': float(item.annual_turnover or 0),
                'velocity_per_day': float(item.velocity_per_day),
                'days_of_stock': (
                    int(item.stock_quantity / item.velocity_per_day)
                    if item.velocity_per_day > 0 else None
                ),
                'stock_status': self._classify_stock_status(
                    item.annual_turnover, item.velocity_per_day, item.stock_quantity
                )
            }
            for item in inventory_analysis
        ]
    
    def _classify_stock_status(self, turnover: float, velocity: float, stock: int) -> str:
        """Classify stock status based on turnover and velocity."""
        
        if velocity == 0:
            return 'No Sales'
        elif turnover > 8:  # More than 8 turns per year
            return 'Fast Moving'
        elif turnover > 4:
            return 'Medium Moving' 
        elif turnover > 1:
            return 'Slow Moving'
        else:
            return 'Dead Stock'
    
    def geographic_sales_analysis(self) -> List[Dict[str, Any]]:
        """
        Analyze sales by geographic region using JSON queries.
        Demonstrates PostgreSQL JSON operations with SQLAlchemy.
        """
        
        # Extract country from shipping address JSON
        geographic_analysis = self.db.query(
            (Order.shipping_address['country'].astext).label('country'),
            (Order.shipping_address['state'].astext).label('state'),
            (Order.shipping_address['city'].astext).label('city'),
            func.count(Order.id).label('order_count'),
            func.sum(Order.total_amount).label('total_revenue'),
            func.avg(Order.total_amount).label('average_order_value'),
            func.count(distinct(Order.user_id)).label('unique_customers')
        ).filter(
            and_(
                Order.status == 'completed',
                Order.shipping_address.isnot(None)
            )
        ).group_by(
            Order.shipping_address['country'].astext,
            Order.shipping_address['state'].astext,
            Order.shipping_address['city'].astext
        ).order_by(
            func.sum(Order.total_amount).desc()
        ).all()
        
        return [
            {
                'country': item.country,
                'state': item.state,
                'city': item.city,
                'order_count': item.order_count,
                'total_revenue': item.total_revenue,
                'average_order_value': item.average_order_value,
                'unique_customers': item.unique_customers
            }
            for item in geographic_analysis
        ]
    
    def product_cross_sell_analysis(self) -> List[Dict[str, Any]]:
        """
        Analyze which products are frequently bought together.
        Uses self-joins and advanced aggregations.
        """
        
        # Create aliases for self-join
        item1 = aliased(OrderItem)
        item2 = aliased(OrderItem)
        product1 = aliased(Product)
        product2 = aliased(Product)
        
        # Find product pairs frequently bought together
        cross_sell_data = self.db.query(
            item1.product_id.label('product_a_id'),
            product1.name.label('product_a_name'),
            item2.product_id.label('product_b_id'),
            product2.name.label('product_b_name'),
            func.count(distinct(item1.order_id)).label('co_occurrence_count'),
            
            # Calculate lift (how much more likely B is purchased with A)
            (func.count(distinct(item1.order_id)) * 1.0 /
             func.count(distinct(
                 self.db.query(OrderItem.order_id).filter(
                     OrderItem.product_id == item1.product_id
                 ).subquery()
             ))
            ).label('confidence'),
            
        ).join(
            item2, item1.order_id == item2.order_id
        ).join(
            product1, item1.product_id == product1.id
        ).join(
            product2, item2.product_id == product2.id
        ).filter(
            item1.product_id < item2.product_id  # Avoid duplicate pairs
        ).group_by(
            item1.product_id, product1.name,
            item2.product_id, product2.name
        ).having(
            func.count(distinct(item1.order_id)) >= 5  # Minimum co-occurrences
        ).order_by(
            func.count(distinct(item1.order_id)).desc()
        ).limit(50).all()
        
        return [
            {
                'product_a': {'id': item.product_a_id, 'name': item.product_a_name},
                'product_b': {'id': item.product_b_id, 'name': item.product_b_name},
                'co_occurrence_count': item.co_occurrence_count,
                'confidence': round(float(item.confidence or 0), 3),
                'recommendation_strength': (
                    'High' if item.co_occurrence_count >= 20 else
                    'Medium' if item.co_occurrence_count >= 10 else 'Low'
                )
            }
            for item in cross_sell_data
        ]
    
    def time_series_sales_forecast(self, days_ahead: int = 30) -> List[Dict[str, Any]]:
        """
        Simple sales forecasting using historical trends.
        Demonstrates advanced date functions and trend analysis.
        """
        
        # Get daily sales for last 90 days
        ninety_days_ago = datetime.utcnow() - timedelta(days=90)
        
        daily_sales = self.db.query(
            func.date_trunc('day', Order.created_at).label('sale_date'),
            func.count(Order.id).label('order_count'),
            func.sum(Order.total_amount).label('daily_revenue'),
            func.avg(Order.total_amount).label('average_order_value')
        ).filter(
            and_(
                Order.created_at >= ninety_days_ago,
                Order.status == 'completed'
            )
        ).group_by(
            func.date_trunc('day', Order.created_at)
        ).order_by('sale_date').all()
        
        # Simple moving average calculation
        historical_data = []
        for i, item in enumerate(daily_sales):
            # Calculate 7-day moving average
            window_start = max(0, i - 6)
            window_data = daily_sales[window_start:i+1]
            
            moving_avg_revenue = sum(d.daily_revenue for d in window_data) / len(window_data)
            moving_avg_orders = sum(d.order_count for d in window_data) / len(window_data)
            
            historical_data.append({
                'date': item.sale_date.date().isoformat(),
                'actual_revenue': item.daily_revenue,
                'actual_orders': item.order_count,
                'moving_avg_revenue': round(moving_avg_revenue, 2),
                'moving_avg_orders': round(moving_avg_orders, 1),
                'trend': self._calculate_trend(historical_data[-7:] if len(historical_data) >= 7 else historical_data)
            })
        
        # Simple forecast using trend extrapolation
        if len(historical_data) >= 14:
            last_week_avg = sum(d['moving_avg_revenue'] for d in historical_data[-7:]) / 7
            previous_week_avg = sum(d['moving_avg_revenue'] for d in historical_data[-14:-7]) / 7
            growth_rate = (last_week_avg - previous_week_avg) / previous_week_avg if previous_week_avg > 0 else 0
            
            forecast = []
            for i in range(days_ahead):
                forecast_date = datetime.utcnow().date() + timedelta(days=i+1)
                forecast_revenue = last_week_avg * (1 + growth_rate) ** (i / 7)
                
                forecast.append({
                    'date': forecast_date.isoformat(),
                    'forecast_revenue': round(forecast_revenue, 2),
                    'confidence': max(0.5, 1.0 - (i / days_ahead) * 0.4)  # Decreasing confidence
                })
        else:
            forecast = []
        
        return {
            'historical_data': historical_data,
            'forecast': forecast,
            'analysis_date': datetime.utcnow().isoformat()
        }
    
    def _calculate_trend(self, data: List[Dict]) -> str:
        """Calculate trend direction from recent data."""
        if len(data) < 3:
            return 'insufficient_data'
        
        recent_avg = sum(d['moving_avg_revenue'] for d in data[-3:]) / 3
        earlier_avg = sum(d['moving_avg_revenue'] for d in data[:3]) / 3
        
        if recent_avg > earlier_avg * 1.05:
            return 'increasing'
        elif recent_avg < earlier_avg * 0.95:
            return 'decreasing'
        else:
            return 'stable'


# Query optimization utilities
class QueryOptimizer:
    """Utilities for query optimization and performance monitoring."""
    
    def __init__(self, db: Session):
        self.db = db
    
    def analyze_query_performance(self, query: Query) -> Dict[str, Any]:
        """
        Analyze query performance using EXPLAIN.
        Useful for identifying optimization opportunities.
        """
        
        # Get query execution plan
        explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {str(query.statement.compile(compile_kwargs={'literal_binds': True}))}"
        
        result = self.db.execute(text(explain_query)).fetchone()
        execution_plan = result[0] if result else []
        
        # Extract key metrics from execution plan
        if execution_plan:
            plan_data = execution_plan[0]
            return {
                'execution_time_ms': plan_data.get('Execution Time'),
                'planning_time_ms': plan_data.get('Planning Time'),
                'total_cost': plan_data.get('Plan', {}).get('Total Cost'),
                'rows': plan_data.get('Plan', {}).get('Actual Rows'),
                'plan': plan_data
            }
        
        return {}
    
    def get_slow_queries_log(self) -> List[Dict[str, Any]]:
        """
        Get slow query log from PostgreSQL.
        Requires pg_stat_statements extension.
        """
        
        try:
            slow_queries = self.db.execute(text("""
                SELECT 
                    query,
                    calls,
                    total_time,
                    mean_time,
                    rows,
                    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
                FROM pg_stat_statements 
                WHERE query NOT LIKE '%pg_stat_statements%'
                ORDER BY total_time DESC 
                LIMIT 10
            """)).fetchall()
            
            return [
                {
                    'query': row.query[:200] + '...' if len(row.query) > 200 else row.query,
                    'calls': row.calls,
                    'total_time_ms': round(row.total_time, 2),
                    'mean_time_ms': round(row.mean_time, 2),
                    'rows': row.rows,
                    'cache_hit_percent': round(row.hit_percent or 0, 2)
                }
                for row in slow_queries
            ]
            
        except Exception as e:
            return [{'error': f'Could not retrieve slow query log: {str(e)}'}]
    
    def suggest_indexes(self, table_name: str) -> List[str]:
        """
        Suggest indexes based on query patterns.
        Analyzes missing indexes from query plans.
        """
        
        try:
            # Get table statistics
            table_stats = self.db.execute(text(f"""
                SELECT 
                    schemaname,
                    tablename,
                    attname as column_name,
                    n_distinct,
                    correlation
                FROM pg_stats 
                WHERE tablename = '{table_name}'
                ORDER BY n_distinct DESC
            """)).fetchall()
            
            suggestions = []
            
            for stat in table_stats:
                if stat.n_distinct > 100:  # High cardinality
                    suggestions.append(f"CREATE INDEX idx_{table_name}_{stat.column_name} ON {table_name}({stat.column_name});")
                
                if abs(stat.correlation or 0) > 0.8:  # High correlation with physical order
                    suggestions.append(f"-- Consider clustering table {table_name} on {stat.column_name}")
            
            return suggestions
            
        except Exception as e:
            return [f"-- Error analyzing table {table_name}: {str(e)}"]

## Section 4.5: E-commerce API Endpoints and Advanced Features

Building complete REST API endpoints for the e-commerce system with advanced features like real-time inventory tracking, payment processing, and business analytics.

**app/schemas/ecommerce.py** - E-commerce API Schemas:

```python
"""
Comprehensive Pydantic schemas for e-commerce API endpoints.
Includes product management, order processing, payment handling, and analytics.
"""

from datetime import datetime, date
from decimal import Decimal
from typing import Optional, List, Dict, Any, Union
from uuid import UUID

from pydantic import BaseModel, Field, validator, root_validator, HttpUrl
from pydantic.types import EmailStr, constr

from app.models.ecommerce import OrderStatus, PaymentStatus
from app.schemas.common import BaseSchema, TimestampMixin, UUIDMixin, PaginatedResponse
from app.schemas.user import UserSummaryResponse


# Product Schemas
class ProductBase(BaseSchema):
    """Base product schema with common fields."""
    name: str = Field(..., min_length=2, max_length=200)
    slug: str = Field(..., min_length=2, max_length=200)
    sku: str = Field(..., min_length=2, max_length=50)
    short_description: Optional[str] = Field(None, max_length=500)
    description: Optional[str] = Field(None, max_length=10000)
    sale_price: Decimal = Field(..., gt=0, max_digits=12, decimal_places=2)
    cost_price: Optional[Decimal] = Field(None, gt=0, max_digits=12, decimal_places=2)
    is_active: bool = Field(default=True)
    manage_stock: bool = Field(default=True)
    stock_quantity: int = Field(default=0, ge=0)
    low_stock_threshold: int = Field(default=5, ge=0)
    specifications: Optional[Dict[str, Any]] = None
    meta_title: Optional[str] = Field(None, max_length=200)
    meta_description: Optional[str] = Field(None, max_length=500)

    @validator('slug')
    def validate_slug(cls, v):
        if not v.replace('-', '').replace('_', '').isalnum():
            raise ValueError('Slug can only contain letters, numbers, hyphens, and underscores')
        return v

    @validator('specifications')
    def validate_specifications(cls, v):
        if v and not isinstance(v, dict):
            raise ValueError('Specifications must be a dictionary')
        return v


class ProductCreate(ProductBase):
    """Schema for product creation."""
    brand_id: Optional[int] = Field(None, gt=0)
    category_ids: List[int] = Field(default_factory=list, description="List of category IDs")
    tag_ids: List[int] = Field(default_factory=list, description="List of tag IDs")


class ProductUpdate(BaseSchema):
    """Schema for product updates."""
    name: Optional[str] = Field(None, min_length=2, max_length=200)
    slug: Optional[str] = Field(None, min_length=2, max_length=200)
    description: Optional[str] = Field(None, max_length=10000)
    sale_price: Optional[Decimal] = Field(None, gt=0, max_digits=12, decimal_places=2)
    is_active: Optional[bool] = None
    stock_quantity: Optional[int] = Field(None, ge=0)
    category_ids: Optional[List[int]] = Field(None, description="List of category IDs")


class CategoryResponse(BaseSchema, TimestampMixin, UUIDMixin):
    """Category response schema."""
    id: int
    name: str
    slug: str
    description: Optional[str]
    parent_id: Optional[int]
    is_visible: bool
    sort_order: int
    products_count: int = 0


class BrandResponse(BaseSchema, TimestampMixin, UUIDMixin):
    """Brand response schema."""
    id: int
    name: str
    slug: str
    description: Optional[str]
    logo_url: Optional[str]
    website_url: Optional[str]
    is_active: bool
    products_count: int = 0


class ProductResponse(ProductBase, TimestampMixin, UUIDMixin):
    """Product response schema."""
    id: int
    product_type: str
    brand: Optional[BrandResponse]
    categories: List[CategoryResponse] = []
    average_rating: Optional[Decimal] = None
    reviews_count: int = 0
    total_sales: int = 0
    view_count: int = 0

    @validator('average_rating')
    def round_rating(cls, v):
        return round(v, 2) if v else None


class ProductSummaryResponse(BaseSchema):
    """Minimal product info for listings."""
    id: int
    name: str
    slug: str
    sale_price: Decimal
    stock_quantity: int
    is_active: bool
    image_url: Optional[str] = None


# Cart Schemas
class CartItemCreate(BaseSchema):
    """Schema for adding item to cart."""
    product_id: int = Field(..., gt=0)
    quantity: int = Field(..., gt=0, le=100)
    product_options: Optional[Dict[str, Any]] = None

    @validator('product_options')
    def validate_options(cls, v):
        if v and not isinstance(v, dict):
            raise ValueError('Product options must be a dictionary')
        return v


class CartItemUpdate(BaseSchema):
    """Schema for updating cart item."""
    quantity: int = Field(..., gt=0, le=100)


class CartItemResponse(BaseSchema, TimestampMixin):
    """Cart item response schema."""
    id: int
    product_id: int
    product: ProductSummaryResponse
    quantity: int
    unit_price: Decimal
    total_price: Decimal
    product_options: Optional[Dict[str, Any]] = None


class CartResponse(BaseSchema, TimestampMixin, UUIDMixin):
    """Cart response schema."""
    id: int
    user_id: int
    status: str
    items: List[CartItemResponse] = []
    items_count: int = 0
    subtotal: Decimal = Decimal('0.00')
    currency: str = "USD"


# Order Schemas
class AddressSchema(BaseSchema):
    """Address schema for shipping/billing."""
    first_name: str = Field(..., min_length=1, max_length=100)
    last_name: str = Field(..., min_length=1, max_length=100)
    company: Optional[str] = Field(None, max_length=100)
    address_line_1: str = Field(..., min_length=5, max_length=200)
    address_line_2: Optional[str] = Field(None, max_length=200)
    city: str = Field(..., min_length=2, max_length=100)
    state: str = Field(..., min_length=2, max_length=100)
    postal_code: str = Field(..., min_length=3, max_length=20)
    country: str = Field(..., min_length=2, max_length=100)
    phone: Optional[str] = Field(None, max_length=20)


class OrderCreate(BaseSchema):
    """Schema for order creation."""
    cart_id: int = Field(..., gt=0)
    shipping_address: AddressSchema
    billing_address: Optional[AddressSchema] = None
    shipping_method: Optional[str] = Field(None, max_length=50)
    notes: Optional[str] = Field(None, max_length=1000)

    @root_validator
    def set_billing_address(cls, values):
        if not values.get('billing_address'):
            values['billing_address'] = values.get('shipping_address')
        return values


class OrderItemResponse(BaseSchema):
    """Order item response schema."""
    id: int
    product_id: int
    product_name: str
    product_sku: str
    quantity: int
    unit_price: Decimal
    total_price: Decimal
    product_options: Optional[Dict[str, Any]] = None


class OrderResponse(BaseSchema, TimestampMixin, UUIDMixin):
    """Order response schema."""
    id: int
    order_number: str
    user_id: int
    user: UserSummaryResponse
    status: OrderStatus
    items: List[OrderItemResponse] = []
    subtotal: Decimal
    tax_amount: Decimal
    shipping_cost: Decimal
    total_amount: Decimal
    currency: str
    customer_email: EmailStr
    shipping_address: Dict[str, Any]
    billing_address: Dict[str, Any]
    confirmed_at: Optional[datetime] = None
    shipped_at: Optional[datetime] = None
    delivered_at: Optional[datetime] = None


class OrderSummaryResponse(BaseSchema):
    """Minimal order info for listings."""
    id: int
    order_number: str
    status: OrderStatus
    total_amount: Decimal
    currency: str
    created_at: datetime
    items_count: int


class OrderStatusUpdate(BaseSchema):
    """Schema for updating order status."""
    status: OrderStatus
    notes: Optional[str] = Field(None, max_length=500)


# Payment Schemas
class PaymentMethodBase(BaseSchema):
    """Base payment method schema."""
    payment_method: str = Field(..., description="Payment method type")


class CreditCardPaymentCreate(PaymentMethodBase):
    """Credit card payment creation schema."""
    payment_method: str = Field(default="credit_card", const=True)
    card_number: constr(regex=r'^\d{13,19}$') = Field(..., description="Card number")
    card_holder_name: str = Field(..., min_length=2, max_length=100)
    exp_month: int = Field(..., ge=1, le=12)
    exp_year: int = Field(..., ge=2024, le=2040)
    cvv: constr(regex=r'^\d{3,4}$') = Field(..., description="CVV code")


class PayPalPaymentCreate(PaymentMethodBase):
    """PayPal payment creation schema."""
    payment_method: str = Field(default="paypal", const=True)
    paypal_email: EmailStr = Field(..., description="PayPal email address")


class BankTransferPaymentCreate(PaymentMethodBase):
    """Bank transfer payment creation schema."""
    payment_method: str = Field(default="bank_transfer", const=True)
    bank_name: str = Field(..., min_length=2, max_length=100)
    account_holder: str = Field(..., min_length=2, max_length=100)
    routing_number: str = Field(..., min_length=8, max_length=12)
    account_number: str = Field(..., min_length=8, max_length=20)


PaymentCreateUnion = Union[CreditCardPaymentCreate, PayPalPaymentCreate, BankTransferPaymentCreate]


class PaymentResponse(BaseSchema, TimestampMixin):
    """Payment response schema."""
    id: int
    order_id: int
    payment_method: str
    amount: Decimal
    currency: str
    status: PaymentStatus
    gateway_transaction_id: Optional[str] = None
    processed_at: Optional[datetime] = None
    failure_reason: Optional[str] = None


# Coupon Schemas
class CouponCreate(BaseSchema):
    """Schema for coupon creation."""
    code: str = Field(..., min_length=3, max_length=50)
    name: str = Field(..., min_length=3, max_length=200)
    description: Optional[str] = Field(None, max_length=500)
    discount_type: str = Field(..., regex="^(percentage|fixed_amount|free_shipping)$")
    discount_value: Decimal = Field(..., gt=0, max_digits=10, decimal_places=2)
    max_discount_amount: Optional[Decimal] = Field(None, gt=0, max_digits=10, decimal_places=2)
    min_order_amount: Optional[Decimal] = Field(None, gt=0, max_digits=10, decimal_places=2)
    max_uses: Optional[int] = Field(None, gt=0)
    max_uses_per_user: int = Field(default=1, gt=0)
    valid_from: datetime
    valid_until: Optional[datetime] = None
    first_order_only: bool = Field(default=False)

    @root_validator
    def validate_dates(cls, values):
        valid_from = values.get('valid_from')
        valid_until = values.get('valid_until')
        
        if valid_until and valid_from and valid_until <= valid_from:
            raise ValueError('Valid until date must be after valid from date')
        
        return values


class CouponResponse(BaseSchema, TimestampMixin, UUIDMixin):
    """Coupon response schema."""
    id: int
    code: str
    name: str
    description: Optional[str]
    discount_type: str
    discount_value: Decimal
    max_discount_amount: Optional[Decimal]
    min_order_amount: Optional[Decimal]
    max_uses: Optional[int]
    max_uses_per_user: int
    valid_from: datetime
    valid_until: Optional[datetime]
    is_active: bool
    total_uses: int = 0
    total_discount_given: Decimal = Decimal('0.00')


class CouponValidationRequest(BaseSchema):
    """Schema for coupon validation."""
    coupon_code: str = Field(..., min_length=3, max_length=50)
    order_amount: Decimal = Field(..., gt=0, max_digits=12, decimal_places=2)


class CouponValidationResponse(BaseSchema):
    """Coupon validation response."""
    valid: bool
    coupon_id: Optional[int] = None
    discount_amount: Decimal = Decimal('0.00')
    error_message: Optional[str] = None
    coupon_description: Optional[str] = None


# Analytics Schemas
class ProductAnalytics(BaseSchema):
    """Product analytics response schema."""
    product_id: int
    total_views: int
    total_sales: int
    revenue: Decimal
    average_rating: Optional[Decimal]
    conversion_rate: float
    inventory_turnover: float
    profit_margin: Optional[float]


class OrderAnalytics(BaseSchema):
    """Order analytics response schema."""
    period: str
    total_orders: int
    total_revenue: Decimal
    average_order_value: Decimal
    new_customers: int
    returning_customers: int
    conversion_rate: float


class CustomerSegmentAnalytics(BaseSchema):
    """Customer segment analytics."""
    segment_name: str
    customer_count: int
    total_revenue: Decimal
    average_order_value: Decimal
    average_orders_per_customer: float
    lifetime_value: Decimal


class InventoryAnalytics(BaseSchema):
    """Inventory analytics response."""
    total_products: int
    total_stock_value: Decimal
    low_stock_products: int
    out_of_stock_products: int
    fast_moving_products: int
    slow_moving_products: int
    turnover_rate: float


# Subscription Schemas (for subscription products)
class SubscriptionCreate(BaseSchema):
    """Schema for subscription creation."""
    user_id: int = Field(..., gt=0)
    product_id: int = Field(..., gt=0)
    trial_days: Optional[int] = Field(None, ge=0, le=365)


class SubscriptionResponse(BaseSchema, TimestampMixin):
    """Subscription response schema."""
    id: int
    subscription_reference: str
    user_id: int
    product_id: int
    status: str
    started_at: datetime
    trial_ends_at: Optional[datetime]
    next_billing_date: datetime
    billing_cycles_completed: int
    total_amount_paid: Decimal
    current_users: int
    storage_used_gb: Decimal


# Filter Schemas
class ProductFilterParams(BaseSchema):
    """Product filtering parameters."""
    search_query: Optional[str] = Field(None, max_length=200)
    category_ids: Optional[List[int]] = Field(None, description="Filter by category IDs")
    brand_ids: Optional[List[int]] = Field(None, description="Filter by brand IDs")
    min_price: Optional[Decimal] = Field(None, gt=0)
    max_price: Optional[Decimal] = Field(None, gt=0)
    in_stock_only: bool = Field(default=False)
    min_rating: Optional[float] = Field(None, ge=0, le=5)
    sort_by: str = Field(default="relevance", regex="^(relevance|name|price_asc|price_desc|newest|popularity)$")


class OrderFilterParams(BaseSchema):
    """Order filtering parameters."""
    status: Optional[OrderStatus] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    min_amount: Optional[Decimal] = Field(None, gt=0)
    max_amount: Optional[Decimal] = Field(None, gt=0)


# Paginated Responses
ProductsPaginatedResponse = PaginatedResponse[ProductSummaryResponse]
ProductsDetailPaginatedResponse = PaginatedResponse[ProductResponse]
OrdersPaginatedResponse = PaginatedResponse[OrderSummaryResponse]
OrdersDetailPaginatedResponse = PaginatedResponse[OrderResponse]
CouponsPaginatedResponse = PaginatedResponse[CouponResponse]
```

**app/api/v1/ecommerce.py** - E-commerce API Endpoints:

```python
"""
Complete e-commerce API endpoints with advanced features.
Includes product management, order processing, payment handling, and analytics.
"""

from typing import List, Optional
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import (
    get_current_user, get_current_admin, get_current_author_or_above,
    get_pagination_params, get_optional_user
)
from app.crud.ecommerce import (
    ProductCRUD, CartCRUD, OrderCRUD, PaymentCRUD, CouponCRUD,
    SubscriptionCRUD, InventoryCRUD
)
from app.services.inventory import InventoryManager, OrderProcessor
from app.models.user import User
from app.models.ecommerce import OrderStatus, PaymentStatus
from app.schemas.ecommerce import (
    ProductCreate, ProductUpdate, ProductResponse, ProductSummaryResponse,
    ProductsPaginatedResponse, ProductFilterParams,
    CartItemCreate, CartItemUpdate, CartResponse,
    OrderCreate, OrderResponse, OrdersPaginatedResponse, OrderStatusUpdate,
    PaymentCreateUnion, PaymentResponse,
    CouponCreate, CouponResponse, CouponValidationRequest, CouponValidationResponse,
    ProductAnalytics, OrderAnalytics, InventoryAnalytics
)
from app.schemas.common import APIResponse, PaginationParams

router = APIRouter(prefix="/ecommerce", tags=["ecommerce"])


# Product Management Endpoints
@router.get("/products", response_model=ProductsPaginatedResponse)
async def get_products(
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_user),
    pagination: PaginationParams = Depends(get_pagination_params),
    filters: ProductFilterParams = Depends()
):
    """
    Get products with advanced filtering, search, and pagination.
    Public endpoint with optional authentication for personalized features.
    """
    product_crud = ProductCRUD(db)
    
    try:
        products = product_crud.search_products(
            search_term=filters.search_query,
            category_ids=filters.category_ids,
            brand_ids=filters.brand_ids,
            min_price=filters.min_price,
            max_price=filters.max_price,
            in_stock_only=filters.in_stock_only,
            sort_by=filters.sort_by
        )
        
        # Apply pagination
        total_count = len(products)
        start_idx = (pagination.page - 1) * pagination.size
        end_idx = start_idx + pagination.size
        products_page = products[start_idx:end_idx]
        
        return ProductsPaginatedResponse(
            items=[ProductSummaryResponse.from_orm(p) for p in products_page],
            total_count=total_count,
            page=pagination.page,
            size=pagination.size
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving products: {str(e)}"
        )


@router.get("/products/{product_id}", response_model=APIResponse[ProductResponse])
async def get_product(
    product_id: int,
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_user)
):
    """
    Get detailed product information with analytics tracking.
    """
    product_crud = ProductCRUD(db)
    
    product = product_crud.get(product_id)
    if not product:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Product not found"
        )
    
    if not product.is_active and not (current_user and current_user.is_admin):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Product not found"
        )
    
    # Track product view (background task)
    if current_user:
        # In a real application, this would be a background task
        product_crud.track_product_view(product_id, current_user.id)
    
    return APIResponse(
        success=True,
        data=ProductResponse.from_orm(product),
        message="Product retrieved successfully"
    )


@router.post("/products", response_model=APIResponse[ProductResponse])
async def create_product(
    product_data: ProductCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)
):
    """
    Create a new product (admin only).
    """
    product_crud = ProductCRUD(db)
    
    try:
        product = product_crud.create_with_categories_and_tags(
            obj_in=product_data,
            created_by_user_id=current_user.id
        )
        
        return APIResponse(
            success=True,
            data=ProductResponse.from_orm(product),
            message="Product created successfully"
        )
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.put("/products/{product_id}", response_model=APIResponse[ProductResponse])
async def update_product(
    product_id: int,
    product_data: ProductUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)
):
    """
    Update a product (admin only).
    """
    product_crud = ProductCRUD(db)
    
    existing_product = product_crud.get(product_id)
    if not existing_product:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Product not found"
        )
    
    try:
        updated_product = product_crud.update(
            db_obj=existing_product,
            obj_in=product_data
        )
        
        return APIResponse(
            success=True,
            data=ProductResponse.from_orm(updated_product),
            message="Product updated successfully"
        )
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.get("/products/{product_id}/analytics", response_model=APIResponse[ProductAnalytics])
async def get_product_analytics(
    product_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin),
    days: int = Query(30, ge=1, le=365, description="Number of days for analytics")
):
    """
    Get detailed product analytics (admin only).
    """
    product_crud = ProductCRUD(db)
    
    analytics_data = product_crud.get_product_analytics(product_id)
    if not analytics_data:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Product not found or no analytics data available"
        )
    
    return APIResponse(
        success=True,
        data=ProductAnalytics(**analytics_data),
        message="Product analytics retrieved successfully"
    )


@router.get("/products/{product_id}/related", response_model=APIResponse[List[ProductSummaryResponse]])
async def get_related_products(
    product_id: int,
    db: Session = Depends(get_db),
    limit: int = Query(5, ge=1, le=20, description="Number of related products")
):
    """
    Get related products based on categories and purchase patterns.
    """
    product_crud = ProductCRUD(db)
    
    related_products = product_crud.get_related_products(product_id, limit)
    
    return APIResponse(
        success=True,
        data=[ProductSummaryResponse.from_orm(p) for p in related_products],
        message="Related products retrieved successfully"
    )


# Shopping Cart Endpoints
@router.get("/cart", response_model=APIResponse[CartResponse])
async def get_user_cart(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get current user's shopping cart.
    """
    cart_crud = CartCRUD(db)
    
    cart = cart_crud.get_or_create_cart(current_user.id)
    cart_with_totals = cart_crud.get_cart_with_totals(cart.id)
    
    return APIResponse(
        success=True,
        data=CartResponse.from_orm(cart_with_totals),
        message="Cart retrieved successfully"
    )


@router.post("/cart/items", response_model=APIResponse[CartResponse])
async def add_item_to_cart(
    item_data: CartItemCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Add item to shopping cart with stock validation.
    """
    cart_crud = CartCRUD(db)
    
    try:
        cart = cart_crud.get_or_create_cart(current_user.id)
        cart_item = cart_crud.add_item_to_cart(
            cart_id=cart.id,
            product_id=item_data.product_id,
            quantity=item_data.quantity,
            product_options=item_data.product_options
        )
        
        db.commit()
        
        # Return updated cart
        updated_cart = cart_crud.get_cart_with_totals(cart.id)
        
        return APIResponse(
            success=True,
            data=CartResponse.from_orm(updated_cart),
            message="Item added to cart successfully"
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.put("/cart/items/{item_id}", response_model=APIResponse[CartResponse])
async def update_cart_item(
    item_id: int,
    item_data: CartItemUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update cart item quantity.
    """
    cart_crud = CartCRUD(db)
    
    try:
        updated_item = cart_crud.update_item_quantity(
            cart_item_id=item_id,
            new_quantity=item_data.quantity
        )
        
        if updated_item is None:
            db.commit()
            message = "Item removed from cart"
        else:
            db.commit()
            message = "Cart item updated successfully"
        
        # Get user's cart to return updated data
        cart = cart_crud.get_or_create_cart(current_user.id)
        updated_cart = cart_crud.get_cart_with_totals(cart.id)
        
        return APIResponse(
            success=True,
            data=CartResponse.from_orm(updated_cart),
            message=message
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/cart/items/{item_id}")
async def remove_cart_item(
    item_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Remove item from shopping cart.
    """
    cart_crud = CartCRUD(db)
    
    # Update quantity to 0 to remove item
    updated_item = cart_crud.update_item_quantity(item_id, 0)
    db.commit()
    
    return APIResponse(
        success=True,
        data=None,
        message="Item removed from cart successfully"
    )


@router.post("/cart/validate-coupon", response_model=CouponValidationResponse)
async def validate_coupon(
    validation_data: CouponValidationRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Validate coupon code for current cart.
    """
    cart_crud = CartCRUD(db)
    
    try:
        cart = cart_crud.get_or_create_cart(current_user.id)
        cart_with_totals = cart_crud.get_cart_with_totals(cart.id)
        
        if not cart_with_totals.items:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Cannot apply coupon to empty cart"
            )
        
        validation_result = cart_crud.apply_coupon(
            cart_id=cart.id,
            coupon_code=validation_data.coupon_code,
            user_id=current_user.id
        )
        
        return CouponValidationResponse(
            valid=True,
            coupon_id=validation_result['coupon_id'],
            discount_amount=validation_result['discount_amount'],
            coupon_description=validation_result['coupon_description']
        )
        
    except ValueError as e:
        return CouponValidationResponse(
            valid=False,
            error_message=str(e)
        )


# Order Management Endpoints
@router.get("/orders", response_model=OrdersPaginatedResponse)
async def get_user_orders(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    pagination: PaginationParams = Depends(get_pagination_params),
    status_filter: Optional[OrderStatus] = Query(None, description="Filter by order status")
):
    """
    Get current user's orders with pagination and filtering.
    """
    order_crud = OrderCRUD(db)
    
    orders = order_crud.get_orders_with_details(
        user_id=current_user.id,
        status=status_filter.value if status_filter else None
    )
    
    # Apply pagination
    total_count = len(orders)
    start_idx = (pagination.page - 1) * pagination.size
    end_idx = start_idx + pagination.size
    orders_page = orders[start_idx:end_idx]
    
    return OrdersPaginatedResponse(
        items=[OrderResponse.from_orm(order) for order in orders_page],
        total_count=total_count,
        page=pagination.page,
        size=pagination.size
    )


@router.get("/orders/{order_id}", response_model=APIResponse[OrderResponse])
async def get_order(
    order_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get specific order details.
    """
    order_crud = OrderCRUD(db)
    
    order = order_crud.get(order_id)
    if not order:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Order not found"
        )
    
    # Check ownership unless user is admin
    if not current_user.is_admin and order.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied"
        )
    
    return APIResponse(
        success=True,
        data=OrderResponse.from_orm(order),
        message="Order retrieved successfully"
    )


@router.post("/orders", response_model=APIResponse[OrderResponse])
async def create_order(
    order_data: OrderCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create order from shopping cart with inventory validation.
    """
    cart_crud = CartCRUD(db)
    order_processor = OrderProcessor(db)
    
    try:
        # Get cart and validate
        cart = cart_crud.get_cart_with_totals(order_data.cart_id)
        if not cart or cart.user_id != current_user.id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Cart not found"
            )
        
        if not cart.items:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Cannot create order from empty cart"
            )
        
        # Create order from cart
        order = order_processor.create_order_from_cart(
            cart=cart,
            shipping_address=order_data.shipping_address.dict(),
            billing_address=order_data.billing_address.dict()
        )
        
        db.commit()
        
        # Clear cart after successful order creation
        background_tasks.add_task(cart_crud.clear_cart, cart.id)
        
        return APIResponse(
            success=True,
            data=OrderResponse.from_orm(order),
            message="Order created successfully"
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.post("/orders/{order_id}/payment", response_model=APIResponse[PaymentResponse])
async def process_payment(
    order_id: int,
    payment_data: PaymentCreateUnion,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Process payment for an order.
    """
    order_crud = OrderCRUD(db)
    
    try:
        # Verify order ownership
        order = order_crud.get(order_id)
        if not order or order.user_id != current_user.id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Order not found"
            )
        
        # Process payment
        payment = order_crud.process_order_payment(
            order_id=order_id,
            payment_data=payment_data.dict()
        )
        
        db.commit()
        
        return APIResponse(
            success=True,
            data=PaymentResponse.from_orm(payment),
            message="Payment processed successfully"
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Admin Order Management
@router.get("/admin/orders", response_model=OrdersPaginatedResponse)
async def get_all_orders(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    pagination: PaginationParams = Depends(get_pagination_params),
    status_filter: Optional[OrderStatus] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None)
):
    """
    Get all orders with filtering (admin only).
    """
    order_crud = OrderCRUD(db)
    
    date_range = None
    if start_date and end_date:
        date_range = (start_date, end_date)
    
    orders = order_crud.get_orders_with_details(
        status=status_filter.value if status_filter else None,
        date_range=date_range
    )
    
    # Apply pagination
    total_count = len(orders)
    start_idx = (pagination.page - 1) * pagination.size
    end_idx = start_idx + pagination.size
    orders_page = orders[start_idx:end_idx]
    
    return OrdersPaginatedResponse(
        items=[OrderResponse.from_orm(order) for order in orders_page],
        total_count=total_count,
        page=pagination.page,
        size=pagination.size
    )


@router.put("/admin/orders/{order_id}/status", response_model=APIResponse[OrderResponse])
async def update_order_status(
    order_id: int,
    status_data: OrderStatusUpdate,
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Update order status (admin only).
    """
    order_crud = OrderCRUD(db)
    
    order = order_crud.get(order_id)
    if not order:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Order not found"
        )
    
    try:
        updated_order = order_crud.update_order_status(
            order_id=order_id,
            new_status=status_data.status,
            notes=status_data.notes
        )
        
        db.commit()
        
        return APIResponse(
            success=True,
            data=OrderResponse.from_orm(updated_order),
            message="Order status updated successfully"
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Analytics Endpoints
@router.get("/analytics/dashboard", response_model=APIResponse[Dict[str, Any]])
async def get_analytics_dashboard(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    days: int = Query(30, ge=1, le=365, description="Number of days for analytics")
):
    """
    Get comprehensive analytics dashboard data (admin only).
    """
    order_crud = OrderCRUD(db)
    product_crud = ProductCRUD(db)
    inventory_crud = InventoryCRUD(db)
    
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # Get various analytics
    order_stats = order_crud.get_order_summary_stats(start_date=start_date)
    inventory_valuation = InventoryManager(db).get_inventory_valuation()
    
    dashboard_data = {
        'period_days': days,
        'order_statistics': order_stats,
        'inventory_valuation': inventory_valuation,
        'generated_at': datetime.utcnow().isoformat()
    }
    
    return APIResponse(
        success=True,
        data=dashboard_data,
        message="Analytics dashboard data retrieved successfully"
    )


@router.get("/analytics/products/performance", response_model=APIResponse[List[ProductAnalytics]])
async def get_product_performance(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    limit: int = Query(20, ge=1, le=100, description="Number of top products")
):
    """
    Get top performing products analytics (admin only).
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    performance_data = analytics.get_product_performance_matrix()
    
    # Limit results
    limited_data = performance_data[:limit]
    
    return APIResponse(
        success=True,
        data=limited_data,
        message="Product performance analytics retrieved successfully"
    )


@router.get("/analytics/customers/segments", response_model=APIResponse[Dict[str, List[Dict[str, Any]]]])
async def get_customer_segments(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Get customer segmentation analysis (admin only).
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    segmentation_data = analytics.get_customer_segmentation()
    
    return APIResponse(
        success=True,
        data=segmentation_data,
        message="Customer segmentation analysis retrieved successfully"
    )

# Coupon Management Endpoints
@router.get("/coupons", response_model=APIResponse[List[CouponResponse]])
async def get_active_coupons(
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_user)
):
    """
    Get active coupons available to users.
    """
    coupon_crud = CouponCRUD(db)
    
    active_coupons = coupon_crud.get_active_coupons()
    
    return APIResponse(
        success=True,
        data=[CouponResponse.from_orm(coupon) for coupon in active_coupons],
        message="Active coupons retrieved successfully"
    )


@router.post("/admin/coupons", response_model=APIResponse[CouponResponse])
async def create_coupon(
    coupon_data: CouponCreate,
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Create a new coupon (admin only).
    """
    coupon_crud = CouponCRUD(db)
    
    try:
        coupon = coupon_crud.create(obj_in=coupon_data)
        db.commit()
        
        return APIResponse(
            success=True,
            data=CouponResponse.from_orm(coupon),
            message="Coupon created successfully"
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Inventory Management Endpoints
@router.get("/admin/inventory/status", response_model=APIResponse[Dict[str, Any]])
async def get_inventory_status(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Get comprehensive inventory status (admin only).
    """
    inventory_crud = InventoryCRUD(db)
    inventory_manager = InventoryManager(db)
    
    # Get inventory analytics
    inventory_stats = {
        'low_stock_products': inventory_crud.get_low_stock_report(),
        'out_of_stock_products': inventory_crud.get_out_of_stock_products(),
        'inventory_valuation': inventory_manager.get_inventory_valuation(),
        'abc_analysis': inventory_crud.get_abc_analysis(),
        'slow_moving_products': inventory_crud.get_slow_moving_products()
    }
    
    return APIResponse(
        success=True,
        data=inventory_stats,
        message="Inventory status retrieved successfully"
    )


@router.post("/admin/inventory/adjust", response_model=APIResponse[Dict[str, str]])
async def adjust_inventory(
    product_id: int,
    adjustment: int,
    reason: str,
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin)
):
    """
    Manually adjust product inventory (admin only).
    """
    inventory_manager = InventoryManager(db)
    
    try:
        success = inventory_manager.adjust_inventory(
            product_id=product_id,
            quantity_change=adjustment,
            reason=reason
        )
        
        if success:
            db.commit()
            return APIResponse(
                success=True,
                data={"message": "Inventory adjusted successfully"},
                message="Inventory adjustment completed"
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Inventory adjustment failed"
            )
            
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Real-time Endpoints
@router.get("/products/{product_id}/availability", response_model=APIResponse[Dict[str, Any]])
async def check_product_availability(
    product_id: int,
    quantity: int = Query(1, ge=1, le=100),
    db: Session = Depends(get_db)
):
    """
    Check real-time product availability for given quantity.
    """
    inventory_manager = InventoryManager(db)
    
    try:
        availability = inventory_manager.check_availability(product_id, quantity)
        
        return APIResponse(
            success=True,
            data={
                'product_id': product_id,
                'requested_quantity': quantity,
                'available': availability['available'],
                'stock_quantity': availability['current_stock'],
                'max_available': availability['max_available']
            },
            message="Product availability checked successfully"
        )
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.get("/cart/{cart_id}/totals", response_model=APIResponse[Dict[str, Any]])
async def get_cart_totals(
    cart_id: int,
    coupon_code: Optional[str] = Query(None),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get real-time cart totals with optional coupon calculation.
    """
    cart_crud = CartCRUD(db)
    
    try:
        cart = cart_crud.get_cart_with_totals(cart_id)
        if not cart or cart.user_id != current_user.id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Cart not found"
            )
        
        totals = {
            'subtotal': cart.subtotal,
            'discount_amount': 0,
            'tax_amount': cart.subtotal * 0.08,  # 8% tax
            'shipping_cost': 15.00 if cart.subtotal < 100 else 0,  # Free shipping over $100
            'total_amount': cart.subtotal
        }
        
        # Calculate totals with tax and shipping
        totals['total_amount'] = (
            totals['subtotal'] + 
            totals['tax_amount'] + 
            totals['shipping_cost'] - 
            totals['discount_amount']
        )
        
        # Apply coupon if provided
        if coupon_code:
            validation_result = cart_crud.apply_coupon(
                cart_id=cart_id,
                coupon_code=coupon_code,
                user_id=current_user.id,
                validate_only=True
            )
            totals['discount_amount'] = validation_result['discount_amount']
            totals['total_amount'] -= totals['discount_amount']
            totals['coupon_applied'] = coupon_code
        
        return APIResponse(
            success=True,
            data=totals,
            message="Cart totals calculated successfully"
        )
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Recommendation Endpoints
@router.get("/recommendations/products", response_model=APIResponse[List[ProductSummaryResponse]])
async def get_product_recommendations(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    limit: int = Query(10, ge=1, le=50, description="Number of recommendations")
):
    """
    Get personalized product recommendations for user.
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    recommendations = analytics.get_product_recommendations_for_user(
        user_id=current_user.id,
        limit=limit
    )
    
    return APIResponse(
        success=True,
        data=[ProductSummaryResponse.from_orm(p) for p in recommendations],
        message="Product recommendations retrieved successfully"
    )


@router.get("/recommendations/cross-sell/{product_id}", response_model=APIResponse[List[ProductSummaryResponse]])
async def get_cross_sell_products(
    product_id: int,
    db: Session = Depends(get_db),
    limit: int = Query(5, ge=1, le=20, description="Number of cross-sell products")
):
    """
    Get cross-sell product recommendations for a specific product.
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    cross_sell_products = analytics.analyze_cross_sell_patterns(
        target_product_id=product_id,
        limit=limit
    )
    
    return APIResponse(
        success=True,
        data=[ProductSummaryResponse.from_orm(p) for p in cross_sell_products],
        message="Cross-sell recommendations retrieved successfully"
    )


# Advanced Search Endpoints
@router.get("/search/suggestions", response_model=APIResponse[Dict[str, List[str]]])
async def get_search_suggestions(
    q: str = Query(..., min_length=2, description="Search query"),
    db: Session = Depends(get_db)
):
    """
    Get search suggestions for auto-complete functionality.
    """
    product_crud = ProductCRUD(db)
    
    suggestions = product_crud.get_search_suggestions(q)
    
    return APIResponse(
        success=True,
        data=suggestions,
        message="Search suggestions retrieved successfully"
    )


@router.get("/search/full-text", response_model=ProductsPaginatedResponse)
async def full_text_search(
    q: str = Query(..., min_length=2, description="Search query"),
    db: Session = Depends(get_db),
    pagination: PaginationParams = Depends(get_pagination_params)
):
    """
    Perform full-text search across products using PostgreSQL.
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    search_results = analytics.full_text_product_search(q)
    
    # Apply pagination
    total_count = len(search_results)
    start_idx = (pagination.page - 1) * pagination.size
    end_idx = start_idx + pagination.size
    results_page = search_results[start_idx:end_idx]
    
    return ProductsPaginatedResponse(
        items=[ProductSummaryResponse.from_orm(p) for p in results_page],
        total_count=total_count,
        page=pagination.page,
        size=pagination.size
    )


# Reporting Endpoints
@router.get("/admin/reports/sales", response_model=APIResponse[Dict[str, Any]])
async def get_sales_report(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    group_by: str = Query("day", regex="^(day|week|month)$")
):
    """
    Generate detailed sales reports (admin only).
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    
    # Default to last 30 days if no dates provided
    if not start_date:
        start_date = datetime.utcnow() - timedelta(days=30)
    if not end_date:
        end_date = datetime.utcnow()
    
    sales_data = analytics.generate_sales_forecast(
        start_date=start_date,
        end_date=end_date,
        forecast_days=30
    )
    
    report_data = {
        'period': {
            'start_date': start_date.isoformat(),
            'end_date': end_date.isoformat(),
            'group_by': group_by
        },
        'sales_data': sales_data,
        'generated_at': datetime.utcnow().isoformat()
    }
    
    return APIResponse(
        success=True,
        data=report_data,
        message="Sales report generated successfully"
    )


# Subscription Management Endpoints  
@router.get("/subscriptions", response_model=APIResponse[List[Dict[str, Any]]])
async def get_user_subscriptions(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get current user's active subscriptions.
    """
    subscription_crud = SubscriptionCRUD(db)
    
    subscriptions = subscription_crud.get_user_subscriptions(current_user.id)
    
    return APIResponse(
        success=True,
        data=[{
            'id': sub.id,
            'product_name': sub.product.name,
            'status': sub.status,
            'next_billing_date': sub.next_billing_date.isoformat(),
            'amount': sub.amount,
            'billing_cycle': sub.billing_cycle
        } for sub in subscriptions],
        message="User subscriptions retrieved successfully"
    )


@router.post("/subscriptions/cancel/{subscription_id}")
async def cancel_subscription(
    subscription_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Cancel a user's subscription.
    """
    subscription_crud = SubscriptionCRUD(db)
    
    subscription = subscription_crud.get(subscription_id)
    if not subscription or subscription.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Subscription not found"
        )
    
    try:
        cancelled_subscription = subscription_crud.cancel_subscription(subscription_id)
        db.commit()
        
        return APIResponse(
            success=True,
            data=None,
            message="Subscription cancelled successfully"
        )
        
    except ValueError as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Advanced Admin Analytics Endpoints
@router.get("/admin/analytics/revenue", response_model=APIResponse[Dict[str, Any]])
async def get_revenue_analytics(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    period: str = Query("month", regex="^(week|month|quarter|year)$"),
    compare_previous: bool = Query(False)
):
    """
    Get detailed revenue analytics with period comparison (admin only).
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    
    # Calculate period dates
    end_date = datetime.utcnow()
    
    if period == "week":
        start_date = end_date - timedelta(weeks=1)
    elif period == "month":
        start_date = end_date - timedelta(days=30)
    elif period == "quarter":
        start_date = end_date - timedelta(days=90)
    else:  # year
        start_date = end_date - timedelta(days=365)
    
    revenue_data = {
        'current_period': {
            'start_date': start_date.isoformat(),
            'end_date': end_date.isoformat(),
            'revenue': analytics.calculate_revenue_for_period(start_date, end_date),
            'orders_count': analytics.count_orders_for_period(start_date, end_date),
            'customers_count': analytics.count_customers_for_period(start_date, end_date)
        }
    }
    
    if compare_previous:
        prev_end_date = start_date
        prev_start_date = start_date - (end_date - start_date)
        
        revenue_data['previous_period'] = {
            'start_date': prev_start_date.isoformat(),
            'end_date': prev_end_date.isoformat(),
            'revenue': analytics.calculate_revenue_for_period(prev_start_date, prev_end_date),
            'orders_count': analytics.count_orders_for_period(prev_start_date, prev_end_date),
            'customers_count': analytics.count_customers_for_period(prev_start_date, prev_end_date)
        }
        
        # Calculate growth percentages
        current_revenue = revenue_data['current_period']['revenue']
        previous_revenue = revenue_data['previous_period']['revenue']
        
        if previous_revenue > 0:
            revenue_data['growth'] = {
                'revenue_growth_percent': ((current_revenue - previous_revenue) / previous_revenue) * 100,
                'orders_growth_percent': ((
                    revenue_data['current_period']['orders_count'] - 
                    revenue_data['previous_period']['orders_count']
                ) / revenue_data['previous_period']['orders_count']) * 100
            }
    
    return APIResponse(
        success=True,
        data=revenue_data,
        message="Revenue analytics retrieved successfully"
    )


@router.get("/admin/analytics/customers", response_model=APIResponse[Dict[str, Any]])
async def get_customer_analytics(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    cohort_months: int = Query(6, ge=1, le=24, description="Number of months for cohort analysis")
):
    """
    Get comprehensive customer analytics (admin only).
    """
    from app.db.query_patterns import EcommerceAnalytics
    
    analytics = EcommerceAnalytics(db)
    
    customer_data = {
        'cohort_retention': analytics.calculate_customer_cohort_retention(months=cohort_months),
        'clv_analysis': analytics.calculate_customer_lifetime_value(),
        'rfm_segmentation': analytics.perform_rfm_customer_segmentation(),
        'geographic_distribution': analytics.analyze_sales_by_geography()
    }
    
    return APIResponse(
        success=True,
        data=customer_data,
        message="Customer analytics retrieved successfully"
    )


@router.get("/admin/analytics/inventory/turnover", response_model=APIResponse[Dict[str, Any]])
async def get_inventory_turnover_analysis(
    db: Session = Depends(get_db),
    current_admin: User = Depends(get_current_admin),
    months: int = Query(12, ge=1, le=24, description="Number of months for analysis")
):
    """
    Get detailed inventory turnover analysis (admin only).
    """
    inventory_crud = InventoryCRUD(db)
    
    turnover_data = {
        'abc_analysis': inventory_crud.get_abc_analysis(),
        'slow_moving_products': inventory_crud.get_slow_moving_products(),
        'fast_moving_products': inventory_crud.get_fast_moving_products(limit=20),
        'reorder_suggestions': inventory_crud.get_reorder_suggestions(),
        'analysis_period_months': months,
        'generated_at': datetime.utcnow().isoformat()
    }
    
    return APIResponse(
        success=True,
        data=turnover_data,
        message="Inventory turnover analysis retrieved successfully"
    )
```

**app/services/notifications.py** - Real-time Notification Services:

```python
"""
Real-time notification service for e-commerce events.
Handles order updates, inventory alerts, and customer notifications.
"""

import asyncio
import json
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from decimal import Decimal

from sqlalchemy.orm import Session
from sqlalchemy import select, and_

from app.database import get_db
from app.models.user import User
from app.models.ecommerce import Order, Product, SubscriptionBilling
from app.crud.user import UserCRUD


class NotificationService:
    """Service for handling various types of notifications."""
    
    def __init__(self, db: Session):
        self.db = db
        self.user_crud = UserCRUD(db)
    
    async def send_order_confirmation(self, order_id: int) -> Dict[str, Any]:
        """
        Send order confirmation notification to customer.
        """
        order = self.db.get(Order, order_id)
        if not order:
            raise ValueError("Order not found")
        
        user = self.db.get(User, order.user_id)
        
        notification_data = {
            'type': 'order_confirmation',
            'recipient': user.email,
            'order_number': order.order_number,
            'total_amount': str(order.total_amount),
            'currency': order.currency,
            'estimated_delivery': self._calculate_estimated_delivery(order),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        # In production, this would integrate with email service
        # For demonstration, we'll store in a notifications table
        await self._store_notification(notification_data)
        
        return notification_data
    
    async def send_order_status_update(self, order_id: int, new_status: str) -> Dict[str, Any]:
        """
        Send order status update notification.
        """
        order = self.db.get(Order, order_id)
        if not order:
            raise ValueError("Order not found")
        
        user = self.db.get(User, order.user_id)
        
        notification_data = {
            'type': 'order_status_update',
            'recipient': user.email,
            'order_number': order.order_number,
            'old_status': order.status.value,
            'new_status': new_status,
            'tracking_info': self._get_tracking_info(order),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        await self._store_notification(notification_data)
        return notification_data
    
    async def send_low_stock_alert(self, product_id: int) -> Dict[str, Any]:
        """
        Send low stock alert to admin users.
        """
        product = self.db.get(Product, product_id)
        if not product:
            raise ValueError("Product not found")
        
        # Get admin users
        admin_users = self.user_crud.get_admin_users()
        
        notification_data = {
            'type': 'low_stock_alert',
            'recipients': [admin.email for admin in admin_users],
            'product_id': product_id,
            'product_name': product.name,
            'current_stock': product.stock_quantity,
            'threshold': product.low_stock_threshold,
            'urgency': 'high' if product.stock_quantity == 0 else 'medium',
            'timestamp': datetime.utcnow().isoformat()
        }
        
        await self._store_notification(notification_data)
        return notification_data
    
    async def send_subscription_reminder(self, subscription_id: int, days_until_renewal: int) -> Dict[str, Any]:
        """
        Send subscription renewal reminder.
        """
        # This would fetch subscription details and send reminder
        notification_data = {
            'type': 'subscription_reminder',
            'subscription_id': subscription_id,
            'days_until_renewal': days_until_renewal,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        await self._store_notification(notification_data)
        return notification_data
    
    def _calculate_estimated_delivery(self, order: Order) -> str:
        """Calculate estimated delivery date based on order details."""
        base_days = 3  # Standard delivery days
        
        # Add extra days for specific conditions
        if any(item.product.product_type in ['physical_downloadable', 'service'] 
               for item in order.items):
            base_days += 2
        
        estimated_date = datetime.utcnow() + timedelta(days=base_days)
        return estimated_date.strftime("%Y-%m-%d")
    
    def _get_tracking_info(self, order: Order) -> Optional[Dict[str, str]]:
        """Get tracking information for order."""
        # In production, this would integrate with shipping providers
        return {
            'tracking_number': f"TRK{order.id:08d}",
            'carrier': 'Standard Shipping',
            'estimated_delivery': self._calculate_estimated_delivery(order)
        }
    
    async def _store_notification(self, notification_data: Dict[str, Any]) -> None:
        """
        Store notification in database for audit trail.
        In production, this would also trigger real sending.
        """
        # This would insert into a notifications table
        # For demonstration, we'll just log the notification
        print(f"Notification sent: {json.dumps(notification_data, indent=2)}")


class InventoryAlertService:
    """Service for inventory-related alerts and automation."""
    
    def __init__(self, db: Session):
        self.db = db
        self.notification_service = NotificationService(db)
    
    async def check_and_alert_low_stock(self) -> List[Dict[str, Any]]:
        """
        Check for low stock products and send alerts.
        """
        from app.crud.ecommerce import InventoryCRUD
        
        inventory_crud = InventoryCRUD(db=self.db)
        low_stock_products = inventory_crud.get_low_stock_report()
        
        alerts_sent = []
        for product_data in low_stock_products:
            alert_result = await self.notification_service.send_low_stock_alert(
                product_data['product_id']
            )
            alerts_sent.append(alert_result)
        
        return alerts_sent
    
    async def auto_reorder_suggestions(self) -> List[Dict[str, Any]]:
        """
        Generate automatic reorder suggestions based on historical data.
        """
        from app.crud.ecommerce import InventoryCRUD
        
        inventory_crud = InventoryCRUD(db=self.db)
        suggestions = inventory_crud.get_reorder_suggestions()
        
        return [
            {
                'product_id': s['product_id'],
                'product_name': s['product_name'],
                'current_stock': s['current_stock'],
                'suggested_reorder_quantity': s['suggested_quantity'],
                'reason': s['reason'],
                'priority': s['priority']
            }
            for s in suggestions
        ]


class PaymentWebhookService:
    """Service for handling payment provider webhooks."""
    
    def __init__(self, db: Session):
        self.db = db
        self.notification_service = NotificationService(db)
    
    async def handle_payment_success(self, payment_id: int, gateway_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle successful payment webhook.
        """
        from app.crud.ecommerce import PaymentCRUD, OrderCRUD
        
        payment_crud = PaymentCRUD(db=self.db)
        order_crud = OrderCRUD(db=self.db)
        
        try:
            # Update payment status
            payment = payment_crud.update_payment_status(
                payment_id=payment_id,
                new_status='completed',
                gateway_transaction_id=gateway_data.get('transaction_id'),
                gateway_response=gateway_data
            )
            
            # Update order status
            order = order_crud.update_order_status(
                order_id=payment.order_id,
                new_status='confirmed'
            )
            
            # Send confirmation
            await self.notification_service.send_order_confirmation(payment.order_id)
            
            self.db.commit()
            
            return {
                'success': True,
                'payment_id': payment_id,
                'order_id': payment.order_id,
                'new_order_status': 'confirmed'
            }
            
        except Exception as e:
            self.db.rollback()
            raise ValueError(f"Payment webhook processing failed: {str(e)}")
    
    async def handle_payment_failure(self, payment_id: int, failure_reason: str) -> Dict[str, Any]:
        """
        Handle failed payment webhook.
        """
        from app.crud.ecommerce import PaymentCRUD, OrderCRUD
        
        payment_crud = PaymentCRUD(db=self.db)
        order_crud = OrderCRUD(db=self.db)
        
        try:
            # Update payment status
            payment = payment_crud.update_payment_status(
                payment_id=payment_id,
                new_status='failed',
                failure_reason=failure_reason
            )
            
            # Update order status
            order = order_crud.update_order_status(
                order_id=payment.order_id,
                new_status='payment_failed'
            )
            
            self.db.commit()
            
            return {
                'success': True,
                'payment_id': payment_id,
                'order_id': payment.order_id,
                'new_order_status': 'payment_failed'
            }
            
        except Exception as e:
            self.db.rollback()
            raise ValueError(f"Payment failure webhook processing failed: {str(e)}")
```

**app/api/v1/webhooks.py** - Webhook Endpoints:

```python
"""
Webhook endpoints for external service integrations.
Handles payment provider webhooks and other third-party notifications.
"""

from typing import Dict, Any
from fastapi import APIRouter, Depends, HTTPException, status, Request, Header
from sqlalchemy.orm import Session

from app.database import get_db
from app.services.notifications import PaymentWebhookService
from app.schemas.common import APIResponse

router = APIRouter(prefix="/webhooks", tags=["webhooks"])


@router.post("/payment/stripe", response_model=APIResponse[Dict[str, Any]])
async def handle_stripe_webhook(
    request: Request,
    db: Session = Depends(get_db),
    stripe_signature: str = Header(None, alias="stripe-signature")
):
    """
    Handle Stripe payment webhooks.
    """
    try:
        # Get raw body for signature verification
        body = await request.body()
        
        # In production, verify Stripe signature here
        # stripe.Webhook.construct_event(body, stripe_signature, webhook_secret)
        
        # Parse webhook data
        webhook_data = await request.json()
        event_type = webhook_data.get('type')
        payment_intent = webhook_data.get('data', {}).get('object', {})
        
        webhook_service = PaymentWebhookService(db)
        
        if event_type == 'payment_intent.succeeded':
            # Extract payment ID from metadata
            payment_id = payment_intent.get('metadata', {}).get('payment_id')
            if not payment_id:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Payment ID not found in webhook data"
                )
            
            result = await webhook_service.handle_payment_success(
                payment_id=int(payment_id),
                gateway_data=payment_intent
            )
            
        elif event_type == 'payment_intent.payment_failed':
            payment_id = payment_intent.get('metadata', {}).get('payment_id')
            failure_reason = payment_intent.get('last_payment_error', {}).get('message', 'Unknown error')
            
            result = await webhook_service.handle_payment_failure(
                payment_id=int(payment_id),
                failure_reason=failure_reason
            )
            
        else:
            # Log unhandled webhook type
            result = {'message': f'Unhandled webhook type: {event_type}'}
        
        return APIResponse(
            success=True,
            data=result,
            message="Webhook processed successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Webhook processing failed: {str(e)}"
        )


@router.post("/payment/paypal", response_model=APIResponse[Dict[str, Any]])
async def handle_paypal_webhook(
    request: Request,
    db: Session = Depends(get_db)
):
    """
    Handle PayPal payment webhooks.
    """
    try:
        webhook_data = await request.json()
        event_type = webhook_data.get('event_type')
        resource = webhook_data.get('resource', {})
        
        webhook_service = PaymentWebhookService(db)
        
        if event_type == 'PAYMENT.CAPTURE.COMPLETED':
            payment_id = resource.get('custom_id')  # Our payment ID stored in custom_id
            
            result = await webhook_service.handle_payment_success(
                payment_id=int(payment_id),
                gateway_data=resource
            )
            
        elif event_type == 'PAYMENT.CAPTURE.DENIED':
            payment_id = resource.get('custom_id')
            failure_reason = resource.get('status_details', {}).get('reason', 'Payment denied')
            
            result = await webhook_service.handle_payment_failure(
                payment_id=int(payment_id),
                failure_reason=failure_reason
            )
            
        else:
            result = {'message': f'Unhandled webhook type: {event_type}'}
        
        return APIResponse(
            success=True,
            data=result,
            message="PayPal webhook processed successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"PayPal webhook processing failed: {str(e)}"
        )


@router.post("/inventory/restock", response_model=APIResponse[Dict[str, Any]])
async def handle_inventory_restock_webhook(
    request: Request,
    db: Session = Depends(get_db),
    api_key: str = Header(..., description="API key for authentication")
):
    """
    Handle inventory restock notifications from suppliers.
    """
    # Verify API key (in production, use proper API key validation)
    if api_key != "supplier-webhook-secret":
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API key"
        )
    
    try:
        restock_data = await request.json()
        
        from app.services.inventory import InventoryManager
        inventory_manager = InventoryManager(db)
        
        # Process restock for each product
        results = []
        for item in restock_data.get('items', []):
            product_sku = item.get('sku')
            quantity_added = item.get('quantity')
            
            if not product_sku or not quantity_added:
                continue
            
            # Find product by SKU
            stmt = select(Product).where(Product.sku == product_sku)
            product = db.scalar(stmt)
            
            if product:
                success = inventory_manager.adjust_inventory(
                    product_id=product.id,
                    quantity_change=quantity_added,
                    reason=f"Supplier restock - {restock_data.get('supplier_name', 'Unknown')}"
                )
                
                results.append({
                    'sku': product_sku,
                    'product_name': product.name,
                    'quantity_added': quantity_added,
                    'new_stock_level': product.stock_quantity + quantity_added,
                    'success': success
                })
        
        db.commit()
        
        return APIResponse(
            success=True,
            data={
                'processed_items': results,
                'supplier': restock_data.get('supplier_name'),
                'processed_at': datetime.utcnow().isoformat()
            },
            message="Inventory restock processed successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Restock webhook processing failed: {str(e)}"
        )
    
    def _calculate_estimated_delivery(self, order: Order) -> str:
        """Calculate estimated delivery date."""
        base_days = 3
        estimated_date = datetime.utcnow() + timedelta(days=base_days)
        return estimated_date.strftime("%Y-%m-%d")
    
    def _get_tracking_info(self, order: Order) -> Dict[str, str]:
        """Get tracking information for order."""
        return {
            'tracking_number': f"TRK{order.id:08d}",
            'carrier': 'Standard Shipping',
            'tracking_url': f"https://tracking.example.com/TRK{order.id:08d}"
        }
    
    async def _store_notification(self, notification_data: Dict[str, Any]) -> None:
        """Store notification for audit trail."""
        # In production, store in notifications table
        print(f"Notification: {json.dumps(notification_data, indent=2)}")
```

**app/tasks/background_tasks.py** - Background Task Processing:

```python
"""
Background task processing for e-commerce operations.
Handles automated tasks like inventory monitoring, subscription billing, and analytics updates.
"""

import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any

from sqlalchemy.orm import Session
from sqlalchemy import select, and_

from app.database import get_db
from app.services.notifications import NotificationService, InventoryAlertService
from app.services.inventory import InventoryManager
from app.models.ecommerce import Product, SubscriptionBilling
from app.crud.ecommerce import SubscriptionCRUD, InventoryCRUD


class BackgroundTaskProcessor:
    """Processor for background tasks."""
    
    def __init__(self, db: Session):
        self.db = db
        self.notification_service = NotificationService(db)
        self.inventory_alert_service = InventoryAlertService(db)
    
    async def process_subscription_billing(self) -> Dict[str, Any]:
        """
        Process subscription billing for due subscriptions.
        """
        subscription_crud = SubscriptionCRUD(self.db)
        
        # Get subscriptions due for billing
        due_subscriptions = subscription_crud.get_subscriptions_due_for_billing()
        
        processed_count = 0
        failed_count = 0
        results = []
        
        for subscription in due_subscriptions:
            try:
                billing_result = subscription_crud.process_subscription_billing(
                    subscription_id=subscription.id
                )
                
                if billing_result['success']:
                    processed_count += 1
                    results.append({
                        'subscription_id': subscription.id,
                        'user_email': subscription.user.email,
                        'amount': billing_result['amount'],
                        'status': 'success'
                    })
                else:
                    failed_count += 1
                    results.append({
                        'subscription_id': subscription.id,
                        'user_email': subscription.user.email,
                        'status': 'failed',
                        'error': billing_result.get('error', 'Unknown error')
                    })
                    
            except Exception as e:
                failed_count += 1
                results.append({
                    'subscription_id': subscription.id,
                    'status': 'failed',
                    'error': str(e)
                })
        
        self.db.commit()
        
        return {
            'total_processed': len(due_subscriptions),
            'successful': processed_count,
            'failed': failed_count,
            'results': results,
            'processed_at': datetime.utcnow().isoformat()
        }
    
    async def inventory_monitoring_task(self) -> Dict[str, Any]:
        """
        Monitor inventory levels and send alerts.
        """
        alerts_sent = await self.inventory_alert_service.check_and_alert_low_stock()
        reorder_suggestions = await self.inventory_alert_service.auto_reorder_suggestions()
        
        return {
            'low_stock_alerts_sent': len(alerts_sent),
            'reorder_suggestions_generated': len(reorder_suggestions),
            'alerts': alerts_sent,
            'suggestions': reorder_suggestions,
            'processed_at': datetime.utcnow().isoformat()
        }
    
    async def cleanup_expired_carts(self) -> Dict[str, Any]:
        """
        Clean up expired shopping carts.
        """
        from app.crud.ecommerce import CartCRUD
        
        cart_crud = CartCRUD(self.db)
        
        # Get carts older than 7 days
        cutoff_date = datetime.utcnow() - timedelta(days=7)
        expired_carts = cart_crud.get_expired_carts(cutoff_date)
        
        cleanup_count = 0
        for cart in expired_carts:
            cart_crud.clear_cart(cart.id)
            cleanup_count += 1
        
        self.db.commit()
        
        return {
            'expired_carts_cleaned': cleanup_count,
            'cutoff_date': cutoff_date.isoformat(),
            'processed_at': datetime.utcnow().isoformat()
        }
    
    async def update_product_analytics(self) -> Dict[str, Any]:
        """
        Update product analytics and performance metrics.
        """
        from app.db.query_patterns import EcommerceAnalytics
        
        analytics = EcommerceAnalytics(self.db)
        
        # Update various analytics metrics
        performance_matrix = analytics.get_product_performance_matrix()
        sales_forecasts = analytics.generate_sales_forecast()
        
        # In production, these would be stored in analytics tables
        # For demonstration, we'll return the data
        
        return {
            'products_analyzed': len(performance_matrix),
            'sales_forecasts_generated': len(sales_forecasts),
            'processed_at': datetime.utcnow().isoformat()
        }


# Background Task Endpoints (for admin monitoring)
async def run_background_task_endpoint(
    task_name: str,
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """
    Manually trigger background tasks (admin only).
    """
    processor = BackgroundTaskProcessor(db)
    
    task_functions = {
        'subscription_billing': processor.process_subscription_billing,
        'inventory_monitoring': processor.inventory_monitoring_task,
        'cleanup_carts': processor.cleanup_expired_carts,
        'update_analytics': processor.update_product_analytics
    }
    
    if task_name not in task_functions:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unknown task: {task_name}"
        )
    
    try:
        result = await task_functions[task_name]()
        
        return APIResponse(
            success=True,
            data=result,
            message=f"Background task '{task_name}' completed successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Background task failed: {str(e)}"
        )
```

### Section 4.6: Integration with Existing Database Tables

One of the most common real-world scenarios is working with existing database tables rather than creating everything from scratch.

**app/models/legacy.py** - Working with Legacy Tables:

```python
"""
Demonstrate working with existing/legacy database tables.
Shows how to map SQLAlchemy models to existing table structures.
"""

from sqlalchemy import Column, Integer, String, DateTime, Decimal, Boolean, ForeignKey, Text
from sqlalchemy.orm import relationship, declarative_base
from sqlalchemy.ext.declarative import declared_attr

from app.database import Base

# Existing tables that we need to work with
class LegacyCustomer(Base):
    """
    Map to existing customers table with different naming convention.
    """
    __tablename__ = "customers"  # Existing table name
    
    # Map to existing column names
    customer_id = Column(Integer, primary_key=True, index=True)
    first_name = Column(String(50), nullable=False)
    last_name = Column(String(50), nullable=False) 
    email_address = Column(String(255), unique=True, nullable=False)
    phone_number = Column(String(20))
    date_created = Column(DateTime, nullable=False)
    date_modified = Column(DateTime)
    is_active_flag = Column(Boolean, default=True)
    
    # Legacy relationships
    orders = relationship("LegacyOrder", back_populates="customer")
    
    @property
    def full_name(self):
        """Computed property for full name."""
        return f"{self.first_name} {self.last_name}"


class LegacyOrder(Base):
    """
    Map to existing orders table with legacy structure.
    """
    __tablename__ = "orders"  # Existing table name
    
    order_id = Column(Integer, primary_key=True, index=True)
    customer_id = Column(Integer, ForeignKey("customers.customer_id"), nullable=False)
    order_date = Column(DateTime, nullable=False)
    order_status = Column(String(20), default="pending")
    order_total = Column(Decimal(12, 2), nullable=False)
    shipping_cost = Column(Decimal(8, 2), default=0)
    tax_amount = Column(Decimal(8, 2), default=0)
    
    # Separate address fields (not normalized)
    shipping_first_name = Column(String(50))
    shipping_last_name = Column(String(50))
    shipping_address_1 = Column(String(200))
    shipping_address_2 = Column(String(200))
    shipping_city = Column(String(100))
    shipping_state = Column(String(50))
    shipping_zip = Column(String(20))
    shipping_country = Column(String(50))
    
    billing_first_name = Column(String(50))
    billing_last_name = Column(String(50))
    billing_address_1 = Column(String(200))
    billing_address_2 = Column(String(200))
    billing_city = Column(String(100))
    billing_state = Column(String(50))
    billing_zip = Column(String(20))
    billing_country = Column(String(50))
    
    # Legacy relationships
    customer = relationship("LegacyCustomer", back_populates="orders")
    order_items = relationship("LegacyOrderItem", back_populates="order")
    
    @property
    def shipping_address(self):
        """Convert legacy address fields to dictionary."""
        return {
            'first_name': self.shipping_first_name,
            'last_name': self.shipping_last_name,
            'address_line_1': self.shipping_address_1,
            'address_line_2': self.shipping_address_2,
            'city': self.shipping_city,
            'state': self.shipping_state,
            'postal_code': self.shipping_zip,
            'country': self.shipping_country
        }


class LegacyOrderItem(Base):
    """
    Map to existing order_items table.
    """
    __tablename__ = "order_items"
    
    order_item_id = Column(Integer, primary_key=True, index=True)
    order_id = Column(Integer, ForeignKey("orders.order_id"), nullable=False)
    product_id = Column(Integer, ForeignKey("legacy_products.product_id"), nullable=False)
    quantity = Column(Integer, nullable=False)
    unit_price = Column(Decimal(10, 2), nullable=False)
    total_price = Column(Decimal(12, 2), nullable=False)
    
    # Legacy relationships
    order = relationship("LegacyOrder", back_populates="order_items")
    product = relationship("LegacyProduct", back_populates="order_items")


class LegacyProduct(Base):
    """
    Map to existing products table with legacy structure.
    """
    __tablename__ = "legacy_products"
    
    product_id = Column(Integer, primary_key=True, index=True)
    product_name = Column(String(200), nullable=False)
    product_code = Column(String(50), unique=True, nullable=False)
    product_description = Column(Text)
    unit_price = Column(Decimal(10, 2), nullable=False)
    units_in_stock = Column(Integer, default=0)
    reorder_level = Column(Integer, default=0)
    discontinued = Column(Boolean, default=False)
    category_id = Column(Integer, ForeignKey("categories.category_id"))
    supplier_id = Column(Integer, ForeignKey("suppliers.supplier_id"))
    
    # Legacy relationships
    category = relationship("LegacyCategory", back_populates="products")
    supplier = relationship("LegacySupplier", back_populates="products")
    order_items = relationship("LegacyOrderItem", back_populates="product")


class LegacyCategory(Base):
    """Map to existing categories table."""
    __tablename__ = "categories"
    
    category_id = Column(Integer, primary_key=True, index=True)
    category_name = Column(String(100), nullable=False)
    description = Column(Text)
    
    # Legacy relationships
    products = relationship("LegacyProduct", back_populates="category")


class LegacySupplier(Base):
    """Map to existing suppliers table."""
    __tablename__ = "suppliers"
    
    supplier_id = Column(Integer, primary_key=True, index=True)
    company_name = Column(String(200), nullable=False)
    contact_name = Column(String(100))
    contact_email = Column(String(255))
    phone_number = Column(String(20))
    address = Column(String(500))
    
    # Legacy relationships
    products = relationship("LegacyProduct", back_populates="supplier")
```

**app/crud/legacy.py** - CRUD Operations for Legacy Tables:

```python
"""
CRUD operations for legacy database tables.
Demonstrates working with existing table structures and data migration patterns.
"""

from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from decimal import Decimal

from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy import select, and_, or_, func, desc, asc, text

from app.models.legacy import (
    LegacyCustomer, LegacyOrder, LegacyOrderItem, 
    LegacyProduct, LegacyCategory, LegacySupplier
)
from app.crud.base import BaseCRUD


class LegacyCustomerCRUD(BaseCRUD[LegacyCustomer]):
    """CRUD operations for legacy customers table."""
    
    def __init__(self, db: Session):
        super().__init__(LegacyCustomer, db)
    
    def get_by_email(self, email: str) -> Optional[LegacyCustomer]:
        """Get customer by email address."""
        stmt = select(LegacyCustomer).where(LegacyCustomer.email_address == email)
        return self.db.scalar(stmt)
    
    def get_active_customers(self, limit: Optional[int] = None) -> List[LegacyCustomer]:
        """Get active customers with optional limit."""
        stmt = select(LegacyCustomer).where(LegacyCustomer.is_active_flag == True)
        
        if limit:
            stmt = stmt.limit(limit)
            
        return list(self.db.scalars(stmt))
    
    def get_customers_with_orders(self) -> List[LegacyCustomer]:
        """Get customers with their order information loaded."""
        stmt = (
            select(LegacyCustomer)
            .options(selectinload(LegacyCustomer.orders))
            .where(LegacyCustomer.is_active_flag == True)
        )
        return list(self.db.scalars(stmt))
    
    def get_customer_order_summary(self, customer_id: int) -> Dict[str, Any]:
        """Get order summary for a specific customer."""
        stmt = (
            select(
                func.count(LegacyOrder.order_id).label('total_orders'),
                func.sum(LegacyOrder.order_total).label('total_spent'),
                func.avg(LegacyOrder.order_total).label('average_order_value'),
                func.max(LegacyOrder.order_date).label('last_order_date')
            )
            .where(LegacyOrder.customer_id == customer_id)
        )
        
        result = self.db.execute(stmt).first()
        
        return {
            'customer_id': customer_id,
            'total_orders': result.total_orders or 0,
            'total_spent': float(result.total_spent or 0),
            'average_order_value': float(result.average_order_value or 0),
            'last_order_date': result.last_order_date.isoformat() if result.last_order_date else None
        }


class LegacyOrderCRUD(BaseCRUD[LegacyOrder]):
    """CRUD operations for legacy orders table."""
    
    def __init__(self, db: Session):
        super().__init__(LegacyOrder, db)
    
    def get_orders_with_items(self, customer_id: Optional[int] = None) -> List[LegacyOrder]:
        """Get orders with their items loaded."""
        stmt = (
            select(LegacyOrder)
            .options(
                selectinload(LegacyOrder.order_items)
                .selectinload(LegacyOrderItem.product),
                joinedload(LegacyOrder.customer)
            )
            .order_by(desc(LegacyOrder.order_date))
        )
        
        if customer_id:
            stmt = stmt.where(LegacyOrder.customer_id == customer_id)
            
        return list(self.db.scalars(stmt))
    
    def get_orders_by_date_range(
        self, 
        start_date: datetime, 
        end_date: datetime
    ) -> List[LegacyOrder]:
        """Get orders within date range."""
        stmt = (
            select(LegacyOrder)
            .where(
                and_(
                    LegacyOrder.order_date >= start_date,
                    LegacyOrder.order_date <= end_date
                )
            )
            .order_by(desc(LegacyOrder.order_date))
        )
        return list(self.db.scalars(stmt))
    
    def get_sales_summary_by_period(
        self, 
        start_date: datetime, 
        end_date: datetime
    ) -> Dict[str, Any]:
        """Get sales summary for a specific period."""
        stmt = (
            select(
                func.count(LegacyOrder.order_id).label('total_orders'),
                func.sum(LegacyOrder.order_total).label('total_revenue'),
                func.avg(LegacyOrder.order_total).label('average_order_value'),
                func.count(func.distinct(LegacyOrder.customer_id)).label('unique_customers')
            )
            .where(
                and_(
                    LegacyOrder.order_date >= start_date,
                    LegacyOrder.order_date <= end_date,
                    LegacyOrder.order_status.in_(['completed', 'shipped'])
                )
            )
        )
        
        result = self.db.execute(stmt).first()
        
        return {
            'period_start': start_date.isoformat(),
            'period_end': end_date.isoformat(),
            'total_orders': result.total_orders or 0,
            'total_revenue': float(result.total_revenue or 0),
            'average_order_value': float(result.average_order_value or 0),
            'unique_customers': result.unique_customers or 0
        }


class LegacyProductCRUD(BaseCRUD[LegacyProduct]):
    """CRUD operations for legacy products table."""
    
    def __init__(self, db: Session):
        super().__init__(LegacyProduct, db)
    
    def get_by_product_code(self, product_code: str) -> Optional[LegacyProduct]:
        """Get product by legacy product code."""
        stmt = select(LegacyProduct).where(LegacyProduct.product_code == product_code)
        return self.db.scalar(stmt)
    
    def get_products_with_details(self) -> List[LegacyProduct]:
        """Get products with category and supplier information."""
        stmt = (
            select(LegacyProduct)
            .options(
                joinedload(LegacyProduct.category),
                joinedload(LegacyProduct.supplier)
            )
            .where(LegacyProduct.discontinued == False)
            .order_by(LegacyProduct.product_name)
        )
        return list(self.db.scalars(stmt))
    
    def get_low_stock_products(self) -> List[LegacyProduct]:
        """Get products below reorder level."""
        stmt = (
            select(LegacyProduct)
            .where(
                and_(
                    LegacyProduct.units_in_stock <= LegacyProduct.reorder_level,
                    LegacyProduct.discontinued == False,
                    LegacyProduct.reorder_level > 0
                )
            )
            .order_by(asc(LegacyProduct.units_in_stock))
        )
        return list(self.db.scalars(stmt))
    
    def get_product_sales_summary(self, product_id: int, days: int = 30) -> Dict[str, Any]:
        """Get sales summary for a specific product."""
        start_date = datetime.utcnow() - timedelta(days=days)
        
        stmt = (
            select(
                func.sum(LegacyOrderItem.quantity).label('total_quantity_sold'),
                func.sum(LegacyOrderItem.total_price).label('total_revenue'),
                func.count(func.distinct(LegacyOrder.customer_id)).label('unique_buyers'),
                func.count(LegacyOrderItem.order_item_id).label('total_transactions')
            )
            .select_from(LegacyOrderItem)
            .join(LegacyOrder, LegacyOrderItem.order_id == LegacyOrder.order_id)
            .where(
                and_(
                    LegacyOrderItem.product_id == product_id,
                    LegacyOrder.order_date >= start_date,
                    LegacyOrder.order_status.in_(['completed', 'shipped'])
                )
            )
        )
        
        result = self.db.execute(stmt).first()
        
        return {
            'product_id': product_id,
            'period_days': days,
            'total_quantity_sold': result.total_quantity_sold or 0,
            'total_revenue': float(result.total_revenue or 0),
            'unique_buyers': result.unique_buyers or 0,
            'total_transactions': result.total_transactions or 0
        }


class LegacyMigrationService:
    """Service for migrating data between legacy and modern tables."""
    
    def __init__(self, db: Session):
        self.db = db
        self.legacy_customer_crud = LegacyCustomerCRUD(db)
        self.legacy_order_crud = LegacyOrderCRUD(db)
        self.legacy_product_crud = LegacyProductCRUD(db)
    
    def migrate_customers_to_modern_format(self, batch_size: int = 100) -> Dict[str, Any]:
        """
        Migrate customers from legacy table to modern User table format.
        """
        from app.models.user import User
        from app.crud.user import UserCRUD
        
        user_crud = UserCRUD(self.db)
        
        # Get legacy customers that haven't been migrated
        legacy_customers = self.legacy_customer_crud.get_active_customers(limit=batch_size)
        
        migrated_count = 0
        skipped_count = 0
        errors = []
        
        for legacy_customer in legacy_customers:
            try:
                # Check if customer already exists in modern table
                existing_user = user_crud.get_by_email(legacy_customer.email_address)
                if existing_user:
                    skipped_count += 1
                    continue
                
                # Create modern user from legacy customer
                user_data = {
                    'email': legacy_customer.email_address,
                    'first_name': legacy_customer.first_name,
                    'last_name': legacy_customer.last_name,
                    'phone': legacy_customer.phone_number,
                    'is_active': legacy_customer.is_active_flag,
                    'password': 'temporary_password_reset_required',  # Force password reset
                    'migration_source': 'legacy_customers',
                    'legacy_customer_id': legacy_customer.customer_id
                }
                
                new_user = user_crud.create(obj_in=user_data)
                migrated_count += 1
                
            except Exception as e:
                errors.append({
                    'customer_id': legacy_customer.customer_id,
                    'email': legacy_customer.email_address,
                    'error': str(e)
                })
        
        self.db.commit()
        
        return {
            'batch_size': batch_size,
            'migrated_customers': migrated_count,
            'skipped_existing': skipped_count,
            'errors': len(errors),
            'error_details': errors,
            'migration_timestamp': datetime.utcnow().isoformat()
        }
    
    def sync_legacy_inventory_with_modern(self) -> Dict[str, Any]:
        """
        Synchronize inventory between legacy and modern product tables.
        """
        from app.models.ecommerce import Product
        from app.crud.ecommerce import ProductCRUD
        
        product_crud = ProductCRUD(self.db)
        
        # Get all legacy products
        legacy_products = self.legacy_product_crud.get_products_with_details()
        
        synced_count = 0
        created_count = 0
        errors = []
        
        for legacy_product in legacy_products:
            try:
                # Try to find matching modern product by SKU/product_code
                modern_product = product_crud.get_by_sku(legacy_product.product_code)
                
                if modern_product:
                    # Update existing product inventory
                    product_crud.update_stock_quantity(
                        product_id=modern_product.id,
                        new_quantity=legacy_product.units_in_stock
                    )
                    synced_count += 1
                else:
                    # Create new product from legacy data
                    product_data = {
                        'name': legacy_product.product_name,
                        'slug': self._generate_slug(legacy_product.product_name),
                        'sku': legacy_product.product_code,
                        'description': legacy_product.product_description,
                        'sale_price': legacy_product.unit_price,
                        'stock_quantity': legacy_product.units_in_stock,
                        'low_stock_threshold': legacy_product.reorder_level,
                        'is_active': not legacy_product.discontinued,
                        'migration_source': 'legacy_products',
                        'legacy_product_id': legacy_product.product_id
                    }
                    
                    new_product = product_crud.create(obj_in=product_data)
                    created_count += 1
                    
            except Exception as e:
                errors.append({
                    'legacy_product_id': legacy_product.product_id,
                    'product_code': legacy_product.product_code,
                    'error': str(e)
                })
        
        self.db.commit()
        
        return {
            'total_legacy_products': len(legacy_products),
            'synced_existing': synced_count,
            'created_new': created_count,
            'errors': len(errors),
            'error_details': errors,
            'sync_timestamp': datetime.utcnow().isoformat()
        }
    
    def generate_legacy_sales_report(
        self, 
        start_date: datetime, 
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Generate comprehensive sales report from legacy data.
        """
        # Get order summary
        order_summary = self.legacy_order_crud.get_sales_summary_by_period(
            start_date, end_date
        )
        
        # Get top products
        top_products_stmt = (
            select(
                LegacyProduct.product_id,
                LegacyProduct.product_name,
                func.sum(LegacyOrderItem.quantity).label('total_sold'),
                func.sum(LegacyOrderItem.total_price).label('revenue')
            )
            .select_from(LegacyOrderItem)
            .join(LegacyOrder, LegacyOrderItem.order_id == LegacyOrder.order_id)
            .join(LegacyProduct, LegacyOrderItem.product_id == LegacyProduct.product_id)
            .where(
                and_(
                    LegacyOrder.order_date >= start_date,
                    LegacyOrder.order_date <= end_date,
                    LegacyOrder.order_status.in_(['completed', 'shipped'])
                )
            )
            .group_by(LegacyProduct.product_id, LegacyProduct.product_name)
            .order_by(desc(func.sum(LegacyOrderItem.total_price)))
            .limit(10)
        )
        
        top_products = [
            {
                'product_id': row.product_id,
                'product_name': row.product_name,
                'total_sold': row.total_sold,
                'revenue': float(row.revenue)
            }
            for row in self.db.execute(top_products_stmt)
        ]
        
        # Get category performance
        category_performance_stmt = (
            select(
                LegacyCategory.category_id,
                LegacyCategory.category_name,
                func.sum(LegacyOrderItem.total_price).label('revenue'),
                func.sum(LegacyOrderItem.quantity).label('units_sold')
            )
            .select_from(LegacyOrderItem)
            .join(LegacyOrder, LegacyOrderItem.order_id == LegacyOrder.order_id)
            .join(LegacyProduct, LegacyOrderItem.product_id == LegacyProduct.product_id)
            .join(LegacyCategory, LegacyProduct.category_id == LegacyCategory.category_id)
            .where(
                and_(
                    LegacyOrder.order_date >= start_date,
                    LegacyOrder.order_date <= end_date,
                    LegacyOrder.order_status.in_(['completed', 'shipped'])
                )
            )
            .group_by(LegacyCategory.category_id, LegacyCategory.category_name)
            .order_by(desc(func.sum(LegacyOrderItem.total_price)))
        )
        
        category_performance = [
            {
                'category_id': row.category_id,
                'category_name': row.category_name,
                'revenue': float(row.revenue),
                'units_sold': row.units_sold
            }
            for row in self.db.execute(category_performance_stmt)
        ]
        
        return {
            'period': order_summary,
            'top_products': top_products,
            'category_performance': category_performance,
            'generated_at': datetime.utcnow().isoformat()
        }
    
    def _generate_slug(self, name: str) -> str:
        """Generate URL-friendly slug from product name."""
        import re
        slug = re.sub(r'[^a-zA-Z0-9\s-]', '', name.lower())
        slug = re.sub(r'[\s-]+', '-', slug)
        return slug.strip('-')


# Data Migration Utilities
class DataMigrationUtilities:
    """Utilities for data migration and legacy system integration."""
    
    def __init__(self, db: Session):
        self.db = db
        self.migration_service = LegacyMigrationService(db)
    
    def create_migration_tracking_table(self):
        """
        Create table to track migration status.
        """
        migration_sql = """
        CREATE TABLE IF NOT EXISTS migration_log (
            id SERIAL PRIMARY KEY,
            migration_name VARCHAR(100) NOT NULL,
            source_table VARCHAR(100) NOT NULL,
            target_table VARCHAR(100) NOT NULL,
            records_processed INTEGER DEFAULT 0,
            records_successful INTEGER DEFAULT 0,
            records_failed INTEGER DEFAULT 0,
            started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            completed_at TIMESTAMP,
            status VARCHAR(20) DEFAULT 'pending',
            error_details JSONB,
            created_by VARCHAR(100)
        );
        
        CREATE INDEX IF NOT EXISTS idx_migration_log_name 
        ON migration_log(migration_name);
        
        CREATE INDEX IF NOT EXISTS idx_migration_log_status 
        ON migration_log(status);
        """
        
        self.db.execute(text(migration_sql))
        self.db.commit()
    
    def log_migration_start(
        self, 
        migration_name: str, 
        source_table: str, 
        target_table: str
    ) -> int:
        """Log the start of a migration process."""
        insert_sql = text("""
            INSERT INTO migration_log 
            (migration_name, source_table, target_table, status)
            VALUES (:migration_name, :source_table, :target_table, 'in_progress')
            RETURNING id
        """)
        
        result = self.db.execute(insert_sql, {
            'migration_name': migration_name,
            'source_table': source_table,
            'target_table': target_table
        })
        
        migration_id = result.scalar()
        self.db.commit()
        return migration_id
    
    def log_migration_completion(
        self, 
        migration_id: int, 
        records_processed: int, 
        records_successful: int, 
        records_failed: int,
        error_details: Optional[Dict[str, Any]] = None
    ):
        """Log the completion of a migration process."""
        update_sql = text("""
            UPDATE migration_log 
            SET 
                records_processed = :processed,
                records_successful = :successful,
                records_failed = :failed,
                completed_at = CURRENT_TIMESTAMP,
                status = :status,
                error_details = :error_details
            WHERE id = :migration_id
        """)
        
        status = 'completed' if records_failed == 0 else 'completed_with_errors'
        
        self.db.execute(update_sql, {
            'migration_id': migration_id,
            'processed': records_processed,
            'successful': records_successful,
            'failed': records_failed,
            'status': status,
            'error_details': error_details
        })
        
        self.db.commit()
    
    def get_migration_history(self) -> List[Dict[str, Any]]:
        """Get migration history from log table."""
        select_sql = text("""
            SELECT 
                migration_name,
                source_table,
                target_table,
                records_processed,
                records_successful,
                records_failed,
                started_at,
                completed_at,
                status
            FROM migration_log
            ORDER BY started_at DESC
        """)
        
        result = self.db.execute(select_sql)
        
        return [
            {
                'migration_name': row.migration_name,
                'source_table': row.source_table,
                'target_table': row.target_table,
                'records_processed': row.records_processed,
                'records_successful': row.records_successful,
                'records_failed': row.records_failed,
                'started_at': row.started_at.isoformat() if row.started_at else None,
                'completed_at': row.completed_at.isoformat() if row.completed_at else None,
                'status': row.status
            }
            for row in result
        ]
```

**app/api/v1/legacy.py** - Legacy System Integration API:

```python
"""
API endpoints for legacy system integration and data migration.
Provides endpoints for working with existing database tables.
"""

from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import get_current_admin, get_pagination_params
from app.crud.legacy import (
    LegacyCustomerCRUD, LegacyOrderCRUD, LegacyProductCRUD, 
    LegacyMigrationService, DataMigrationUtilities
)
from app.schemas.common import APIResponse, PaginationParams

router = APIRouter(prefix="/legacy", tags=["legacy"])


@router.get("/customers", response_model=APIResponse[List[Dict[str, Any]]])
async def get_legacy_customers(
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin),
    pagination: PaginationParams = Depends(get_pagination_params)
):
    """
    Get customers from legacy table (admin only).
    """
    customer_crud = LegacyCustomerCRUD(db)
    
    customers = customer_crud.get_active_customers()
    
    # Apply pagination
    total_count = len(customers)
    start_idx = (pagination.page - 1) * pagination.size
    end_idx = start_idx + pagination.size
    customers_page = customers[start_idx:end_idx]
    
    customer_data = []
    for customer in customers_page:
        customer_summary = customer_crud.get_customer_order_summary(customer.customer_id)
        customer_data.append({
            'customer_id': customer.customer_id,
            'full_name': customer.full_name,
            'email': customer.email_address,
            'phone': customer.phone_number,
            'is_active': customer.is_active_flag,
            'date_created': customer.date_created.isoformat(),
            'order_summary': customer_summary
        })
    
    return APIResponse(
        success=True,
        data=customer_data,
        message="Legacy customers retrieved successfully"
    )


@router.get("/products/low-stock", response_model=APIResponse[List[Dict[str, Any]]])
async def get_legacy_low_stock_products(
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """
    Get low stock products from legacy table (admin only).
    """
    product_crud = LegacyProductCRUD(db)
    
    low_stock_products = product_crud.get_low_stock_products()
    
    product_data = []
    for product in low_stock_products:
        sales_summary = product_crud.get_product_sales_summary(product.product_id)
        
        product_data.append({
            'product_id': product.product_id,
            'product_name': product.product_name,
            'product_code': product.product_code,
            'current_stock': product.units_in_stock,
            'reorder_level': product.reorder_level,
            'unit_price': float(product.unit_price),
            'category': product.category.category_name if product.category else None,
            'supplier': product.supplier.company_name if product.supplier else None,
            'sales_summary': sales_summary
        })
    
    return APIResponse(
        success=True,
        data=product_data,
        message="Low stock products retrieved successfully"
    )


@router.get("/reports/sales", response_model=APIResponse[Dict[str, Any]])
async def get_legacy_sales_report(
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None)
):
    """
    Generate sales report from legacy data (admin only).
    """
    migration_service = LegacyMigrationService(db)
    
    # Default to last 30 days if no dates provided
    if not start_date:
        start_date = datetime.utcnow() - timedelta(days=30)
    if not end_date:
        end_date = datetime.utcnow()
    
    sales_report = migration_service.generate_legacy_sales_report(start_date, end_date)
    
    return APIResponse(
        success=True,
        data=sales_report,
        message="Legacy sales report generated successfully"
    )


@router.post("/migrate/customers", response_model=APIResponse[Dict[str, Any]])
async def migrate_customers(
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin),
    batch_size: int = Query(100, ge=1, le=1000, description="Number of customers to migrate")
):
    """
    Migrate customers from legacy to modern table (admin only).
    """
    migration_service = LegacyMigrationService(db)
    
    try:
        migration_result = migration_service.migrate_customers_to_modern_format(batch_size)
        
        return APIResponse(
            success=True,
            data=migration_result,
            message="Customer migration completed"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Migration failed: {str(e)}"
        )


@router.post("/sync/inventory", response_model=APIResponse[Dict[str, Any]])
async def sync_legacy_inventory(
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """
    Synchronize inventory between legacy and modern systems (admin only).
    """
    migration_service = LegacyMigrationService(db)
    
    try:
        sync_result = migration_service.sync_legacy_inventory_with_modern()
        
        return APIResponse(
            success=True,
            data=sync_result,
            message="Inventory synchronization completed"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Inventory sync failed: {str(e)}"
        )


@router.get("/migration/history", response_model=APIResponse[List[Dict[str, Any]]])
async def get_migration_history(
    db: Session = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """
    Get migration history and status (admin only).
    """
    migration_utils = DataMigrationUtilities(db)
    
    try:
        # Ensure migration tracking table exists
        migration_utils.create_migration_tracking_table()
        
        history = migration_utils.get_migration_history()
        
        return APIResponse(
            success=True,
            data=history,
            message="Migration history retrieved successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve migration history: {str(e)}"
        )
```

### Section 4.7: Testing the Complete E-commerce System

**tests/test_ecommerce_integration.py** - Integration Tests:

```python
"""
Integration tests for the complete e-commerce system.
Tests end-to-end workflows and business logic.
"""

import pytest
from decimal import Decimal
from datetime import datetime, timedelta

from fastapi.testclient import TestClient
from sqlalchemy.orm import Session

from app.main import app
from app.database import get_db
from app.models.user import User
from app.models.ecommerce import Product, Cart, Order, OrderStatus
from app.crud.ecommerce import ProductCRUD, CartCRUD, OrderCRUD
from app.services.inventory import InventoryManager, OrderProcessor


@pytest.fixture
def test_client():
    """Create test client with dependency overrides."""
    return TestClient(app)


@pytest.fixture
def test_user(db_session: Session):
    """Create test user."""
    from app.crud.user import UserCRUD
    
    user_crud = UserCRUD(db_session)
    test_user = user_crud.create({
        'email': 'test@example.com',
        'password': 'testpassword123',
        'first_name': 'Test',
        'last_name': 'User'
    })
    db_session.commit()
    return test_user


@pytest.fixture
def test_products(db_session: Session):
    """Create test products."""
    product_crud = ProductCRUD(db_session)
    
    products = []
    for i in range(3):
        product = product_crud.create({
            'name': f'Test Product {i+1}',
            'slug': f'test-product-{i+1}',
            'sku': f'TEST-SKU-{i+1}',
            'description': f'Description for test product {i+1}',
            'sale_price': Decimal(f'{(i+1) * 10}.99'),
            'stock_quantity': 100 + (i * 50),
            'is_active': True
        })
        products.append(product)
    
    db_session.commit()
    return products


class TestEcommerceWorkflow:
    """Test complete e-commerce workflow."""
    
    def test_complete_order_workflow(
        self, 
        test_client: TestClient, 
        test_user: User, 
        test_products: List[Product],
        db_session: Session
    ):
        """Test complete order workflow from cart to payment."""
        
        # 1. Add products to cart
        cart_crud = CartCRUD(db_session)
        cart = cart_crud.get_or_create_cart(test_user.id)
        
        # Add multiple products
        for i, product in enumerate(test_products[:2]):
            cart_crud.add_item_to_cart(
                cart_id=cart.id,
                product_id=product.id,
                quantity=2 + i
            )
        
        db_session.commit()
        
        # 2. Verify cart totals
        cart_with_totals = cart_crud.get_cart_with_totals(cart.id)
        assert len(cart_with_totals.items) == 2
        assert cart_with_totals.subtotal > Decimal('0')
        
        # 3. Create order from cart
        order_processor = OrderProcessor(db_session)
        
        shipping_address = {
            'first_name': 'Test',
            'last_name': 'User',
            'address_line_1': '123 Test St',
            'city': 'Test City',
            'state': 'Test State',
            'postal_code': '12345',
            'country': 'Test Country'
        }
        
        order = order_processor.create_order_from_cart(
            cart=cart_with_totals,
            shipping_address=shipping_address,
            billing_address=shipping_address
        )
        
        db_session.commit()
        
        # 4. Verify order creation
        assert order.status == OrderStatus.PENDING
        assert len(order.items) == 2
        assert order.total_amount > Decimal('0')
        
        # 5. Process payment
        order_crud = OrderCRUD(db_session)
        payment = order_crud.process_order_payment(
            order_id=order.id,
            payment_data={
                'payment_method': 'credit_card',
                'card_number': '4111111111111111',
                'card_holder_name': 'Test User',
                'exp_month': 12,
                'exp_year': 2025,
                'cvv': '123'
            }
        )
        
        db_session.commit()
        
        # 6. Verify payment and order status
        assert payment.status.value == 'completed'
        
        # Refresh order from database
        db_session.refresh(order)
        assert order.status == OrderStatus.CONFIRMED
    
    def test_inventory_management(self, test_products: List[Product], db_session: Session):
        """Test inventory management functionality."""
        
        inventory_manager = InventoryManager(db_session)
        product = test_products[0]
        initial_stock = product.stock_quantity
        
        # Test inventory adjustment
        adjustment_result = inventory_manager.adjust_inventory(
            product_id=product.id,
            quantity_change=-10,
            reason="Test adjustment"
        )
        
        assert adjustment_result == True
        
        # Verify stock changed
        db_session.refresh(product)
        assert product.stock_quantity == initial_stock - 10
        
        # Test inventory reservation
        reservation_result = inventory_manager.reserve_inventory(
            product_id=product.id,
            quantity=5,
            reference_type="order",
            reference_id=1
        )
        
        assert reservation_result == True
        
        # Test inventory release
        release_result = inventory_manager.release_reservation(
            product_id=product.id,
            quantity=5,
            reference_type="order",
            reference_id=1
        )
        
        assert release_result == True
    
    def test_coupon_system(self, test_user: User, db_session: Session):
        """Test coupon creation and validation."""
        from app.crud.ecommerce import CouponCRUD
        
        coupon_crud = CouponCRUD(db_session)
        
        # Create test coupon
        coupon = coupon_crud.create({
            'code': 'TEST10',
            'name': 'Test 10% Off',
            'discount_type': 'percentage',
            'discount_value': Decimal('10'),
            'max_uses': 100,
            'max_uses_per_user': 1,
            'valid_from': datetime.utcnow(),
            'valid_until': datetime.utcnow() + timedelta(days=30),
            'is_active': True
        })
        
        db_session.commit()
        
        # Test coupon validation
        validation_result = coupon_crud.validate_coupon(
            coupon_code='TEST10',
            user_id=test_user.id,
            order_amount=Decimal('100.00')
        )
        
        assert validation_result['valid'] == True
        assert validation_result['discount_amount'] == Decimal('10.00')
    
    def test_analytics_queries(self, test_products: List[Product], db_session: Session):
        """Test analytics and reporting queries."""
        from app.db.query_patterns import EcommerceAnalytics
        
        analytics = EcommerceAnalytics(db_session)
        
        # Test product performance matrix
        performance_matrix = analytics.get_product_performance_matrix()
        assert isinstance(performance_matrix, list)
        
        # Test customer segmentation
        segmentation = analytics.get_customer_segmentation()
        assert isinstance(segmentation, dict)
        
        # Test sales forecasting
        forecast = analytics.generate_sales_forecast()
        assert isinstance(forecast, dict)


class TestLegacyIntegration:
    """Test legacy system integration."""
    
    def test_legacy_data_access(self, db_session: Session):
        """Test accessing legacy table data."""
        from app.models.legacy import LegacyCustomer, LegacyProduct
        
        # This assumes legacy tables exist and have test data
        # In real testing, you'd set up test legacy data
        
        customer_crud = LegacyCustomerCRUD(db_session)
        product_crud = LegacyProductCRUD(db_session)
        
        # Test getting legacy customers
        customers = customer_crud.get_active_customers(limit=5)
        assert isinstance(customers, list)
        
        # Test getting legacy products
        products = product_crud.get_products_with_details()
        assert isinstance(products, list)
    
    def test_data_migration(self, db_session: Session):
        """Test data migration utilities."""
        migration_utils = DataMigrationUtilities(db_session)
        
        # Test migration tracking table creation
        migration_utils.create_migration_tracking_table()
        
        # Test migration logging
        migration_id = migration_utils.log_migration_start(
            migration_name='test_migration',
            source_table='legacy_customers',
            target_table='users'
        )
        
        assert migration_id > 0
        
        # Test completion logging
        migration_utils.log_migration_completion(
            migration_id=migration_id,
            records_processed=10,
            records_successful=9,
            records_failed=1
        )
        
        # Test history retrieval
        history = migration_utils.get_migration_history()
        assert isinstance(history, list)
        assert len(history) > 0


class TestBusinessLogic:
    """Test business logic and rules."""
    
    def test_stock_validation(self, test_products: List[Product], db_session: Session):
        """Test stock validation during order creation."""
        cart_crud = CartCRUD(db_session)
        product = test_products[0]
        
        # Try to add more items than in stock
        with pytest.raises(ValueError, match="Insufficient stock"):
            cart_crud.add_item_to_cart(
                cart_id=1,  # Assuming cart exists
                product_id=product.id,
                quantity=product.stock_quantity + 1
            )
    
    def test_payment_processing_validation(self, db_session: Session):
        """Test payment processing validation."""
        order_crud = OrderCRUD(db_session)
        
        # Test invalid payment data
        with pytest.raises(ValueError):
            order_crud.process_order_payment(
                order_id=999,  # Non-existent order
                payment_data={'payment_method': 'invalid_method'}
            )
    
    def test_subscription_billing(self, test_user: User, db_session: Session):
        """Test subscription billing workflow."""
        from app.crud.ecommerce import SubscriptionCRUD
        
        subscription_crud = SubscriptionCRUD(db_session)
        
        # This would test subscription creation and billing
        # Implementation depends on subscription product setup


# Performance Tests
class TestPerformance:
    """Test query performance and optimization."""
    
    def test_product_search_performance(self, db_session: Session):
        """Test product search query performance."""
        product_crud = ProductCRUD(db_session)
        
        # Time the search operation
        import time
        start_time = time.time()
        
        products = product_crud.search_products(
            search_term="test",
            category_ids=None,
            limit=50
        )
        
        end_time = time.time()
        query_time = end_time - start_time
        
        # Ensure query completes in reasonable time (adjust threshold as needed)
        assert query_time < 1.0, f"Product search took {query_time} seconds"
    
    def test_analytics_query_performance(self, db_session: Session):
        """Test analytics query performance."""
        from app.db.query_patterns import EcommerceAnalytics
        
        analytics = EcommerceAnalytics(db_session)
        
        import time
        start_time = time.time()
        
        # Test performance of complex analytics query
        segmentation = analytics.get_customer_segmentation()
        
        end_time = time.time()
        query_time = end_time - start_time
        
        assert query_time < 2.0, f"Analytics query took {query_time} seconds"
```

### Section 4.8: API Documentation and Client Examples

**docs/api_examples.py** - Client Usage Examples:

```python
"""
Client usage examples for the e-commerce API.
Shows how to interact with the API from different programming languages.
"""

import requests
from typing import Dict, Any
from decimal import Decimal


class EcommerceAPIClient:
    """Python client for the e-commerce API."""
    
    def __init__(self, base_url: str, api_token: str):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'Authorization': f'Bearer {api_token}',
            'Content-Type': 'application/json'
        }
    
    def get_products(
        self, 
        page: int = 1, 
        size: int = 20,
        search: str = None,
        category_ids: list = None
    ) -> Dict[str, Any]:
        """Get products with filtering and pagination."""
        params = {'page': page, 'size': size}
        
        if search:
            params['search_query'] = search
        if category_ids:
            params['category_ids'] = category_ids
        
        response = requests.get(
            f'{self.base_url}/api/v1/ecommerce/products',
            headers=self.headers,
            params=params
        )
        response.raise_for_status()
        return response.json()
    
    def add_to_cart(self, product_id: int, quantity: int) -> Dict[str, Any]:
        """Add product to shopping cart."""
        data = {
            'product_id': product_id,
            'quantity': quantity
        }
        
        response = requests.post(
            f'{self.base_url}/api/v1/ecommerce/cart/items',
            headers=self.headers,
            json=data
        )
        response.raise_for_status()
        return response.json()
    
    def get_cart(self) -> Dict[str, Any]:
        """Get current user's cart."""
        response = requests.get(
            f'{self.base_url}/api/v1/ecommerce/cart',
            headers=self.headers
        )
        response.raise_for_status()
        return response.json()
    
    def create_order(self, cart_id: int, shipping_address: Dict[str, str]) -> Dict[str, Any]:
        """Create order from cart."""
        data = {
            'cart_id': cart_id,
            'shipping_address': shipping_address
        }
        
        response = requests.post(
            f'{self.base_url}/api/v1/ecommerce/orders',
            headers=self.headers,
            json=data
        )
        response.raise_for_status()
        return response.json()
    
    def process_payment(self, order_id: int, payment_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process payment for order."""
        response = requests.post(
            f'{self.base_url}/api/v1/ecommerce/orders/{order_id}/payment',
            headers=self.headers,
            json=payment_data
        )
        response.raise_for_status()
        return response.json()


# Usage examples
def example_shopping_workflow():
    """Example of complete shopping workflow."""
    
    # Initialize client
    client = EcommerceAPIClient(
        base_url='http://localhost:8000',
        api_token='your-jwt-token-here'
    )
    
    try:
        # 1. Browse products
        products_response = client.get_products(
            search='laptop',
            category_ids=[1, 2]
        )
        
        products = products_response['data']['items']
        print(f"Found {len(products)} products")
        
        # 2. Add products to cart
        if products:
            cart_response = client.add_to_cart(
                product_id=products[0]['id'],
                quantity=1
            )
            print(f"Added to cart: {cart_response['message']}")
        
        # 3. Review cart
        cart = client.get_cart()
        print(f"Cart total: ${cart['data']['subtotal']}")
        
        # 4. Create order
        shipping_address = {
            'first_name': 'John',
            'last_name': 'Doe',
            'address_line_1': '123 Main St',
            'city': 'Anytown',
            'state': 'CA',
            'postal_code': '12345',
            'country': 'USA'
        }
        
        order_response = client.create_order(
            cart_id=cart['data']['id'],
            shipping_address=shipping_address
        )
        
        order_id = order_response['data']['id']
        print(f"Created order: {order_id}")
        
        # 5. Process payment
        payment_data = {
            'payment_method': 'credit_card',
            'card_number': '4111111111111111',
            'card_holder_name': 'John Doe',
            'exp_month': 12,
            'exp_year': 2025,
            'cvv': '123'
        }
        
        payment_response = client.process_payment(
            order_id=order_id,
            payment_data=payment_data
        )
        
        print(f"Payment processed: {payment_response['data']['status']}")
        
    except requests.exceptions.HTTPError as e:
        print(f"API Error: {e}")
        print(f"Response: {e.response.text}")


# JavaScript/Node.js example
js_example = """
// JavaScript/Node.js client example
class EcommerceAPIClient {
    constructor(baseUrl, apiToken) {
        this.baseUrl = baseUrl.replace(/\/$/, '');
        this.headers = {
            'Authorization': `Bearer ${apiToken}`,
            'Content-Type': 'application/json'
        };
    }
    
    async getProducts(options = {}) {
        const params = new URLSearchParams();
        
        if (options.page) params.append('page', options.page);
        if (options.size) params.append('size', options.size);
        if (options.search) params.append('search_query', options.search);
        if (options.categoryIds) {
            options.categoryIds.forEach(id => params.append('category_ids', id));
        }
        
        const response = await fetch(
            `${this.baseUrl}/api/v1/ecommerce/products?${params}`,
            { headers: this.headers }
        );
        
        if (!response.ok) throw new Error(`HTTP ${response.status}`);
        return response.json();
    }
    
    async addToCart(productId, quantity) {
        const response = await fetch(
            `${this.baseUrl}/api/v1/ecommerce/cart/items`,
            {
                method: 'POST',
                headers: this.headers,
                body: JSON.stringify({
                    product_id: productId,
                    quantity: quantity
                })
            }
        );
        
        if (!response.ok) throw new Error(`HTTP ${response.status}`);
        return response.json();
    }
    
    async createOrder(cartId, shippingAddress) {
        const response = await fetch(
            `${this.baseUrl}/api/v1/ecommerce/orders`,
            {
                method: 'POST',
                headers: this.headers,
                body: JSON.stringify({
                    cart_id: cartId,
                    shipping_address: shippingAddress
                })
            }
        );
        
        if (!response.ok) throw new Error(`HTTP ${response.status}`);
        return response.json();
    }
}

// Usage example
const client = new EcommerceAPIClient('http://localhost:8000', 'your-token');

async function shoppingWorkflow() {
    try {
        // Get products
        const products = await client.getProducts({ search: 'laptop', page: 1 });
        console.log(`Found ${products.data.items.length} products`);
        
        // Add to cart
        if (products.data.items.length > 0) {
            const cartResponse = await client.addToCart(products.data.items[0].id, 1);
            console.log('Added to cart:', cartResponse.message);
        }
        
    } catch (error) {
        console.error('Shopping workflow error:', error);
    }
}
"""


# curl command examples
curl_examples = """
# Get products with filtering
curl -X GET "http://localhost:8000/api/v1/ecommerce/products?search_query=laptop&page=1&size=10" \
  -H "Authorization: Bearer your-token"

# Add item to cart
curl -X POST "http://localhost:8000/api/v1/ecommerce/cart/items" \
  -H "Authorization: Bearer your-token" \
  -H "Content-Type: application/json" \
  -d '{
    "product_id": 1,
    "quantity": 2
  }'

# Get cart contents
curl -X GET "http://localhost:8000/api/v1/ecommerce/cart" \
  -H "Authorization: Bearer your-token"

# Create order
curl -X POST "http://localhost:8000/api/v1/ecommerce/orders" \
  -H "Authorization: Bearer your-token" \
  -H "Content-Type: application/json" \
  -d '{
    "cart_id": 1,
    "shipping_address": {
      "first_name": "John",
      "last_name": "Doe",
      "address_line_1": "123 Main St",
      "city": "Anytown",
      "state": "CA",
      "postal_code": "12345",
      "country": "USA"
    }
  }'

# Process payment
curl -X POST "http://localhost:8000/api/v1/ecommerce/orders/1/payment" \
  -H "Authorization: Bearer your-token" \
  -H "Content-Type: application/json" \
  -d '{
    "payment_method": "credit_card",
    "card_number": "4111111111111111",
    "card_holder_name": "John Doe",
    "exp_month": 12,
    "exp_year": 2025,
    "cvv": "123"
  }'

# Get analytics dashboard (admin only)
curl -X GET "http://localhost:8000/api/v1/ecommerce/analytics/dashboard?days=30" \
  -H "Authorization: Bearer admin-token"
"""
```

This completes Section 4.5 with comprehensive API endpoints for the e-commerce system. The section includes:

1. **Complete API Schemas** - Pydantic models for all e-commerce operations
2. **Product Management APIs** - CRUD operations with advanced filtering and search
3. **Shopping Cart APIs** - Full cart management with real-time totals
4. **Order Processing APIs** - End-to-end order workflow with payment integration
5. **Analytics APIs** - Business intelligence and reporting endpoints
6. **Webhook Handling** - Payment provider and supplier integration
7. **Background Tasks** - Automated processes for maintenance and billing
8. **Legacy Integration** - Working with existing database tables
9. **Comprehensive Testing** - Integration tests for the complete system
10. **Client Examples** - Usage examples in Python, JavaScript, and curl

The e-commerce system demonstrates advanced SQLAlchemy concepts including polymorphic inheritance, advanced query patterns, business logic integration, real-time inventory management, payment processing workflows, analytics and reporting, legacy system integration, and background task processing.

---

## Part 5: Advanced Transaction Management System

This section demonstrates SQLAlchemy's advanced transaction management capabilities, including distributed transactions, savepoints, connection pools, and handling complex business scenarios with strict ACID compliance.

### 5.1 Transaction Management Models

**app/models/transactions.py** - Financial Transaction System:
```python
"""
Advanced transaction management models demonstrating:
- Complex multi-table transactions with strict consistency
- Financial operations with decimal precision
- Audit trails and transaction logging
- Distributed transaction patterns
"""

import enum
import uuid
from decimal import Decimal
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any

from sqlalchemy import (
    Column, Integer, String, DateTime, Boolean, Numeric, Text,
    ForeignKey, CheckConstraint, Index, UniqueConstraint, func
)
from sqlalchemy.orm import relationship, validates, Session
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.sql import select

from app.models.base import Base
from app.models.user import User


class TransactionType(enum.Enum):
    """Transaction type enumeration."""
    DEPOSIT = "deposit"
    WITHDRAWAL = "withdrawal"
    TRANSFER = "transfer"
    PAYMENT = "payment"
    REFUND = "refund"
    FEE = "fee"
    ADJUSTMENT = "adjustment"
    CHARGEBACK = "chargeback"


class TransactionStatus(enum.Enum):
    """Transaction status enumeration."""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    REVERSED = "reversed"


class AccountType(enum.Enum):
    """Account type enumeration."""
    CHECKING = "checking"
    SAVINGS = "savings"
    CREDIT = "credit"
    MERCHANT = "merchant"
    ESCROW = "escrow"
    SYSTEM = "system"


class Currency(enum.Enum):
    """Currency enumeration."""
    USD = "USD"
    EUR = "EUR"
    GBP = "GBP"
    JPY = "JPY"
    CAD = "CAD"


class Account(Base):
    """
    Financial accounts with strict balance management and audit trails.
    Demonstrates complex constraints and business rules.
    """
    __tablename__ = 'accounts'

    id = Column(Integer, primary_key=True)
    account_number = Column(String(20), unique=True, nullable=False, index=True)
    account_name = Column(String(100), nullable=False)
    account_type = Column(Enum(AccountType), nullable=False)
    currency = Column(Enum(Currency), nullable=False, default=Currency.USD)
    
    # Balance tracking with decimal precision for financial accuracy
    balance = Column(Numeric(15, 2), nullable=False, default=0)
    available_balance = Column(Numeric(15, 2), nullable=False, default=0)
    reserved_balance = Column(Numeric(15, 2), nullable=False, default=0)
    
    # Account limits and settings
    minimum_balance = Column(Numeric(15, 2), nullable=False, default=0)
    maximum_balance = Column(Numeric(15, 2))
    daily_withdrawal_limit = Column(Numeric(15, 2))
    monthly_withdrawal_limit = Column(Numeric(15, 2))
    
    # Account status and metadata
    is_active = Column(Boolean, nullable=False, default=True)
    is_frozen = Column(Boolean, nullable=False, default=False)
    freeze_reason = Column(Text)
    
    # Relationships
    owner_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    owner = relationship("User", back_populates="accounts")
    
    # Audit fields
    opened_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    last_activity_at = Column(DateTime)
    
    # Additional metadata
    metadata = Column(JSONB, default=dict)
    
    # Relationships to transactions
    debit_transactions = relationship(
        "Transaction",
        foreign_keys="Transaction.from_account_id",
        back_populates="from_account",
        lazy="dynamic"
    )
    credit_transactions = relationship(
        "Transaction", 
        foreign_keys="Transaction.to_account_id",
        back_populates="to_account",
        lazy="dynamic"
    )
    
    # Constraints
    __table_args__ = (
        CheckConstraint('balance >= 0', name='positive_balance'),
        CheckConstraint('available_balance >= 0', name='positive_available_balance'),
        CheckConstraint('reserved_balance >= 0', name='positive_reserved_balance'),
        CheckConstraint('balance = available_balance + reserved_balance', 
                       name='balance_consistency'),
        CheckConstraint(
            'minimum_balance <= maximum_balance OR maximum_balance IS NULL',
            name='balance_limits_consistency'
        ),
        Index('ix_accounts_owner_type', 'owner_id', 'account_type'),
        Index('ix_accounts_currency_active', 'currency', 'is_active'),
    )

    @hybrid_property
    def total_debits(self) -> Decimal:
        """Calculate total debit amount."""
        return self.debit_transactions.filter(
            Transaction.status == TransactionStatus.COMPLETED
        ).with_entities(func.sum(Transaction.amount)).scalar() or Decimal('0')

    @hybrid_property 
    def total_credits(self) -> Decimal:
        """Calculate total credit amount."""
        return self.credit_transactions.filter(
            Transaction.status == TransactionStatus.COMPLETED
        ).with_entities(func.sum(Transaction.amount)).scalar() or Decimal('0')

    @validates('balance', 'available_balance', 'reserved_balance')
    def validate_balances(self, key: str, value: Decimal) -> Decimal:
        """Validate balance amounts."""
        if value < 0:
            raise ValueError(f"{key} cannot be negative")
        return value

    def can_withdraw(self, amount: Decimal) -> bool:
        """Check if withdrawal amount is allowed."""
        if self.is_frozen or not self.is_active:
            return False
        if self.available_balance < amount:
            return False
        if self.minimum_balance and (self.balance - amount) < self.minimum_balance:
            return False
        return True

    def reserve_funds(self, amount: Decimal) -> bool:
        """Reserve funds for pending transaction."""
        if not self.can_withdraw(amount):
            return False
        self.available_balance -= amount
        self.reserved_balance += amount
        return True

    def release_reserved_funds(self, amount: Decimal) -> None:
        """Release reserved funds back to available balance."""
        if self.reserved_balance >= amount:
            self.reserved_balance -= amount
            self.available_balance += amount

    def __repr__(self) -> str:
        return f"<Account {self.account_number}: {self.balance} {self.currency.value}>"


class Transaction(Base):
    """
    Financial transactions with comprehensive audit trail and state management.
    Supports complex multi-party transactions and distributed consistency.
    """
    __tablename__ = 'transactions'

    id = Column(Integer, primary_key=True)
    transaction_id = Column(UUID(as_uuid=True), nullable=False, default=uuid.uuid4, 
                          unique=True, index=True)
    
    # Transaction details
    transaction_type = Column(Enum(TransactionType), nullable=False)
    status = Column(Enum(TransactionStatus), nullable=False, 
                   default=TransactionStatus.PENDING)
    
    # Financial details
    amount = Column(Numeric(15, 2), nullable=False)
    currency = Column(Enum(Currency), nullable=False)
    exchange_rate = Column(Numeric(10, 6), default=1.0)
    fee_amount = Column(Numeric(15, 2), default=0)
    
    # Account relationships
    from_account_id = Column(Integer, ForeignKey('accounts.id'))
    to_account_id = Column(Integer, ForeignKey('accounts.id'))
    from_account = relationship("Account", foreign_keys=[from_account_id],
                              back_populates="debit_transactions")
    to_account = relationship("Account", foreign_keys=[to_account_id],
                            back_populates="credit_transactions")
    
    # Transaction metadata
    description = Column(String(500))
    reference = Column(String(100), index=True)  # External reference
    batch_id = Column(String(50), index=True)   # For batch processing
    
    # Audit and tracking
    initiated_by_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    initiated_by = relationship("User", foreign_keys=[initiated_by_id])
    approved_by_id = Column(Integer, ForeignKey('users.id'))
    approved_by = relationship("User", foreign_keys=[approved_by_id])
    
    # Timestamps
    initiated_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    processed_at = Column(DateTime)
    completed_at = Column(DateTime)
    
    # Additional data
    metadata = Column(JSONB, default=dict)
    
    # Error tracking
    error_code = Column(String(50))
    error_message = Column(Text)
    retry_count = Column(Integer, default=0)
    
    # Parent transaction for complex operations
    parent_transaction_id = Column(Integer, ForeignKey('transactions.id'))
    parent_transaction = relationship("Transaction", remote_side=[id],
                                    back_populates="child_transactions")
    child_transactions = relationship("Transaction", back_populates="parent_transaction")
    
    # Constraints
    __table_args__ = (
        CheckConstraint('amount > 0', name='positive_amount'),
        CheckConstraint('fee_amount >= 0', name='non_negative_fee'),
        CheckConstraint('exchange_rate > 0', name='positive_exchange_rate'),
        CheckConstraint(
            '(from_account_id IS NOT NULL OR to_account_id IS NOT NULL)',
            name='at_least_one_account'
        ),
        Index('ix_transactions_status_type', 'status', 'transaction_type'),
        Index('ix_transactions_accounts', 'from_account_id', 'to_account_id'),
        Index('ix_transactions_date', 'initiated_at'),
        Index('ix_transactions_batch', 'batch_id'),
        UniqueConstraint('transaction_id', name='unique_transaction_id'),
    )

    @validates('amount', 'fee_amount')
    def validate_amounts(self, key: str, value: Decimal) -> Decimal:
        """Validate transaction amounts."""
        if key == 'amount' and value <= 0:
            raise ValueError("Transaction amount must be positive")
        if key == 'fee_amount' and value < 0:
            raise ValueError("Fee amount cannot be negative")
        return value

    @property
    def total_amount(self) -> Decimal:
        """Get total amount including fees."""
        return self.amount + self.fee_amount

    @property
    def is_pending(self) -> bool:
        """Check if transaction is pending."""
        return self.status == TransactionStatus.PENDING

    @property
    def is_completed(self) -> bool:
        """Check if transaction is completed."""
        return self.status == TransactionStatus.COMPLETED

    @property
    def processing_time(self) -> Optional[timedelta]:
        """Calculate transaction processing time."""
        if self.completed_at and self.initiated_at:
            return self.completed_at - self.initiated_at
        return None

    def __repr__(self) -> str:
        return (f"<Transaction {self.transaction_id}: "
                f"{self.amount} {self.currency.value} "
                f"{self.transaction_type.value} [{self.status.value}]>")


class TransactionLog(Base):
    """
    Comprehensive audit log for all transaction state changes.
    Immutable record for compliance and debugging.
    """
    __tablename__ = 'transaction_logs'

    id = Column(Integer, primary_key=True)
    transaction_id = Column(Integer, ForeignKey('transactions.id'), nullable=False)
    transaction = relationship("Transaction")
    
    # State change information
    from_status = Column(Enum(TransactionStatus))
    to_status = Column(Enum(TransactionStatus), nullable=False)
    
    # Change metadata
    changed_by_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    changed_by = relationship("User")
    changed_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    
    # Additional context
    reason = Column(String(500))
    metadata = Column(JSONB, default=dict)
    
    # System information
    ip_address = Column(String(45))  # IPv6 compatible
    user_agent = Column(String(500))
    
    __table_args__ = (
        Index('ix_transaction_logs_transaction', 'transaction_id'),
        Index('ix_transaction_logs_changed_at', 'changed_at'),
    )


class BalanceSnapshot(Base):
    """
    Daily balance snapshots for accounts to track historical balances.
    Used for reporting and reconciliation.
    """
    __tablename__ = 'balance_snapshots'

    id = Column(Integer, primary_key=True)
    account_id = Column(Integer, ForeignKey('accounts.id'), nullable=False)
    account = relationship("Account")
    
    # Snapshot data
    snapshot_date = Column(DateTime, nullable=False)
    opening_balance = Column(Numeric(15, 2), nullable=False)
    closing_balance = Column(Numeric(15, 2), nullable=False)
    
    # Daily transaction summary
    total_debits = Column(Numeric(15, 2), default=0)
    total_credits = Column(Numeric(15, 2), default=0)
    transaction_count = Column(Integer, default=0)
    
    # Additional metrics
    average_balance = Column(Numeric(15, 2))
    minimum_balance = Column(Numeric(15, 2))
    maximum_balance = Column(Numeric(15, 2))
    
    __table_args__ = (
        UniqueConstraint('account_id', 'snapshot_date', 
                        name='unique_account_snapshot'),
        Index('ix_balance_snapshots_date', 'snapshot_date'),
    )


# Update User model to include accounts relationship
User.accounts = relationship("Account", back_populates="owner", lazy="dynamic")
```

### 5.2 Advanced Transaction Processing Service

**app/services/transaction_service.py** - Transaction Processing Engine:
```python
"""
Advanced transaction processing service with distributed transaction support.
Handles complex financial operations with strict consistency guarantees.
"""

import logging
import uuid
from contextlib import contextmanager
from decimal import Decimal
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Tuple

from sqlalchemy import and_, or_, func, select, text
from sqlalchemy.orm import Session, sessionmaker
from sqlalchemy.exc import SQLAlchemyError, IntegrityError
from sqlalchemy.engine import Engine

from app.database import engine, get_db_session
from app.models.transactions import (
    Account, Transaction, TransactionLog, BalanceSnapshot,
    TransactionType, TransactionStatus, Currency
)
from app.models.user import User
from app.core.exceptions import (
    InsufficientFundsError, AccountFrozenError, TransactionError,
    InvalidTransactionError
)

logger = logging.getLogger(__name__)


class TransactionProcessor:
    """
    Advanced transaction processing engine with distributed transaction support.
    Handles complex financial operations with ACID compliance.
    """

    def __init__(self, db_session: Session):
        self.db = db_session

    @contextmanager
    def transaction_context(self, isolation_level: str = "READ_COMMITTED"):
        """
        Advanced transaction context with configurable isolation levels.
        Supports nested transactions using savepoints.
        """
        savepoint = None
        try:
            # Set isolation level for this transaction
            self.db.execute(text(f"SET TRANSACTION ISOLATION LEVEL {isolation_level}"))
            
            # Create savepoint for nested transaction support
            savepoint = self.db.begin_nested()
            
            yield self.db
            
            # Commit the savepoint
            savepoint.commit()
            
        except Exception as e:
            logger.error(f"Transaction failed: {str(e)}")
            if savepoint:
                savepoint.rollback()
            raise TransactionError(f"Transaction processing failed: {str(e)}")

    def create_account(
        self,
        owner_id: int,
        account_type: str,
        initial_balance: Decimal = Decimal('0'),
        currency: Currency = Currency.USD,
        **kwargs
    ) -> Account:
        """Create a new financial account with validation."""
        with self.transaction_context():
            # Generate unique account number
            account_number = self._generate_account_number(account_type)
            
            account = Account(
                account_number=account_number,
                account_name=kwargs.get('account_name', f"{account_type} Account"),
                account_type=account_type,
                currency=currency,
                balance=initial_balance,
                available_balance=initial_balance,
                reserved_balance=Decimal('0'),
                owner_id=owner_id,
                minimum_balance=kwargs.get('minimum_balance', Decimal('0')),
                maximum_balance=kwargs.get('maximum_balance'),
                daily_withdrawal_limit=kwargs.get('daily_withdrawal_limit'),
                monthly_withdrawal_limit=kwargs.get('monthly_withdrawal_limit'),
                metadata=kwargs.get('metadata', {})
            )
            
            self.db.add(account)
            self.db.flush()  # Get the ID without committing
            
            # Create initial balance snapshot if there's a balance
            if initial_balance > 0:
                self._create_balance_snapshot(account, initial_balance, Decimal('0'))
            
            logger.info(f"Created account {account.account_number} for user {owner_id}")
            return account

    def process_transfer(
        self,
        from_account_id: int,
        to_account_id: int,
        amount: Decimal,
        description: str = None,
        initiated_by_id: int = None,
        reference: str = None,
        metadata: Dict = None
    ) -> Transaction:
        """
        Process a transfer between two accounts with full validation and audit trail.
        Uses distributed transaction pattern for consistency.
        """
        with self.transaction_context(isolation_level="SERIALIZABLE"):
            # Lock and validate both accounts
            from_account = self._lock_account(from_account_id)
            to_account = self._lock_account(to_account_id)
            
            # Validate transfer
            self._validate_transfer(from_account, to_account, amount)
            
            # Create transaction record
            transaction = Transaction(
                transaction_id=uuid.uuid4(),
                transaction_type=TransactionType.TRANSFER,
                status=TransactionStatus.PENDING,
                amount=amount,
                currency=from_account.currency,
                from_account_id=from_account_id,
                to_account_id=to_account_id,
                description=description,
                reference=reference,
                initiated_by_id=initiated_by_id,
                metadata=metadata or {}
            )
            
            self.db.add(transaction)
            self.db.flush()
            
            try:
                # Reserve funds in source account
                if not from_account.reserve_funds(amount):
                    raise InsufficientFundsError(
                        f"Cannot reserve {amount} from account {from_account.account_number}"
                    )
                
                # Update transaction status
                self._update_transaction_status(
                    transaction, TransactionStatus.PROCESSING, initiated_by_id
                )
                
                # Execute the transfer
                from_account.balance -= amount
                from_account.reserved_balance -= amount
                to_account.balance += amount
                to_account.available_balance += amount
                
                # Update last activity timestamps
                now = datetime.utcnow()
                from_account.last_activity_at = now
                to_account.last_activity_at = now
                transaction.processed_at = now
                
                # Complete the transaction
                self._update_transaction_status(
                    transaction, TransactionStatus.COMPLETED, initiated_by_id
                )
                transaction.completed_at = now
                
                logger.info(
                    f"Transfer completed: {amount} from {from_account.account_number} "
                    f"to {to_account.account_number}"
                )
                
                return transaction
                
            except Exception as e:
                # Rollback reserved funds and mark transaction as failed
                from_account.release_reserved_funds(amount)
                self._update_transaction_status(
                    transaction, TransactionStatus.FAILED, initiated_by_id,
                    error_code="TRANSFER_FAILED", error_message=str(e)
                )
                raise

    def process_deposit(
        self,
        account_id: int,
        amount: Decimal,
        description: str = None,
        initiated_by_id: int = None,
        reference: str = None,
        metadata: Dict = None
    ) -> Transaction:
        """Process a deposit to an account."""
        with self.transaction_context():
            account = self._lock_account(account_id)
            
            # Validate deposit
            if account.is_frozen:
                raise AccountFrozenError(f"Account {account.account_number} is frozen")
            
            if account.maximum_balance and (account.balance + amount) > account.maximum_balance:
                raise InvalidTransactionError("Deposit would exceed maximum balance")
            
            # Create transaction
            transaction = Transaction(
                transaction_id=uuid.uuid4(),
                transaction_type=TransactionType.DEPOSIT,
                status=TransactionStatus.PROCESSING,
                amount=amount,
                currency=account.currency,
                to_account_id=account_id,
                description=description,
                reference=reference,
                initiated_by_id=initiated_by_id,
                metadata=metadata or {}
            )
            
            self.db.add(transaction)
            self.db.flush()
            
            # Execute deposit
            account.balance += amount
            account.available_balance += amount
            account.last_activity_at = datetime.utcnow()
            
            # Complete transaction
            self._update_transaction_status(
                transaction, TransactionStatus.COMPLETED, initiated_by_id
            )
            transaction.completed_at = datetime.utcnow()
            transaction.processed_at = datetime.utcnow()
            
            logger.info(f"Deposit completed: {amount} to {account.account_number}")
            return transaction

    def process_withdrawal(
        self,
        account_id: int,
        amount: Decimal,
        description: str = None,
        initiated_by_id: int = None,
        reference: str = None,
        metadata: Dict = None
    ) -> Transaction:
        """Process a withdrawal from an account."""
        with self.transaction_context():
            account = self._lock_account(account_id)
            
            # Validate withdrawal
            if not account.can_withdraw(amount):
                reasons = []
                if account.is_frozen:
                    reasons.append("account is frozen")
                if not account.is_active:
                    reasons.append("account is inactive")
                if account.available_balance < amount:
                    reasons.append("insufficient funds")
                if account.minimum_balance and (account.balance - amount) < account.minimum_balance:
                    reasons.append("would violate minimum balance requirement")
                
                raise InsufficientFundsError(
                    f"Cannot withdraw {amount} from {account.account_number}: "
                    f"{', '.join(reasons)}"
                )
            
            # Check daily/monthly limits
            self._validate_withdrawal_limits(account, amount)
            
            # Create transaction
            transaction = Transaction(
                transaction_id=uuid.uuid4(),
                transaction_type=TransactionType.WITHDRAWAL,
                status=TransactionStatus.PROCESSING,
                amount=amount,
                currency=account.currency,
                from_account_id=account_id,
                description=description,
                reference=reference,
                initiated_by_id=initiated_by_id,
                metadata=metadata or {}
            )
            
            self.db.add(transaction)
            self.db.flush()
            
            # Execute withdrawal
            account.balance -= amount
            account.available_balance -= amount
            account.last_activity_at = datetime.utcnow()
            
            # Complete transaction
            self._update_transaction_status(
                transaction, TransactionStatus.COMPLETED, initiated_by_id
            )
            transaction.completed_at = datetime.utcnow()
            transaction.processed_at = datetime.utcnow()
            
            logger.info(f"Withdrawal completed: {amount} from {account.account_number}")
            return transaction

    def batch_process_transactions(
        self,
        transactions_data: List[Dict[str, Any]],
        batch_id: str = None
    ) -> List[Transaction]:
        """
        Process multiple transactions in a single batch with rollback on any failure.
        Demonstrates distributed transaction patterns.
        """
        if not batch_id:
            batch_id = str(uuid.uuid4())
        
        processed_transactions = []
        
        with self.transaction_context(isolation_level="SERIALIZABLE"):
            try:
                for tx_data in transactions_data:
                    tx_data['batch_id'] = batch_id
                    tx_data['metadata'] = tx_data.get('metadata', {})
                    tx_data['metadata']['batch_size'] = len(transactions_data)
                    
                    # Process based on transaction type
                    tx_type = tx_data.get('transaction_type')
                    
                    if tx_type == 'transfer':
                        transaction = self.process_transfer(**tx_data)
                    elif tx_type == 'deposit':
                        transaction = self.process_deposit(**tx_data)
                    elif tx_type == 'withdrawal':
                        transaction = self.process_withdrawal(**tx_data)
                    else:
                        raise InvalidTransactionError(f"Unsupported transaction type: {tx_type}")
                    
                    processed_transactions.append(transaction)
                
                logger.info(f"Batch {batch_id} processed successfully: {len(processed_transactions)} transactions")
                return processed_transactions
                
            except Exception as e:
                logger.error(f"Batch {batch_id} failed: {str(e)}")
                # Mark all transactions in batch as failed
                for transaction in processed_transactions:
                    self._update_transaction_status(
                        transaction, TransactionStatus.FAILED,
                        error_code="BATCH_FAILED",
                        error_message=f"Batch processing failed: {str(e)}"
                    )
                raise TransactionError(f"Batch processing failed: {str(e)}")

    def reverse_transaction(
        self,
        transaction_id: int,
        reason: str,
        reversed_by_id: int
    ) -> Transaction:
        """
        Reverse a completed transaction by creating a compensating transaction.
        """
        with self.transaction_context():
            # Get original transaction
            original_tx = self.db.query(Transaction).filter_by(id=transaction_id).first()
            if not original_tx:
                raise InvalidTransactionError(f"Transaction {transaction_id} not found")
            
            if original_tx.status != TransactionStatus.COMPLETED:
                raise InvalidTransactionError("Only completed transactions can be reversed")
            
            # Create reversal transaction
            reversal_data = {
                'amount': original_tx.amount,
                'description': f"Reversal of transaction {original_tx.transaction_id}: {reason}",
                'reference': f"REV-{original_tx.reference}" if original_tx.reference else None,
                'initiated_by_id': reversed_by_id,
                'metadata': {
                    'original_transaction_id': str(original_tx.transaction_id),
                    'reversal_reason': reason,
                    'is_reversal': True
                }
            }
            
            # Create opposite transaction
            if original_tx.transaction_type == TransactionType.TRANSFER:
                # Reverse transfer direction
                reversal_tx = self.process_transfer(
                    from_account_id=original_tx.to_account_id,
                    to_account_id=original_tx.from_account_id,
                    **reversal_data
                )
            elif original_tx.transaction_type == TransactionType.DEPOSIT:
                # Create withdrawal to reverse deposit
                reversal_tx = self.process_withdrawal(
                    account_id=original_tx.to_account_id,
                    **reversal_data
                )
            elif original_tx.transaction_type == TransactionType.WITHDRAWAL:
                # Create deposit to reverse withdrawal
                reversal_tx = self.process_deposit(
                    account_id=original_tx.from_account_id,
                    **reversal_data
                )
            else:
                raise InvalidTransactionError(f"Cannot reverse {original_tx.transaction_type} transactions")
            
            # Update original transaction status
            self._update_transaction_status(
                original_tx, TransactionStatus.REVERSED, reversed_by_id
            )
            
            # Link transactions
            reversal_tx.parent_transaction_id = original_tx.id
            
            logger.info(f"Transaction {original_tx.transaction_id} reversed successfully")
            return reversal_tx

    def create_balance_snapshots(self, snapshot_date: datetime = None) -> int:
        """Create daily balance snapshots for all active accounts."""
        if not snapshot_date:
            snapshot_date = datetime.utcnow().date()
        
        snapshots_created = 0
        
        with self.transaction_context():
            # Get all active accounts
            accounts = self.db.query(Account).filter(Account.is_active == True).all()
            
            for account in accounts:
                # Check if snapshot already exists
                existing = self.db.query(BalanceSnapshot).filter(
                    and_(
                        BalanceSnapshot.account_id == account.id,
                        func.date(BalanceSnapshot.snapshot_date) == snapshot_date
                    )
                ).first()
                
                if existing:
                    continue
                
                # Calculate daily statistics
                daily_stats = self._calculate_daily_account_stats(account, snapshot_date)
                
                snapshot = BalanceSnapshot(
                    account_id=account.id,
                    snapshot_date=snapshot_date,
                    opening_balance=daily_stats['opening_balance'],
                    closing_balance=account.balance,
                    total_debits=daily_stats['total_debits'],
                    total_credits=daily_stats['total_credits'],
                    transaction_count=daily_stats['transaction_count'],
                    average_balance=daily_stats['average_balance'],
                    minimum_balance=daily_stats['minimum_balance'],
                    maximum_balance=daily_stats['maximum_balance']
                )
                
                self.db.add(snapshot)
                snapshots_created += 1
            
            logger.info(f"Created {snapshots_created} balance snapshots for {snapshot_date}")
            return snapshots_created

    # Private helper methods
    def _lock_account(self, account_id: int) -> Account:
        """Lock account for exclusive access in transaction."""
        account = self.db.query(Account).with_for_update().filter_by(id=account_id).first()
        if not account:
            raise InvalidTransactionError(f"Account {account_id} not found")
        return account

    def _validate_transfer(self, from_account: Account, to_account: Account, amount: Decimal):
        """Validate transfer prerequisites."""
        if from_account.id == to_account.id:
            raise InvalidTransactionError("Cannot transfer to the same account")
        
        if from_account.currency != to_account.currency:
            raise InvalidTransactionError("Currency conversion transfers not supported")
        
        if from_account.is_frozen or to_account.is_frozen:
            raise AccountFrozenError("One or both accounts are frozen")
        
        if not from_account.is_active or not to_account.is_active:
            raise InvalidTransactionError("One or both accounts are inactive")
        
        if not from_account.can_withdraw(amount):
            raise InsufficientFundsError(f"Insufficient funds in account {from_account.account_number}")

    def _update_transaction_status(
        self,
        transaction: Transaction,
        new_status: TransactionStatus,
        changed_by_id: int = None,
        error_code: str = None,
        error_message: str = None
    ):
        """Update transaction status with audit logging."""
        old_status = transaction.status
        transaction.status = new_status
        
        if error_code:
            transaction.error_code = error_code
        if error_message:
            transaction.error_message = error_message
        
        # Create audit log
        log_entry = TransactionLog(
            transaction_id=transaction.id,
            from_status=old_status,
            to_status=new_status,
            changed_by_id=changed_by_id,
            reason=error_message,
            metadata={
                'error_code': error_code,
                'retry_count': transaction.retry_count
            }
        )
        self.db.add(log_entry)

    def _generate_account_number(self, account_type: str) -> str:
        """Generate unique account number."""
        prefix = {
            'checking': 'CHK',
            'savings': 'SAV', 
            'credit': 'CRD',
            'merchant': 'MER',
            'escrow': 'ESC',
            'system': 'SYS'
        }.get(account_type, 'GEN')
        
        # Generate unique number (in production, use more sophisticated method)
        import time
        timestamp = str(int(time.time()))[-8:]
        random_part = str(uuid.uuid4().int)[-4:]
        return f"{prefix}{timestamp}{random_part}"

    def _validate_withdrawal_limits(self, account: Account, amount: Decimal):
        """Validate daily and monthly withdrawal limits."""
        now = datetime.utcnow()
        
        if account.daily_withdrawal_limit:
            daily_withdrawals = self.db.query(func.sum(Transaction.amount)).filter(
                and_(
                    Transaction.from_account_id == account.id,
                    Transaction.transaction_type == TransactionType.WITHDRAWAL,
                    Transaction.status == TransactionStatus.COMPLETED,
                    func.date(Transaction.completed_at) == now.date()
                )
            ).scalar() or Decimal('0')
            
            if daily_withdrawals + amount > account.daily_withdrawal_limit:
                raise InvalidTransactionError("Daily withdrawal limit exceeded")
        
        if account.monthly_withdrawal_limit:
            month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            monthly_withdrawals = self.db.query(func.sum(Transaction.amount)).filter(
                and_(
                    Transaction.from_account_id == account.id,
                    Transaction.transaction_type == TransactionType.WITHDRAWAL,
                    Transaction.status == TransactionStatus.COMPLETED,
                    Transaction.completed_at >= month_start
                )
            ).scalar() or Decimal('0')
            
            if monthly_withdrawals + amount > account.monthly_withdrawal_limit:
                raise InvalidTransactionError("Monthly withdrawal limit exceeded")

    def _calculate_daily_account_stats(self, account: Account, date) -> Dict[str, Decimal]:
        """Calculate daily statistics for an account."""
        # Get previous day's closing balance as opening balance
        previous_snapshot = self.db.query(BalanceSnapshot).filter(
            and_(
                BalanceSnapshot.account_id == account.id,
                func.date(BalanceSnapshot.snapshot_date) < date
            )
        ).order_by(BalanceSnapshot.snapshot_date.desc()).first()
        
        opening_balance = previous_snapshot.closing_balance if previous_snapshot else Decimal('0')
        
        # Calculate daily transaction totals
        daily_transactions = self.db.query(Transaction).filter(
            and_(
                or_(
                    Transaction.from_account_id == account.id,
                    Transaction.to_account_id == account.id
                ),
                func.date(Transaction.completed_at) == date,
                Transaction.status == TransactionStatus.COMPLETED
            )
        ).all()
        
        total_debits = sum(
            tx.amount for tx in daily_transactions 
            if tx.from_account_id == account.id
        )
        
        total_credits = sum(
            tx.amount for tx in daily_transactions
            if tx.to_account_id == account.id
        )
        
        return {
            'opening_balance': opening_balance,
            'total_debits': total_debits,
            'total_credits': total_credits,
            'transaction_count': len(daily_transactions),
            'average_balance': (opening_balance + account.balance) / 2,
            'minimum_balance': min(opening_balance, account.balance),
            'maximum_balance': max(opening_balance, account.balance)
        }

    def _create_balance_snapshot(self, account: Account, opening: Decimal, closing: Decimal):
        """Create a balance snapshot for an account."""
        snapshot = BalanceSnapshot(
            account_id=account.id,
            snapshot_date=datetime.utcnow(),
            opening_balance=opening,
            closing_balance=closing,
            total_debits=Decimal('0'),
            total_credits=closing - opening if closing > opening else Decimal('0'),
            transaction_count=1 if closing != opening else 0
        )
        self.db.add(snapshot)
```

### 5.3 Transaction CRUD Operations

**app/crud/transactions.py** - Advanced Transaction CRUD:
```python
"""
Advanced Transaction CRUD operations with financial compliance.
Includes account management, transaction processing, and audit capabilities.
"""

import logging
from datetime import datetime, timedelta
from decimal import Decimal
from typing import List, Optional, Dict, Any, Tuple

from sqlalchemy import and_, or_, func, desc, asc, text, case
from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy.exc import IntegrityError

from app.crud.base import CRUDBase
from app.models.transactions import (
    Account, Transaction, TransactionLog, BalanceSnapshot,
    TransactionType, TransactionStatus, Currency, AccountType
)
from app.schemas.common import PaginationParams
from app.core.exceptions import TransactionError, InsufficientFundsError

logger = logging.getLogger(__name__)


class AccountCRUD(CRUDBase[Account, dict, dict]):
    """Account CRUD operations with financial validation."""

    def create_account(
        self,
        db: Session,
        owner_id: int,
        account_type: AccountType,
        initial_balance: Decimal = Decimal('0'),
        **kwargs
    ) -> Account:
        """Create new account with validation."""
        account_data = {
            'owner_id': owner_id,
            'account_type': account_type,
            'balance': initial_balance,
            'available_balance': initial_balance,
            'reserved_balance': Decimal('0'),
            **kwargs
        }
        
        # Generate unique account number
        account_number = self._generate_account_number(db, account_type)
        account_data['account_number'] = account_number
        
        return super().create(db, obj_in=account_data)

    def get_user_accounts(
        self,
        db: Session,
        user_id: int,
        account_type: Optional[AccountType] = None,
        active_only: bool = True
    ) -> List[Account]:
        """Get all accounts for a user."""
        query = db.query(Account).filter(Account.owner_id == user_id)
        
        if account_type:
            query = query.filter(Account.account_type == account_type)
        
        if active_only:
            query = query.filter(
                and_(Account.is_active == True, Account.is_frozen == False)
            )
        
        return query.order_by(Account.created_at.desc()).all()

    def get_account_by_number(
        self,
        db: Session,
        account_number: str
    ) -> Optional[Account]:
        """Get account by account number."""
        return db.query(Account).filter(Account.account_number == account_number).first()

    def freeze_account(
        self,
        db: Session,
        account_id: int,
        reason: str,
        frozen_by_id: int
    ) -> Account:
        """Freeze an account with reason."""
        account = self.get(db, id=account_id)
        if not account:
            raise ValueError(f"Account {account_id} not found")
        
        account.is_frozen = True
        account.freeze_reason = reason
        account.updated_at = datetime.utcnow()
        
        db.commit()
        
        logger.info(f"Account {account.account_number} frozen by user {frozen_by_id}: {reason}")
        return account

    def unfreeze_account(
        self,
        db: Session,
        account_id: int,
        unfrozen_by_id: int
    ) -> Account:
        """Unfreeze an account."""
        account = self.get(db, id=account_id)
        if not account:
            raise ValueError(f"Account {account_id} not found")
        
        account.is_frozen = False
        account.freeze_reason = None
        account.updated_at = datetime.utcnow()
        
        db.commit()
        
        logger.info(f"Account {account.account_number} unfrozen by user {unfrozen_by_id}")
        return account

    def get_account_balance_history(
        self,
        db: Session,
        account_id: int,
        start_date: datetime,
        end_date: datetime
    ) -> List[BalanceSnapshot]:
        """Get historical balance snapshots for an account."""
        return db.query(BalanceSnapshot).filter(
            and_(
                BalanceSnapshot.account_id == account_id,
                BalanceSnapshot.snapshot_date >= start_date,
                BalanceSnapshot.snapshot_date <= end_date
            )
        ).order_by(BalanceSnapshot.snapshot_date.asc()).all()

    def get_accounts_summary(
        self,
        db: Session,
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get summary statistics for accounts."""
        query = db.query(Account)
        
        if user_id:
            query = query.filter(Account.owner_id == user_id)
        
        accounts = query.all()
        
        total_balance = sum(acc.balance for acc in accounts)
        total_available = sum(acc.available_balance for acc in accounts)
        total_reserved = sum(acc.reserved_balance for acc in accounts)
        
        by_type = {}
        for acc in accounts:
            acc_type = acc.account_type.value
            if acc_type not in by_type:
                by_type[acc_type] = {'count': 0, 'total_balance': Decimal('0')}
            by_type[acc_type]['count'] += 1
            by_type[acc_type]['total_balance'] += acc.balance
        
        return {
            'total_accounts': len(accounts),
            'total_balance': total_balance,
            'total_available_balance': total_available,
            'total_reserved_balance': total_reserved,
            'active_accounts': len([acc for acc in accounts if acc.is_active and not acc.is_frozen]),
            'frozen_accounts': len([acc for acc in accounts if acc.is_frozen]),
            'by_account_type': by_type
        }

    def _generate_account_number(self, db: Session, account_type: AccountType) -> str:
        """Generate unique account number."""
        prefix = {
            AccountType.CHECKING: 'CHK',
            AccountType.SAVINGS: 'SAV',
            AccountType.CREDIT: 'CRD',
            AccountType.MERCHANT: 'MER',
            AccountType.ESCROW: 'ESC',
            AccountType.SYSTEM: 'SYS'
        }.get(account_type, 'GEN')
        
        # Get next sequence number for this account type
        count = db.query(func.count(Account.id)).filter(
            Account.account_type == account_type
        ).scalar()
        
        # Format: PREFIX + 8-digit sequence + 2-digit check
        sequence = str(count + 1).zfill(8)
        check_digits = str(hash(f"{prefix}{sequence}") % 100).zfill(2)
        
        return f"{prefix}{sequence}{check_digits}"


class TransactionCRUD(CRUDBase[Transaction, dict, dict]):
    """Transaction CRUD operations with financial compliance."""

    def get_transactions_by_account(
        self,
        db: Session,
        account_id: int,
        pagination: PaginationParams,
        transaction_type: Optional[TransactionType] = None,
        status: Optional[TransactionStatus] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Tuple[List[Transaction], int]:
        """Get paginated transactions for an account."""
        query = db.query(Transaction).filter(
            or_(
                Transaction.from_account_id == account_id,
                Transaction.to_account_id == account_id
            )
        )
        
        if transaction_type:
            query = query.filter(Transaction.transaction_type == transaction_type)
        
        if status:
            query = query.filter(Transaction.status == status)
        
        if start_date:
            query = query.filter(Transaction.initiated_at >= start_date)
        
        if end_date:
            query = query.filter(Transaction.initiated_at <= end_date)
        
        # Get total count
        total = query.count()
        
        # Apply pagination and ordering
        transactions = query.order_by(Transaction.initiated_at.desc()).offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return transactions, total

    def get_pending_transactions(
        self,
        db: Session,
        limit: int = 100,
        older_than_minutes: int = 5
    ) -> List[Transaction]:
        """Get transactions that are pending for too long."""
        cutoff_time = datetime.utcnow() - timedelta(minutes=older_than_minutes)
        
        return db.query(Transaction).filter(
            and_(
                Transaction.status == TransactionStatus.PENDING,
                Transaction.initiated_at < cutoff_time
            )
        ).order_by(Transaction.initiated_at.asc()).limit(limit).all()

    def get_failed_transactions(
        self,
        db: Session,
        pagination: PaginationParams,
        start_date: Optional[datetime] = None
    ) -> Tuple[List[Transaction], int]:
        """Get failed transactions for analysis."""
        query = db.query(Transaction).filter(Transaction.status == TransactionStatus.FAILED)
        
        if start_date:
            query = query.filter(Transaction.initiated_at >= start_date)
        
        total = query.count()
        
        transactions = query.order_by(Transaction.initiated_at.desc()).offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return transactions, total

    def get_transaction_analytics(
        self,
        db: Session,
        start_date: datetime,
        end_date: datetime,
        account_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get transaction analytics for a period."""
        query = db.query(Transaction).filter(
            and_(
                Transaction.completed_at >= start_date,
                Transaction.completed_at <= end_date,
                Transaction.status == TransactionStatus.COMPLETED
            )
        )
        
        if account_id:
            query = query.filter(
                or_(
                    Transaction.from_account_id == account_id,
                    Transaction.to_account_id == account_id
                )
            )
        
        transactions = query.all()
        
        # Calculate analytics
        total_amount = sum(tx.amount for tx in transactions)
        total_fees = sum(tx.fee_amount for tx in transactions)
        
        by_type = {}
        for tx in transactions:
            tx_type = tx.transaction_type.value
            if tx_type not in by_type:
                by_type[tx_type] = {'count': 0, 'total_amount': Decimal('0')}
            by_type[tx_type]['count'] += 1
            by_type[tx_type]['total_amount'] += tx.amount
        
        # Calculate processing times
        processing_times = [
            tx.processing_time.total_seconds()
            for tx in transactions 
            if tx.processing_time
        ]
        
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        return {
            'total_transactions': len(transactions),
            'total_amount': total_amount,
            'total_fees': total_fees,
            'average_amount': total_amount / len(transactions) if transactions else Decimal('0'),
            'by_transaction_type': by_type,
            'average_processing_time_seconds': avg_processing_time,
            'success_rate': len(transactions) / max(1, query.count()) * 100
        }

    def search_transactions(
        self,
        db: Session,
        search_term: str,
        pagination: PaginationParams,
        user_id: Optional[int] = None
    ) -> Tuple[List[Transaction], int]:
        """Search transactions by reference, description, or transaction ID."""
        query = db.query(Transaction).filter(
            or_(
                Transaction.reference.ilike(f'%{search_term}%'),
                Transaction.description.ilike(f'%{search_term}%'),
                Transaction.transaction_id.like(f'%{search_term}%')
            )
        )
        
        if user_id:
            # Filter by user's accounts
            user_accounts = db.query(Account.id).filter(Account.owner_id == user_id).subquery()
            query = query.filter(
                or_(
                    Transaction.from_account_id.in_(user_accounts),
                    Transaction.to_account_id.in_(user_accounts)
                )
            )
        
        total = query.count()
        
        transactions = query.order_by(Transaction.initiated_at.desc()).offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return transactions, total


class TransactionLogCRUD(CRUDBase[TransactionLog, dict, dict]):
    """Transaction log CRUD for audit trail."""

    def get_transaction_audit_trail(
        self,
        db: Session,
        transaction_id: int
    ) -> List[TransactionLog]:
        """Get complete audit trail for a transaction."""
        return db.query(TransactionLog).filter(
            TransactionLog.transaction_id == transaction_id
        ).order_by(TransactionLog.changed_at.asc()).all()

    def log_transaction_change(
        self,
        db: Session,
        transaction_id: int,
        from_status: TransactionStatus,
        to_status: TransactionStatus,
        changed_by_id: int,
        reason: str = None,
        metadata: Dict = None
    ) -> TransactionLog:
        """Create audit log entry for transaction status change."""
        log_entry = TransactionLog(
            transaction_id=transaction_id,
            from_status=from_status,
            to_status=to_status,
            changed_by_id=changed_by_id,
            reason=reason,
            metadata=metadata or {}
        )
        
        db.add(log_entry)
        db.flush()
        
        return log_entry

    def get_user_transaction_history(
        self,
        db: Session,
        user_id: int,
        pagination: PaginationParams
    ) -> Tuple[List[TransactionLog], int]:
        """Get transaction history initiated by a user."""
        query = db.query(TransactionLog).filter(TransactionLog.changed_by_id == user_id)
        
        total = query.count()
        
        logs = query.order_by(TransactionLog.changed_at.desc()).offset(
            (pagination.page - 1) * pagination.size
        ).limit(pagination.size).all()
        
        return logs, total


class BalanceSnapshotCRUD(CRUDBase[BalanceSnapshot, dict, dict]):
    """Balance snapshot CRUD for historical analysis."""

    def create_daily_snapshots(
        self,
        db: Session,
        target_date: datetime = None
    ) -> int:
        """Create balance snapshots for all accounts for a specific date."""
        if not target_date:
            target_date = datetime.utcnow().date()
        
        # Get all active accounts
        accounts = db.query(Account).filter(Account.is_active == True).all()
        snapshots_created = 0
        
        for account in accounts:
            # Check if snapshot already exists
            existing = db.query(BalanceSnapshot).filter(
                and_(
                    BalanceSnapshot.account_id == account.id,
                    func.date(BalanceSnapshot.snapshot_date) == target_date
                )
            ).first()
            
            if existing:
                continue
            
            # Calculate daily statistics
            daily_stats = self._calculate_daily_stats(db, account, target_date)
            
            snapshot = BalanceSnapshot(
                account_id=account.id,
                snapshot_date=target_date,
                **daily_stats
            )
            
            db.add(snapshot)
            snapshots_created += 1
        
        db.commit()
        return snapshots_created

    def get_balance_trends(
        self,
        db: Session,
        account_id: int,
        days: int = 30
    ) -> List[Dict[str, Any]]:
        """Get balance trends for an account."""
        start_date = datetime.utcnow().date() - timedelta(days=days)
        
        snapshots = db.query(BalanceSnapshot).filter(
            and_(
                BalanceSnapshot.account_id == account_id,
                BalanceSnapshot.snapshot_date >= start_date
            )
        ).order_by(BalanceSnapshot.snapshot_date.asc()).all()
        
        trends = []
        for snapshot in snapshots:
            trends.append({
                'date': snapshot.snapshot_date.isoformat(),
                'opening_balance': float(snapshot.opening_balance),
                'closing_balance': float(snapshot.closing_balance),
                'total_debits': float(snapshot.total_debits),
                'total_credits': float(snapshot.total_credits),
                'transaction_count': snapshot.transaction_count,
                'net_change': float(snapshot.closing_balance - snapshot.opening_balance)
            })
        
        return trends

    def _calculate_daily_stats(
        self,
        db: Session,
        account: Account,
        target_date: datetime
    ) -> Dict[str, Any]:
        """Calculate daily transaction statistics for an account."""
        # Get transactions for the day
        start_of_day = datetime.combine(target_date, datetime.min.time())
        end_of_day = start_of_day + timedelta(days=1)
        
        daily_transactions = db.query(Transaction).filter(
            and_(
                or_(
                    Transaction.from_account_id == account.id,
                    Transaction.to_account_id == account.id
                ),
                Transaction.completed_at >= start_of_day,
                Transaction.completed_at < end_of_day,
                Transaction.status == TransactionStatus.COMPLETED
            )
        ).all()
        
        # Calculate debits and credits
        total_debits = sum(
            tx.amount for tx in daily_transactions
            if tx.from_account_id == account.id
        )
        
        total_credits = sum(
            tx.amount for tx in daily_transactions
            if tx.to_account_id == account.id
        )
        
        # Get opening balance (previous day's closing balance)
        previous_date = target_date - timedelta(days=1)
        previous_snapshot = db.query(BalanceSnapshot).filter(
            and_(
                BalanceSnapshot.account_id == account.id,
                func.date(BalanceSnapshot.snapshot_date) == previous_date
            )
        ).first()
        
        opening_balance = previous_snapshot.closing_balance if previous_snapshot else Decimal('0')
        closing_balance = opening_balance + total_credits - total_debits
        
        return {
            'opening_balance': opening_balance,
            'closing_balance': closing_balance,
            'total_debits': total_debits,
            'total_credits': total_credits,
            'transaction_count': len(daily_transactions),
            'average_balance': (opening_balance + closing_balance) / 2,
            'minimum_balance': min(opening_balance, closing_balance),
            'maximum_balance': max(opening_balance, closing_balance)
        }


# Create CRUD instances
account_crud = AccountCRUD(Account)
transaction_crud = TransactionCRUD(Transaction)
transaction_log_crud = TransactionLogCRUD(TransactionLog)
balance_snapshot_crud = BalanceSnapshotCRUD(BalanceSnapshot)
```

### 5.4 Transaction API Schemas

**app/schemas/transactions.py** - Transaction Request/Response Schemas:
```python
"""
Pydantic schemas for transaction management API.
Includes account management, transaction processing, and financial reporting.
"""

from datetime import datetime
from decimal import Decimal
from typing import Optional, List, Dict, Any
from uuid import UUID

from pydantic import BaseModel, Field, validator

from app.models.transactions import (
    TransactionType, TransactionStatus, Currency, AccountType
)
from app.schemas.common import BaseSchema, TimestampMixin, PaginatedResponse


# Account Schemas
class AccountBase(BaseSchema):
    """Base account schema."""
    account_name: str = Field(..., min_length=1, max_length=100)
    account_type: AccountType
    currency: Currency = Currency.USD
    minimum_balance: Decimal = Field(default=Decimal('0'), ge=0)
    maximum_balance: Optional[Decimal] = Field(None, gt=0)
    daily_withdrawal_limit: Optional[Decimal] = Field(None, gt=0)
    monthly_withdrawal_limit: Optional[Decimal] = Field(None, gt=0)


class AccountCreate(AccountBase):
    """Schema for account creation."""
    initial_balance: Decimal = Field(default=Decimal('0'), ge=0)
    
    @validator('maximum_balance')
    def validate_maximum_balance(cls, v, values):
        """Ensure maximum balance is greater than minimum balance."""
        if v and 'minimum_balance' in values and v <= values['minimum_balance']:
            raise ValueError('Maximum balance must be greater than minimum balance')
        return v


class AccountUpdate(BaseSchema):
    """Schema for account updates."""
    account_name: Optional[str] = Field(None, min_length=1, max_length=100)
    minimum_balance: Optional[Decimal] = Field(None, ge=0)
    maximum_balance: Optional[Decimal] = Field(None, gt=0)
    daily_withdrawal_limit: Optional[Decimal] = Field(None, gt=0)
    monthly_withdrawal_limit: Optional[Decimal] = Field(None, gt=0)
    is_active: Optional[bool] = None


class AccountResponse(AccountBase, TimestampMixin):
    """Account response schema."""
    id: int
    account_number: str
    balance: Decimal
    available_balance: Decimal
    reserved_balance: Decimal
    is_active: bool
    is_frozen: bool
    freeze_reason: Optional[str] = None
    owner_id: int
    opened_at: datetime
    last_activity_at: Optional[datetime] = None
    metadata: Dict[str, Any] = {}


class AccountSummaryResponse(BaseSchema):
    """Account summary for listings."""
    id: int
    account_number: str
    account_name: str
    account_type: AccountType
    balance: Decimal
    currency: Currency
    is_active: bool
    is_frozen: bool


# Transaction Schemas
class TransactionBase(BaseSchema):
    """Base transaction schema."""
    amount: Decimal = Field(..., gt=0, decimal_places=2)
    currency: Currency = Currency.USD
    description: Optional[str] = Field(None, max_length=500)
    reference: Optional[str] = Field(None, max_length=100)
    metadata: Dict[str, Any] = {}


class TransferRequest(TransactionBase):
    """Schema for transfer requests."""
    from_account_id: int = Field(..., gt=0)
    to_account_id: int = Field(..., gt=0)
    
    @validator('to_account_id')
    def validate_different_accounts(cls, v, values):
        """Ensure transfer is between different accounts."""
        if 'from_account_id' in values and v == values['from_account_id']:
            raise ValueError('Cannot transfer to the same account')
        return v


class DepositRequest(TransactionBase):
    """Schema for deposit requests."""
    account_id: int = Field(..., gt=0)


class WithdrawalRequest(TransactionBase):
    """Schema for withdrawal requests."""
    account_id: int = Field(..., gt=0)


class TransactionResponse(TransactionBase, TimestampMixin):
    """Transaction response schema."""
    id: int
    transaction_id: UUID
    transaction_type: TransactionType
    status: TransactionStatus
    fee_amount: Decimal
    exchange_rate: Decimal
    from_account_id: Optional[int] = None
    to_account_id: Optional[int] = None
    initiated_by_id: int
    approved_by_id: Optional[int] = None
    initiated_at: datetime
    processed_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_code: Optional[str] = None
    error_message: Optional[str] = None
    retry_count: int
    batch_id: Optional[str] = None


class TransactionWithAccountsResponse(TransactionResponse):
    """Transaction response with account details."""
    from_account: Optional[AccountSummaryResponse] = None
    to_account: Optional[AccountSummaryResponse] = None


# Batch Processing Schemas
class BatchTransactionRequest(BaseSchema):
    """Schema for batch transaction processing."""
    transactions: List[Dict[str, Any]] = Field(..., min_items=1, max_items=100)
    batch_description: Optional[str] = Field(None, max_length=200)


class BatchTransactionResponse(BaseSchema):
    """Response for batch transaction processing."""
    batch_id: str
    total_transactions: int
    successful_transactions: int
    failed_transactions: int
    total_amount: Decimal
    processing_time_seconds: float
    transactions: List[TransactionResponse]


# Account Management Schemas
class AccountFreezeRequest(BaseSchema):
    """Schema for freezing an account."""
    reason: str = Field(..., min_length=1, max_length=500)


class AccountLimitsUpdate(BaseSchema):
    """Schema for updating account limits."""
    daily_withdrawal_limit: Optional[Decimal] = Field(None, gt=0)
    monthly_withdrawal_limit: Optional[Decimal] = Field(None, gt=0)
    minimum_balance: Optional[Decimal] = Field(None, ge=0)
    maximum_balance: Optional[Decimal] = Field(None, gt=0)


# Analytics Schemas
class TransactionAnalyticsRequest(BaseSchema):
    """Request schema for transaction analytics."""
    start_date: datetime
    end_date: datetime
    account_id: Optional[int] = Field(None, gt=0)
    
    @validator('end_date')
    def validate_date_range(cls, v, values):
        """Ensure end date is after start date."""
        if 'start_date' in values and v <= values['start_date']:
            raise ValueError('End date must be after start date')
        return v


class TransactionAnalyticsResponse(BaseSchema):
    """Response schema for transaction analytics."""
    total_transactions: int
    total_amount: Decimal
    total_fees: Decimal
    average_amount: Decimal
    by_transaction_type: Dict[str, Dict[str, Any]]
    average_processing_time_seconds: float
    success_rate: float


class BalanceTrendsResponse(BaseSchema):
    """Response schema for balance trends."""
    account_id: int
    period_days: int
    trends: List[Dict[str, Any]]
    summary: Dict[str, Any]


class AccountSummaryStatsResponse(BaseSchema):
    """Response schema for account summary statistics."""
    total_accounts: int
    total_balance: Decimal
    total_available_balance: Decimal
    total_reserved_balance: Decimal
    active_accounts: int
    frozen_accounts: int
    by_account_type: Dict[str, Dict[str, Any]]


# Transaction Filters
class TransactionFilterParams(BaseSchema):
    """Filter parameters for transaction queries."""
    transaction_type: Optional[TransactionType] = None
    status: Optional[TransactionStatus] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    min_amount: Optional[Decimal] = Field(None, gt=0)
    max_amount: Optional[Decimal] = Field(None, gt=0)
    account_id: Optional[int] = Field(None, gt=0)
    reference: Optional[str] = Field(None, max_length=100)


# Paginated Responses
AccountsPaginatedResponse = PaginatedResponse[AccountSummaryResponse]
TransactionsPaginatedResponse = PaginatedResponse[TransactionResponse]
TransactionsWithAccountsPaginatedResponse = PaginatedResponse[TransactionWithAccountsResponse]
```


### 5.5 Transaction API Endpoints

**app/api/v1/transactions.py** - Transaction Management APIs:
```python
"""
Transaction management API endpoints with comprehensive financial operations.
Demonstrates advanced transaction patterns, batch processing, and financial compliance.
"""

import logging
from datetime import datetime, timedelta
from decimal import Decimal
from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import get_current_user, get_current_admin, get_pagination_params
from app.services.transaction_service import TransactionProcessor
from app.crud.transactions import account_crud, transaction_crud, transaction_log_crud
from app.models.user import User
from app.models.transactions import AccountType, TransactionType, TransactionStatus
from app.schemas.transactions import (
    AccountCreate, AccountUpdate, AccountResponse, AccountSummaryResponse,
    TransferRequest, DepositRequest, WithdrawalRequest,
    TransactionResponse, TransactionWithAccountsResponse,
    BatchTransactionRequest, BatchTransactionResponse,
    AccountFreezeRequest, TransactionAnalyticsRequest, TransactionAnalyticsResponse,
    AccountsPaginatedResponse, TransactionsPaginatedResponse,
    BalanceTrendsResponse, AccountSummaryStatsResponse
)
from app.schemas.common import APIResponse, PaginationParams

router = APIRouter(prefix="/transactions", tags=["transactions"])
logger = logging.getLogger(__name__)


# Account Management Endpoints
@router.post("/accounts", response_model=APIResponse[AccountResponse])
async def create_account(
    account_data: AccountCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create a new financial account.
    
    - **account_name**: Display name for the account
    - **account_type**: Type of account (checking, savings, etc.)
    - **currency**: Account currency (default: USD)
    - **initial_balance**: Starting balance (default: 0)
    - **minimum_balance**: Minimum allowed balance
    - **daily_withdrawal_limit**: Daily withdrawal limit
    """
    try:
        processor = TransactionProcessor(db)
        account = processor.create_account(
            owner_id=current_user.id,
            **account_data.dict()
        )
        db.commit()
        
        return APIResponse(
            success=True,
            data=account,
            message="Account created successfully"
        )
    except Exception as e:
        db.rollback()
        logger.error(f"Account creation failed for user {current_user.id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Account creation failed: {str(e)}"
        )


@router.get("/accounts", response_model=AccountsPaginatedResponse)
async def get_user_accounts(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    pagination: PaginationParams = Depends(get_pagination_params),
    account_type: Optional[AccountType] = Query(None),
    active_only: bool = Query(True)
):
    """Get user's accounts with pagination and filtering."""
    accounts = account_crud.get_user_accounts(
        db=db,
        user_id=current_user.id,
        account_type=account_type,
        active_only=active_only
    )
    
    # Calculate pagination
    total = len(accounts)
    start = (pagination.page - 1) * pagination.size
    end = start + pagination.size
    paginated_accounts = accounts[start:end]
    
    return AccountsPaginatedResponse(
        items=paginated_accounts,
        total=total,
        page=pagination.page,
        size=pagination.size,
        pages=(total + pagination.size - 1) // pagination.size
    )


@router.get("/accounts/{account_id}", response_model=APIResponse[AccountResponse])
async def get_account(
    account_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get specific account details (user can only access their own accounts)."""
    account = account_crud.get(db, id=account_id)
    
    if not account:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Account not found"
        )
    
    # Check ownership unless user is admin
    if account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied to this account"
        )
    
    return APIResponse(
        success=True,
        data=account,
        message="Account retrieved successfully"
    )


@router.put("/accounts/{account_id}", response_model=APIResponse[AccountResponse])
async def update_account(
    account_id: int,
    account_data: AccountUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Update account settings (owner or admin only)."""
    account = account_crud.get(db, id=account_id)
    
    if not account:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Account not found"
        )
    
    # Check ownership unless user is admin
    if account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied to this account"
        )
    
    try:
        updated_account = account_crud.update(
            db, db_obj=account, obj_in=account_data.dict(exclude_unset=True)
        )
        db.commit()
        
        return APIResponse(
            success=True,
            data=updated_account,
            message="Account updated successfully"
        )
    except Exception as e:
        db.rollback()
        logger.error(f"Account update failed for account {account_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Account update failed: {str(e)}"
        )


# Transaction Processing Endpoints
@router.post("/transfer", response_model=APIResponse[TransactionResponse])
async def process_transfer(
    transfer_data: TransferRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Process a transfer between two accounts.
    
    Requires user to own the source account or have admin privileges.
    """
    try:
        # Verify user owns the source account
        from_account = account_crud.get(db, id=transfer_data.from_account_id)
        if not from_account:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Source account not found"
            )
        
        if from_account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied to source account"
            )
        
        processor = TransactionProcessor(db)
        transaction = processor.process_transfer(
            from_account_id=transfer_data.from_account_id,
            to_account_id=transfer_data.to_account_id,
            amount=transfer_data.amount,
            description=transfer_data.description,
            initiated_by_id=current_user.id,
            reference=transfer_data.reference,
            metadata=transfer_data.metadata
        )
        
        db.commit()
        
        return APIResponse(
            success=True,
            data=transaction,
            message=f"Transfer of {transfer_data.amount} processed successfully"
        )
        
    except Exception as e:
        db.rollback()
        logger.error(f"Transfer failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.post("/deposit", response_model=APIResponse[TransactionResponse])
async def process_deposit(
    deposit_data: DepositRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Process a deposit to an account."""
    try:
        # Verify user owns the account
        account = account_crud.get(db, id=deposit_data.account_id)
        if not account:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Account not found"
            )
        
        if account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied to this account"
            )
        
        processor = TransactionProcessor(db)
        transaction = processor.process_deposit(
            account_id=deposit_data.account_id,
            amount=deposit_data.amount,
            description=deposit_data.description,
            initiated_by_id=current_user.id,
            reference=deposit_data.reference,
            metadata=deposit_data.metadata
        )
        
        db.commit()
        
        return APIResponse(
            success=True,
            data=transaction,
            message=f"Deposit of {deposit_data.amount} processed successfully"
        )
        
    except Exception as e:
        db.rollback()
        logger.error(f"Deposit failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.post("/withdrawal", response_model=APIResponse[TransactionResponse])
async def process_withdrawal(
    withdrawal_data: WithdrawalRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Process a withdrawal from an account."""
    try:
        # Verify user owns the account
        account = account_crud.get(db, id=withdrawal_data.account_id)
        if not account:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Account not found"
            )
        
        if account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied to this account"
            )
        
        processor = TransactionProcessor(db)
        transaction = processor.process_withdrawal(
            account_id=withdrawal_data.account_id,
            amount=withdrawal_data.amount,
            description=withdrawal_data.description,
            initiated_by_id=current_user.id,
            reference=withdrawal_data.reference,
            metadata=withdrawal_data.metadata
        )
        
        db.commit()
        
        return APIResponse(
            success=True,
            data=transaction,
            message=f"Withdrawal of {withdrawal_data.amount} processed successfully"
        )
        
    except Exception as e:
        db.rollback()
        logger.error(f"Withdrawal failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.post("/batch", response_model=APIResponse[BatchTransactionResponse])
async def process_batch_transactions(
    batch_data: BatchTransactionRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)  # Admin only for batch processing
):
    """
    Process multiple transactions in a batch (Admin only).
    
    All transactions in the batch will succeed or fail together.
    Large batches are processed in the background.
    """
    try:
        start_time = datetime.utcnow()
        processor = TransactionProcessor(db)
        
        # Add initiated_by_id to all transactions
        for tx_data in batch_data.transactions:
            tx_data['initiated_by_id'] = current_user.id
        
        # Process batch
        if len(batch_data.transactions) > 50:
            # Large batch - process in background
            background_tasks.add_task(
                _process_large_batch,
                batch_data.transactions,
                current_user.id
            )
            
            return APIResponse(
                success=True,
                data={
                    'batch_id': 'pending',
                    'status': 'processing_in_background',
                    'total_transactions': len(batch_data.transactions)
                },
                message="Large batch queued for background processing"
            )
        else:
            # Small batch - process immediately
            transactions = processor.batch_process_transactions(batch_data.transactions)
            db.commit()
            
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            return APIResponse(
                success=True,
                data=BatchTransactionResponse(
                    batch_id=transactions[0].batch_id,
                    total_transactions=len(transactions),
                    successful_transactions=len([tx for tx in transactions if tx.status == TransactionStatus.COMPLETED]),
                    failed_transactions=len([tx for tx in transactions if tx.status == TransactionStatus.FAILED]),
                    total_amount=sum(tx.amount for tx in transactions),
                    processing_time_seconds=processing_time,
                    transactions=transactions
                ),
                message="Batch processed successfully"
            )
            
    except Exception as e:
        db.rollback()
        logger.error(f"Batch processing failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Batch processing failed: {str(e)}"
        )


# Transaction Query Endpoints
@router.get("/", response_model=TransactionsPaginatedResponse)
async def get_transactions(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    pagination: PaginationParams = Depends(get_pagination_params),
    account_id: Optional[int] = Query(None, gt=0),
    transaction_type: Optional[TransactionType] = Query(None),
    status: Optional[TransactionStatus] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None)
):
    """Get user's transactions with filtering and pagination."""
    
    # If account_id specified, verify user owns it
    if account_id:
        account = account_crud.get(db, id=account_id)
        if not account or (account.owner_id != current_user.id and current_user.role != UserRole.ADMIN):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied to this account"
            )
    
    # Get user's account IDs if not admin
    if current_user.role != UserRole.ADMIN and not account_id:
        user_accounts = account_crud.get_user_accounts(db, current_user.id)
        account_ids = [acc.id for acc in user_accounts]
    else:
        account_ids = [account_id] if account_id else None
    
    # Build query based on user permissions
    if current_user.role == UserRole.ADMIN and not account_id:
        # Admin can see all transactions
        transactions, total = transaction_crud.get_multi_with_filters(
            db=db,
            pagination=pagination,
            filters={
                'transaction_type': transaction_type,
                'status': status,
                'start_date': start_date,
                'end_date': end_date
            }
        )
    else:
        # Regular users see only their account transactions
        transactions, total = transaction_crud.get_transactions_by_account(
            db=db,
            account_id=account_ids[0] if account_ids else 0,
            pagination=pagination,
            transaction_type=transaction_type,
            status=status,
            start_date=start_date,
            end_date=end_date
        )
    
    return TransactionsPaginatedResponse(
        items=transactions,
        total=total,
        page=pagination.page,
        size=pagination.size,
        pages=(total + pagination.size - 1) // pagination.size
    )


@router.get("/{transaction_id}", response_model=APIResponse[TransactionWithAccountsResponse])
async def get_transaction(
    transaction_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get specific transaction details with account information."""
    transaction = transaction_crud.get(db, id=transaction_id)
    
    if not transaction:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Transaction not found"
        )
    
    # Check access permissions
    user_has_access = False
    if current_user.role == UserRole.ADMIN:
        user_has_access = True
    else:
        # Check if user owns any involved account
        if transaction.from_account and transaction.from_account.owner_id == current_user.id:
            user_has_access = True
        if transaction.to_account and transaction.to_account.owner_id == current_user.id:
            user_has_access = True
    
    if not user_has_access:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied to this transaction"
        )
    
    return APIResponse(
        success=True,
        data=transaction,
        message="Transaction retrieved successfully"
    )


@router.post("/{transaction_id}/reverse", response_model=APIResponse[TransactionResponse])
async def reverse_transaction(
    transaction_id: int,
    reason: str = Query(..., min_length=1, max_length=500),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)  # Admin only
):
    """Reverse a completed transaction (Admin only)."""
    try:
        processor = TransactionProcessor(db)
        reversal_transaction = processor.reverse_transaction(
            transaction_id=transaction_id,
            reason=reason,
            reversed_by_id=current_user.id
        )
        
        db.commit()
        
        return APIResponse(
            success=True,
            data=reversal_transaction,
            message=f"Transaction {transaction_id} reversed successfully"
        )
        
    except Exception as e:
        db.rollback()
        logger.error(f"Transaction reversal failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Account Administration Endpoints
@router.post("/accounts/{account_id}/freeze", response_model=APIResponse[AccountResponse])
async def freeze_account(
    account_id: int,
    freeze_data: AccountFreezeRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)  # Admin only
):
    """Freeze an account (Admin only)."""
    try:
        account = account_crud.freeze_account(
            db=db,
            account_id=account_id,
            reason=freeze_data.reason,
            frozen_by_id=current_user.id
        )
        
        return APIResponse(
            success=True,
            data=account,
            message=f"Account {account.account_number} frozen successfully"
        )
        
    except Exception as e:
        logger.error(f"Account freeze failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/accounts/{account_id}/freeze", response_model=APIResponse[AccountResponse])
async def unfreeze_account(
    account_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)  # Admin only
):
    """Unfreeze an account (Admin only)."""
    try:
        account = account_crud.unfreeze_account(
            db=db,
            account_id=account_id,
            unfrozen_by_id=current_user.id
        )
        
        return APIResponse(
            success=True,
            data=account,
            message=f"Account {account.account_number} unfrozen successfully"
        )
        
    except Exception as e:
        logger.error(f"Account unfreeze failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


# Analytics and Reporting Endpoints
@router.post("/analytics", response_model=APIResponse[TransactionAnalyticsResponse])
async def get_transaction_analytics(
    analytics_request: TransactionAnalyticsRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get transaction analytics for a date range."""
    
    # If account_id specified, verify access
    if analytics_request.account_id:
        account = account_crud.get(db, id=analytics_request.account_id)
        if not account:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Account not found"
            )
        
        if account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied to this account"
            )
    
    analytics = transaction_crud.get_transaction_analytics(
        db=db,
        start_date=analytics_request.start_date,
        end_date=analytics_request.end_date,
        account_id=analytics_request.account_id
    )
    
    return APIResponse(
        success=True,
        data=analytics,
        message="Analytics retrieved successfully"
    )


@router.get("/accounts/{account_id}/balance-trends", response_model=APIResponse[BalanceTrendsResponse])
async def get_balance_trends(
    account_id: int,
    days: int = Query(30, ge=1, le=365),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get balance trends for an account."""
    # Verify account access
    account = account_crud.get(db, id=account_id)
    if not account:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Account not found"
        )
    
    if account.owner_id != current_user.id and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied to this account"
        )
    
    trends = balance_snapshot_crud.get_balance_trends(
        db=db,
        account_id=account_id,
        days=days
    )
    
    # Calculate summary statistics
    if trends:
        balances = [t['closing_balance'] for t in trends]
        summary = {
            'period_start_balance': trends[0]['opening_balance'],
            'period_end_balance': trends[-1]['closing_balance'],
            'highest_balance': max(balances),
            'lowest_balance': min(balances),
            'average_balance': sum(balances) / len(balances),
            'total_transactions': sum(t['transaction_count'] for t in trends),
            'net_change': trends[-1]['closing_balance'] - trends[0]['opening_balance']
        }
    else:
        summary = {}
    
    return APIResponse(
        success=True,
        data=BalanceTrendsResponse(
            account_id=account_id,
            period_days=days,
            trends=trends,
            summary=summary
        ),
        message="Balance trends retrieved successfully"
    )


@router.get("/admin/summary", response_model=APIResponse[AccountSummaryStatsResponse])
async def get_accounts_summary(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin)  # Admin only
):
    """Get summary statistics for all accounts (Admin only)."""
    summary = account_crud.get_accounts_summary(db)
    
    return APIResponse(
        success=True,
        data=summary,
        message="Account summary retrieved successfully"
    )


@router.get("/admin/pending", response_model=APIResponse[List[TransactionResponse]])
async def get_pending_transactions(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin),  # Admin only
    limit: int = Query(100, ge=1, le=500),
    older_than_minutes: int = Query(5, ge=1, le=60)
):
    """Get transactions that have been pending for too long (Admin only)."""
    pending_transactions = transaction_crud.get_pending_transactions(
        db=db,
        limit=limit,
        older_than_minutes=older_than_minutes
    )
    
    return APIResponse(
        success=True,
        data=pending_transactions,
        message=f"Found {len(pending_transactions)} pending transactions"
    )


# Background task for large batch processing
async def _process_large_batch(transactions_data: List[Dict], initiated_by_id: int):
    """Background task for processing large transaction batches."""
    with get_db_session() as db:
        try:
            processor = TransactionProcessor(db)
            
            # Add initiated_by_id to all transactions
            for tx_data in transactions_data:
                tx_data['initiated_by_id'] = initiated_by_id
            
            transactions = processor.batch_process_transactions(transactions_data)
            db.commit()
            
            logger.info(f"Large batch processed successfully: {len(transactions)} transactions")
            
            # Here you could send notification to user about completion
            # notification_service.send_batch_completion_notification(initiated_by_id, len(transactions))
            
        except Exception as e:
            db.rollback()
            logger.error(f"Large batch processing failed: {str(e)}")
            
            # Here you could send notification to user about failure
            # notification_service.send_batch_failure_notification(initiated_by_id, str(e))
```


### 5.6 Advanced Transaction Patterns

**app/services/advanced_transactions.py** - Complex Transaction Patterns:
```python
"""
Advanced transaction patterns demonstrating sophisticated SQLAlchemy usage:
- Two-phase commit protocols
- Distributed transactions across multiple databases
- Compensation patterns for saga transactions
- Event sourcing with transaction logs
"""

import asyncio
import logging
import uuid
from contextlib import contextmanager, asynccontextmanager
from datetime import datetime, timedelta
from decimal import Decimal
from enum import Enum
from typing import List, Optional, Dict, Any, Callable, Union

from sqlalchemy import create_engine, text, event
from sqlalchemy.orm import Session, sessionmaker
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.engine import Engine

from app.database import engine, async_engine, get_db_session
from app.models.transactions import Transaction, Account, TransactionStatus, TransactionType
from app.services.transaction_service import TransactionProcessor
from app.core.exceptions import TransactionError, CompensationError

logger = logging.getLogger(__name__)


class TwoPhaseCommitState(Enum):
    """States for two-phase commit protocol."""
    PREPARING = "preparing"
    PREPARED = "prepared" 
    COMMITTING = "committing"
    COMMITTED = "committed"
    ABORTING = "aborting"
    ABORTED = "aborted"


class DistributedTransaction:
    """
    Two-phase commit implementation for distributed transactions.
    Ensures ACID properties across multiple database operations.
    """

    def __init__(self, transaction_id: str = None):
        self.transaction_id = transaction_id or str(uuid.uuid4())
        self.participants: List[Dict] = []
        self.state = TwoPhaseCommitState.PREPARING
        self.coordinator_session: Optional[Session] = None

    def add_participant(
        self, 
        session: Session, 
        operation: Callable,
        compensation: Optional[Callable] = None,
        **kwargs
    ):
        """Add a participant to the distributed transaction."""
        participant = {
            'id': str(uuid.uuid4()),
            'session': session,
            'operation': operation,
            'compensation': compensation,
            'kwargs': kwargs,
            'prepared': False,
            'committed': False,
            'error': None
        }
        self.participants.append(participant)
        logger.debug(f"Added participant {participant['id']} to transaction {self.transaction_id}")

    @contextmanager
    def execute(self):
        """Execute distributed transaction with two-phase commit."""
        self.coordinator_session = Session(bind=engine)
        
        try:
            # Phase 1: Prepare all participants
            logger.info(f"Starting two-phase commit for transaction {self.transaction_id}")
            self._prepare_phase()
            
            if self.state == TwoPhaseCommitState.PREPARED:
                # Phase 2: Commit all participants
                self._commit_phase()
                yield self
            else:
                # Abort if preparation failed
                self._abort_phase()
                raise TransactionError("Two-phase commit preparation failed")
                
        except Exception as e:
            logger.error(f"Distributed transaction {self.transaction_id} failed: {str(e)}")
            self._abort_phase()
            raise
        finally:
            if self.coordinator_session:
                self.coordinator_session.close()

    def _prepare_phase(self):
        """Phase 1: Prepare all participants."""
        self.state = TwoPhaseCommitState.PREPARING
        
        for participant in self.participants:
            try:
                # Execute the operation but don't commit
                participant['session'].begin()
                result = participant['operation'](participant['session'], **participant['kwargs'])
                participant['result'] = result
                participant['prepared'] = True
                
                logger.debug(f"Participant {participant['id']} prepared successfully")
                
            except Exception as e:
                participant['error'] = str(e)
                logger.error(f"Participant {participant['id']} preparation failed: {str(e)}")
                self.state = TwoPhaseCommitState.ABORTING
                return
        
        # All participants prepared successfully
        self.state = TwoPhaseCommitState.PREPARED
        logger.info(f"All participants prepared for transaction {self.transaction_id}")

    def _commit_phase(self):
        """Phase 2: Commit all participants."""
        self.state = TwoPhaseCommitState.COMMITTING
        
        for participant in self.participants:
            try:
                participant['session'].commit()
                participant['committed'] = True
                logger.debug(f"Participant {participant['id']} committed successfully")
                
            except Exception as e:
                participant['error'] = str(e)
                logger.error(f"Participant {participant['id']} commit failed: {str(e)}")
                
                # In a real implementation, this would require manual intervention
                # or retry logic as we're in an inconsistent state
                self.state = TwoPhaseCommitState.ABORTING
                self._run_compensations()
                raise TransactionError(f"Commit phase failed for participant {participant['id']}")
        
        self.state = TwoPhaseCommitState.COMMITTED
        logger.info(f"Distributed transaction {self.transaction_id} committed successfully")

    def _abort_phase(self):
        """Abort phase: Rollback all participants."""
        self.state = TwoPhaseCommitState.ABORTING
        
        for participant in self.participants:
            try:
                if participant['prepared'] and not participant['committed']:
                    participant['session'].rollback()
                    logger.debug(f"Participant {participant['id']} rolled back")
            except Exception as e:
                logger.error(f"Rollback failed for participant {participant['id']}: {str(e)}")
        
        self.state = TwoPhaseCommitState.ABORTED
        logger.info(f"Distributed transaction {self.transaction_id} aborted")

    def _run_compensations(self):
        """Run compensation actions for committed participants."""
        for participant in self.participants:
            if participant['committed'] and participant['compensation']:
                try:
                    participant['compensation'](participant['session'], participant['result'])
                    logger.info(f"Compensation executed for participant {participant['id']}")
                except Exception as e:
                    logger.error(f"Compensation failed for participant {participant['id']}: {str(e)}")


class SagaOrchestrator:
    """
    Saga pattern implementation for long-running business transactions.
    Manages complex workflows with compensation-based consistency.
    """

    def __init__(self, saga_id: str = None):
        self.saga_id = saga_id or str(uuid.uuid4())
        self.steps: List[Dict] = []
        self.current_step = 0
        self.compensation_stack: List = []

    def add_step(
        self,
        name: str,
        action: Callable,
        compensation: Callable,
        **kwargs
    ):
        """Add a step to the saga workflow."""
        step = {
            'name': name,
            'action': action,
            'compensation': compensation,
            'kwargs': kwargs,
            'executed': False,
            'compensated': False,
            'result': None,
            'error': None
        }
        self.steps.append(step)

    async def execute(self) -> Dict[str, Any]:
        """Execute the saga workflow."""
        logger.info(f"Starting saga {self.saga_id} with {len(self.steps)} steps")
        
        try:
            # Execute all steps in order
            for i, step in enumerate(self.steps):
                self.current_step = i
                logger.info(f"Executing step {i + 1}/{len(self.steps)}: {step['name']}")
                
                try:
                    with get_db_session() as db:
                        result = await step['action'](db, **step['kwargs'])
                        step['result'] = result
                        step['executed'] = True
                        
                        # Add to compensation stack
                        self.compensation_stack.append(step)
                        
                        logger.info(f"Step {step['name']} completed successfully")
                        
                except Exception as e:
                    step['error'] = str(e)
                    logger.error(f"Step {step['name']} failed: {str(e)}")
                    
                    # Run compensations for executed steps
                    await self._run_compensations()
                    
                    raise TransactionError(f"Saga {self.saga_id} failed at step {step['name']}: {str(e)}")
            
            logger.info(f"Saga {self.saga_id} completed successfully")
            return {
                'saga_id': self.saga_id,
                'status': 'completed',
                'steps_executed': len(self.steps),
                'results': [step['result'] for step in self.steps]
            }
            
        except Exception as e:
            logger.error(f"Saga {self.saga_id} failed: {str(e)}")
            return {
                'saga_id': self.saga_id,
                'status': 'failed',
                'error': str(e),
                'steps_executed': self.current_step,
                'compensations_run': len([s for s in self.steps if s.get('compensated')])
            }

    async def _run_compensations(self):
        """Run compensation actions in reverse order."""
        logger.info(f"Running compensations for saga {self.saga_id}")
        
        # Run compensations in reverse order (LIFO)
        for step in reversed(self.compensation_stack):
            if step['executed'] and not step['compensated']:
                try:
                    logger.info(f"Running compensation for step: {step['name']}")
                    
                    with get_db_session() as db:
                        await step['compensation'](db, step['result'])
                        step['compensated'] = True
                        
                    logger.info(f"Compensation for {step['name']} completed")
                    
                except Exception as e:
                    logger.error(f"Compensation failed for {step['name']}: {str(e)}")
                    # In production, this might require manual intervention
                    raise CompensationError(f"Compensation failed for {step['name']}: {str(e)}")


class EventSourcingTransactionManager:
    """
    Event sourcing implementation for financial transactions.
    Maintains complete audit trail with event replay capabilities.
    """

    def __init__(self, db_session: Session):
        self.db = db_session

    def create_event(
        self,
        event_type: str,
        aggregate_id: str,
        data: Dict[str, Any],
        user_id: int,
        version: int = None
    ) -> Dict[str, Any]:
        """Create a new event in the event store."""
        event = {
            'event_id': str(uuid.uuid4()),
            'event_type': event_type,
            'aggregate_id': aggregate_id,
            'aggregate_type': 'Transaction',
            'data': data,
            'user_id': user_id,
            'version': version or 1,
            'timestamp': datetime.utcnow(),
            'metadata': {
                'correlation_id': str(uuid.uuid4()),
                'causation_id': data.get('causation_id')
            }
        }
        
        # In a real implementation, this would be stored in an event store table
        logger.info(f"Created event {event['event_id']} for aggregate {aggregate_id}")
        return event

    def replay_events(
        self,
        aggregate_id: str,
        up_to_version: Optional[int] = None
    ) -> Dict[str, Any]:
        """Replay events to reconstruct aggregate state."""
        # In a real implementation, this would query the event store
        # and apply each event to reconstruct the current state
        events = self._get_events_for_aggregate(aggregate_id, up_to_version)
        
        # Start with empty state
        state = {
            'aggregate_id': aggregate_id,
            'version': 0,
            'balance': Decimal('0'),
            'transaction_history': []
        }
        
        # Apply each event
        for event in events:
            state = self._apply_event(state, event)
        
        return state

    def _get_events_for_aggregate(
        self,
        aggregate_id: str,
        up_to_version: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """Get all events for an aggregate (mock implementation)."""
        # In a real implementation, this would query an events table
        # SELECT * FROM events WHERE aggregate_id = ? AND version <= ? ORDER BY version
        return []

    def _apply_event(self, state: Dict[str, Any], event: Dict[str, Any]) -> Dict[str, Any]:
        """Apply an event to the current state."""
        event_type = event['event_type']
        event_data = event['data']
        
        if event_type == 'AccountCreated':
            state['balance'] = event_data['initial_balance']
        elif event_type == 'FundsDeposited':
            state['balance'] += event_data['amount']
        elif event_type == 'FundsWithdrawn':
            state['balance'] -= event_data['amount']
        elif event_type == 'TransferInitiated':
            if event_data['direction'] == 'outgoing':
                state['balance'] -= event_data['amount']
            else:
                state['balance'] += event_data['amount']
        
        state['version'] = event['version']
        state['transaction_history'].append({
            'event_id': event['event_id'],
            'event_type': event_type,
            'timestamp': event['timestamp'],
            'data': event_data
        })
        
        return state


class TransactionOrchestrator:
    """
    High-level transaction orchestration for complex business processes.
    Combines multiple services and handles cross-cutting concerns.
    """

    def __init__(self):
        self.active_transactions: Dict[str, Dict] = {}

    async def execute_business_transaction(
        self,
        transaction_type: str,
        steps: List[Dict[str, Any]],
        user_id: int,
        metadata: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Execute a complex business transaction using saga pattern."""
        
        business_tx_id = str(uuid.uuid4())
        self.active_transactions[business_tx_id] = {
            'type': transaction_type,
            'status': 'in_progress',
            'started_at': datetime.utcnow(),
            'user_id': user_id,
            'metadata': metadata or {}
        }
        
        try:
            if transaction_type == 'complex_transfer':
                result = await self._execute_complex_transfer(business_tx_id, steps, user_id)
            elif transaction_type == 'batch_payment':
                result = await self._execute_batch_payment(business_tx_id, steps, user_id)
            elif transaction_type == 'account_closure':
                result = await self._execute_account_closure(business_tx_id, steps, user_id)
            else:
                raise ValueError(f"Unsupported business transaction type: {transaction_type}")
            
            self.active_transactions[business_tx_id]['status'] = 'completed'
            self.active_transactions[business_tx_id]['completed_at'] = datetime.utcnow()
            
            return result
            
        except Exception as e:
            self.active_transactions[business_tx_id]['status'] = 'failed'
            self.active_transactions[business_tx_id]['error'] = str(e)
            self.active_transactions[business_tx_id]['failed_at'] = datetime.utcnow()
            
            logger.error(f"Business transaction {business_tx_id} failed: {str(e)}")
            raise

    async def _execute_complex_transfer(
        self,
        business_tx_id: str,
        steps: List[Dict],
        user_id: int
    ) -> Dict[str, Any]:
        """
        Execute complex transfer involving multiple accounts and validation steps.
        Example: Transfer with currency conversion, fees, and compliance checks.
        """
        saga = SagaOrchestrator(business_tx_id)
        
        # Step 1: Validate source account and freeze funds
        saga.add_step(
            name="validate_and_reserve_funds",
            action=self._validate_and_reserve_funds,
            compensation=self._release_reserved_funds,
            **steps[0]
        )
        
        # Step 2: Validate destination account
        saga.add_step(
            name="validate_destination_account",
            action=self._validate_destination_account,
            compensation=self._log_validation_reversal,
            **steps[1]
        )
        
        # Step 3: Apply exchange rate (if different currencies)
        if steps[0].get('currency') != steps[1].get('currency'):
            saga.add_step(
                name="apply_exchange_rate",
                action=self._apply_exchange_rate,
                compensation=self._reverse_exchange_rate,
                **steps[2] if len(steps) > 2 else {}
            )
        
        # Step 4: Execute the actual transfer
        saga.add_step(
            name="execute_transfer",
            action=self._execute_transfer_step,
            compensation=self._reverse_transfer_step,
            source_account_id=steps[0]['account_id'],
            dest_account_id=steps[1]['account_id'],
            amount=steps[0]['amount'],
            user_id=user_id
        )
        
        # Step 5: Apply fees (if any)
        if steps[0].get('apply_fees', False):
            saga.add_step(
                name="apply_transfer_fees",
                action=self._apply_transfer_fees,
                compensation=self._reverse_transfer_fees,
                **steps[0]
            )
        
        # Step 6: Send notifications
        saga.add_step(
            name="send_notifications",
            action=self._send_transfer_notifications,
            compensation=self._log_notification_failure,
            **steps[0]
        )
        
        result = await saga.execute()
        return result

    async def _execute_batch_payment(
        self,
        business_tx_id: str,
        steps: List[Dict],
        user_id: int
    ) -> Dict[str, Any]:
        """Execute batch payment processing with rollback on any failure."""
        
        # Use distributed transaction for batch payments
        distributed_tx = DistributedTransaction(business_tx_id)
        
        with get_db_session() as coordinator_session:
            # Add each payment as a participant
            for i, payment_data in enumerate(steps):
                payment_session = Session(bind=engine)
                
                distributed_tx.add_participant(
                    session=payment_session,
                    operation=self._process_individual_payment,
                    compensation=self._reverse_individual_payment,
                    payment_data=payment_data,
                    user_id=user_id,
                    payment_index=i
                )
            
            with distributed_tx.execute():
                # All payments processed successfully
                total_amount = sum(step['amount'] for step in steps)
                
                return {
                    'business_transaction_id': business_tx_id,
                    'total_payments': len(steps),
                    'total_amount': total_amount,
                    'status': 'completed'
                }

    async def _execute_account_closure(
        self,
        business_tx_id: str,
        steps: List[Dict],
        user_id: int
    ) -> Dict[str, Any]:
        """Execute account closure process with all necessary validations."""
        saga = SagaOrchestrator(business_tx_id)
        
        account_data = steps[0]
        
        # Step 1: Validate account can be closed
        saga.add_step(
            name="validate_account_closure",
            action=self._validate_account_closure,
            compensation=self._log_closure_validation_failure,
            **account_data
        )
        
        # Step 2: Transfer remaining balance to designated account
        if account_data.get('transfer_to_account_id'):
            saga.add_step(
                name="transfer_remaining_balance",
                action=self._transfer_remaining_balance,
                compensation=self._reverse_balance_transfer,
                **account_data
            )
        
        # Step 3: Mark account as closed
        saga.add_step(
            name="close_account",
            action=self._close_account,
            compensation=self._reopen_account,
            **account_data
        )
        
        # Step 4: Create closure audit record
        saga.add_step(
            name="create_closure_audit",
            action=self._create_closure_audit,
            compensation=self._remove_closure_audit,
            user_id=user_id,
            **account_data
        )
        
        result = await saga.execute()
        return result

    # Helper methods for saga steps
    async def _validate_and_reserve_funds(self, db: Session, **kwargs) -> Dict:
        """Validate source account and reserve funds."""
        account_id = kwargs['account_id']
        amount = kwargs['amount']
        
        account = db.query(Account).with_for_update().filter_by(id=account_id).first()
        if not account:
            raise ValueError(f"Account {account_id} not found")
        
        if not account.can_withdraw(amount):
            raise ValueError(f"Insufficient funds in account {account.account_number}")
        
        # Reserve the funds
        success = account.reserve_funds(amount)
        if not success:
            raise ValueError("Failed to reserve funds")
        
        db.flush()
        return {'account_id': account_id, 'reserved_amount': amount}

    async def _release_reserved_funds(self, db: Session, result: Dict):
        """Compensation: Release reserved funds."""
        account = db.query(Account).filter_by(id=result['account_id']).first()
        if account:
            account.release_reserved_funds(result['reserved_amount'])
            db.commit()

    async def _execute_transfer_step(self, db: Session, **kwargs) -> Dict:
        """Execute the actual transfer between accounts."""
        processor = TransactionProcessor(db)
        
        transaction = processor.process_transfer(
            from_account_id=kwargs['source_account_id'],
            to_account_id=kwargs['dest_account_id'],
            amount=kwargs['amount'],
            initiated_by_id=kwargs['user_id'],
            description="Complex transfer operation"
        )
        
        db.flush()
        return {'transaction_id': transaction.id, 'transaction_uuid': str(transaction.transaction_id)}

    async def _reverse_transfer_step(self, db: Session, result: Dict):
        """Compensation: Reverse the transfer."""
        processor = TransactionProcessor(db)
        
        # Reverse the transaction
        processor.reverse_transaction(
            transaction_id=result['transaction_id'],
            reason="Saga compensation - complex transfer failed",
            reversed_by_id=1  # System user
        )
        db.commit()

    async def _process_individual_payment(self, db: Session, **kwargs) -> Dict:
        """Process individual payment in batch."""
        payment_data = kwargs['payment_data']
        user_id = kwargs['user_id']
        
        processor = TransactionProcessor(db)
        
        if payment_data['type'] == 'transfer':
            transaction = processor.process_transfer(
                from_account_id=payment_data['from_account_id'],
                to_account_id=payment_data['to_account_id'],
                amount=payment_data['amount'],
                initiated_by_id=user_id,
                description=payment_data.get('description', 'Batch payment'),
                reference=payment_data.get('reference')
            )
        elif payment_data['type'] == 'withdrawal':
            transaction = processor.process_withdrawal(
                account_id=payment_data['account_id'],
                amount=payment_data['amount'],
                initiated_by_id=user_id,
                description=payment_data.get('description', 'Batch withdrawal'),
                reference=payment_data.get('reference')
            )
        else:
            raise ValueError(f"Unsupported payment type: {payment_data['type']}")
        
        db.flush()
        return {'transaction_id': transaction.id, 'payment_data': payment_data}

    async def _reverse_individual_payment(self, db: Session, result: Dict):
        """Compensation: Reverse individual payment."""
        processor = TransactionProcessor(db)
        
        processor.reverse_transaction(
            transaction_id=result['transaction_id'],
            reason="Batch payment compensation",
            reversed_by_id=1  # System user
        )
        db.commit()


# Global orchestrator instance
transaction_orchestrator = TransactionOrchestrator()


# Example usage patterns
class BusinessTransactionExamples:
    """Example implementations of common business transaction patterns."""

    @staticmethod
    async def process_payroll_batch(
        employee_accounts: List[int],
        amounts: List[Decimal],
        company_account_id: int,
        user_id: int
    ) -> Dict[str, Any]:
        """Process payroll payments to multiple employees."""
        
        steps = []
        for acc_id, amount in zip(employee_accounts, amounts):
            steps.append({
                'type': 'transfer',
                'from_account_id': company_account_id,
                'to_account_id': acc_id,
                'amount': amount,
                'description': f'Payroll payment for {datetime.utcnow().strftime("%B %Y")}',
                'reference': f'PAYROLL-{datetime.utcnow().strftime("%Y%m")}'
            })
        
        return await transaction_orchestrator.execute_business_transaction(
            transaction_type='batch_payment',
            steps=steps,
            user_id=user_id,
            metadata={'type': 'payroll', 'period': datetime.utcnow().strftime("%Y-%m")}
        )

    @staticmethod
    async def process_merchant_settlement(
        merchant_account_id: int,
        platform_account_id: int,
        gross_amount: Decimal,
        platform_fee: Decimal,
        user_id: int
    ) -> Dict[str, Any]:
        """Process merchant settlement with platform fees."""
        
        net_amount = gross_amount - platform_fee
        
        steps = [
            {
                'account_id': merchant_account_id,
                'amount': gross_amount,
                'currency': 'USD',
                'validate_balance': False  # Merchant receives money
            },
            {
                'account_id': platform_account_id,
                'amount': net_amount,
                'currency': 'USD'
            },
            {
                'apply_fees': True,
                'fee_amount': platform_fee,
                'fee_account_id': platform_account_id
            }
        ]
        
        return await transaction_orchestrator.execute_business_transaction(
            transaction_type='complex_transfer',
            steps=steps,
            user_id=user_id,
            metadata={
                'type': 'merchant_settlement',
                'gross_amount': str(gross_amount),
                'platform_fee': str(platform_fee),
                'net_amount': str(net_amount)
            }
        )

    @staticmethod
    async def process_refund_with_fees(
        original_transaction_id: int,
        refund_amount: Decimal,
        refund_reason: str,
        user_id: int
    ) -> Dict[str, Any]:
        """Process refund with fee calculations and compliance checks."""
        
        with get_db_session() as db:
            # Get original transaction
            original_tx = db.query(Transaction).filter_by(id=original_transaction_id).first()
            if not original_tx:
                raise ValueError(f"Original transaction {original_transaction_id} not found")
            
            # Calculate refund fees (example: 2.5% processing fee)
            processing_fee = refund_amount * Decimal('0.025')
            net_refund = refund_amount - processing_fee
            
            steps = [
                {
                    'account_id': original_tx.to_account_id,
                    'amount': net_refund,
                    'currency': original_tx.currency,
                    'validate_balance': True
                },
                {
                    'account_id': original_tx.from_account_id,
                    'amount': net_refund,
                    'currency': original_tx.currency
                }
            ]
            
            return await transaction_orchestrator.execute_business_transaction(
                transaction_type='complex_transfer',
                steps=steps,
                user_id=user_id,
                metadata={
                    'type': 'refund',
                    'original_transaction_id': original_transaction_id,
                    'refund_reason': refund_reason,
                    'processing_fee': str(processing_fee),
                    'net_refund': str(net_refund)
                }
            )
```


### 5.7 Testing Transaction Systems

**tests/test_transactions.py** - Comprehensive Transaction Testing:
```python
"""
Comprehensive testing for transaction management system.
Demonstrates testing patterns for financial operations with strict requirements.
"""

import pytest
import asyncio
from decimal import Decimal
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from typing import List

from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from app.models.transactions import (
    Account, Transaction, TransactionLog, BalanceSnapshot,
    TransactionType, TransactionStatus, Currency, AccountType
)
from app.services.transaction_service import TransactionProcessor
from app.services.advanced_transactions import (
    DistributedTransaction, SagaOrchestrator, transaction_orchestrator
)
from app.crud.transactions import account_crud, transaction_crud
from app.core.exceptions import InsufficientFundsError, TransactionError


class TestAccountManagement:
    """Test account creation and management operations."""

    def test_create_account(self, db_session: Session, test_user):
        """Test basic account creation."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('1000.00')
        )
        
        assert account.id is not None
        assert account.balance == Decimal('1000.00')
        assert account.available_balance == Decimal('1000.00')
        assert account.reserved_balance == Decimal('0')
        assert account.owner_id == test_user.id
        assert account.is_active is True
        assert account.is_frozen is False

    def test_account_number_generation(self, db_session: Session, test_user):
        """Test unique account number generation."""
        processor = TransactionProcessor(db_session)
        
        # Create multiple accounts
        accounts = []
        for i in range(5):
            account = processor.create_account(
                owner_id=test_user.id,
                account_type=AccountType.CHECKING
            )
            accounts.append(account)
        
        # Ensure all account numbers are unique
        account_numbers = [acc.account_number for acc in accounts]
        assert len(set(account_numbers)) == len(account_numbers)
        
        # Ensure all start with correct prefix
        for acc_num in account_numbers:
            assert acc_num.startswith('CHK')

    def test_account_constraints(self, db_session: Session, test_user):
        """Test account validation constraints."""
        processor = TransactionProcessor(db_session)
        
        # Test balance consistency constraint
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.SAVINGS
        )
        
        # Manually break constraint (should be prevented by application logic)
        with pytest.raises(Exception):
            account.available_balance = Decimal('100')
            account.reserved_balance = Decimal('50')
            account.balance = Decimal('100')  # Should be 150
            db_session.commit()

    def test_freeze_unfreeze_account(self, db_session: Session, test_user):
        """Test account freezing and unfreezing."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('500.00')
        )
        
        # Test withdrawal before freeze
        transaction = processor.process_withdrawal(
            account_id=account.id,
            amount=Decimal('100.00'),
            initiated_by_id=test_user.id
        )
        assert transaction.status == TransactionStatus.COMPLETED
        
        # Freeze account
        account_crud.freeze_account(
            db_session,
            account.id,
            "Suspicious activity detected",
            test_user.id
        )
        
        db_session.refresh(account)
        assert account.is_frozen is True
        
        # Test withdrawal after freeze (should fail)
        with pytest.raises(Exception):
            processor.process_withdrawal(
                account_id=account.id,
                amount=Decimal('50.00'),
                initiated_by_id=test_user.id
            )
        
        # Unfreeze account
        account_crud.unfreeze_account(db_session, account.id, test_user.id)
        
        db_session.refresh(account)
        assert account.is_frozen is False
        
        # Test withdrawal after unfreeze (should work)
        transaction = processor.process_withdrawal(
            account_id=account.id,
            amount=Decimal('50.00'),
            initiated_by_id=test_user.id
        )
        assert transaction.status == TransactionStatus.COMPLETED


class TestBasicTransactions:
    """Test basic transaction operations."""

    def test_simple_deposit(self, db_session: Session, test_user):
        """Test basic deposit operation."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING
        )
        
        transaction = processor.process_deposit(
            account_id=account.id,
            amount=Decimal('250.00'),
            description="Test deposit",
            initiated_by_id=test_user.id
        )
        
        assert transaction.status == TransactionStatus.COMPLETED
        assert transaction.amount == Decimal('250.00')
        assert transaction.to_account_id == account.id
        
        db_session.refresh(account)
        assert account.balance == Decimal('250.00')
        assert account.available_balance == Decimal('250.00')

    def test_simple_withdrawal(self, db_session: Session, test_user):
        """Test basic withdrawal operation."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('500.00')
        )
        
        transaction = processor.process_withdrawal(
            account_id=account.id,
            amount=Decimal('150.00'),
            description="Test withdrawal",
            initiated_by_id=test_user.id
        )
        
        assert transaction.status == TransactionStatus.COMPLETED
        assert transaction.amount == Decimal('150.00')
        assert transaction.from_account_id == account.id
        
        db_session.refresh(account)
        assert account.balance == Decimal('350.00')
        assert account.available_balance == Decimal('350.00')

    def test_insufficient_funds(self, db_session: Session, test_user):
        """Test withdrawal with insufficient funds."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('100.00')
        )
        
        with pytest.raises(InsufficientFundsError):
            processor.process_withdrawal(
                account_id=account.id,
                amount=Decimal('200.00'),
                initiated_by_id=test_user.id
            )
        
        # Account balance should remain unchanged
        db_session.refresh(account)
        assert account.balance == Decimal('100.00')

    def test_simple_transfer(self, db_session: Session, test_user, test_user2):
        """Test basic transfer between accounts."""
        processor = TransactionProcessor(db_session)
        
        # Create accounts
        source_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('1000.00')
        )
        
        dest_account = processor.create_account(
            owner_id=test_user2.id,
            account_type=AccountType.SAVINGS
        )
        
        # Execute transfer
        transaction = processor.process_transfer(
            from_account_id=source_account.id,
            to_account_id=dest_account.id,
            amount=Decimal('300.00'),
            description="Test transfer",
            initiated_by_id=test_user.id
        )
        
        assert transaction.status == TransactionStatus.COMPLETED
        assert transaction.amount == Decimal('300.00')
        
        # Check balances
        db_session.refresh(source_account)
        db_session.refresh(dest_account)
        
        assert source_account.balance == Decimal('700.00')
        assert dest_account.balance == Decimal('300.00')

    def test_transfer_same_account(self, db_session: Session, test_user):
        """Test that transfer to same account is rejected."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('500.00')
        )
        
        with pytest.raises(Exception) as exc_info:
            processor.process_transfer(
                from_account_id=account.id,
                to_account_id=account.id,
                amount=Decimal('100.00'),
                initiated_by_id=test_user.id
            )
        
        assert "same account" in str(exc_info.value).lower()


class TestAdvancedTransactions:
    """Test advanced transaction patterns."""

    def test_batch_processing(self, db_session: Session, test_user, test_user2):
        """Test batch transaction processing."""
        processor = TransactionProcessor(db_session)
        
        # Create source account
        source_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('2000.00')
        )
        
        # Create destination accounts
        dest_accounts = []
        for i in range(3):
            account = processor.create_account(
                owner_id=test_user2.id,
                account_type=AccountType.SAVINGS
            )
            dest_accounts.append(account)
        
        # Prepare batch transactions
        batch_data = []
        for i, dest_account in enumerate(dest_accounts):
            batch_data.append({
                'transaction_type': 'transfer',
                'from_account_id': source_account.id,
                'to_account_id': dest_account.id,
                'amount': Decimal('200.00'),
                'description': f'Batch transfer {i + 1}',
                'initiated_by_id': test_user.id
            })
        
        # Execute batch
        transactions = processor.batch_process_transactions(batch_data)
        
        assert len(transactions) == 3
        for transaction in transactions:
            assert transaction.status == TransactionStatus.COMPLETED
            assert transaction.amount == Decimal('200.00')
        
        # Check balances
        db_session.refresh(source_account)
        assert source_account.balance == Decimal('1400.00')  # 2000 - (3 * 200)
        
        for dest_account in dest_accounts:
            db_session.refresh(dest_account)
            assert dest_account.balance == Decimal('200.00')

    def test_batch_processing_failure_rollback(self, db_session: Session, test_user, test_user2):
        """Test that batch processing rolls back on any failure."""
        processor = TransactionProcessor(db_session)
        
        # Create accounts
        source_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('500.00')  # Not enough for all transfers
        )
        
        dest_account = processor.create_account(
            owner_id=test_user2.id,
            account_type=AccountType.SAVINGS
        )
        
        # Prepare batch that will fail (total amount exceeds balance)
        batch_data = []
        for i in range(3):
            batch_data.append({
                'transaction_type': 'transfer',
                'from_account_id': source_account.id,
                'to_account_id': dest_account.id,
                'amount': Decimal('300.00'),  # 900 total > 500 available
                'initiated_by_id': test_user.id
            })
        
        # Batch should fail
        with pytest.raises(Exception):
            processor.batch_process_transactions(batch_data)
        
        # No changes should be persisted
        db_session.refresh(source_account)
        db_session.refresh(dest_account)
        
        assert source_account.balance == Decimal('500.00')
        assert dest_account.balance == Decimal('0')

    def test_transaction_reversal(self, db_session: Session, test_user, test_user2):
        """Test transaction reversal functionality."""
        processor = TransactionProcessor(db_session)
        
        # Create accounts
        source_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('1000.00')
        )
        
        dest_account = processor.create_account(
            owner_id=test_user2.id,
            account_type=AccountType.SAVINGS
        )
        
        # Execute original transfer
        original_transaction = processor.process_transfer(
            from_account_id=source_account.id,
            to_account_id=dest_account.id,
            amount=Decimal('400.00'),
            description="Transfer to be reversed",
            initiated_by_id=test_user.id
        )
        
        # Check balances after original transaction
        db_session.refresh(source_account)
        db_session.refresh(dest_account)
        
        assert source_account.balance == Decimal('600.00')
        assert dest_account.balance == Decimal('400.00')
        
        # Reverse the transaction
        reversal_transaction = processor.reverse_transaction(
            transaction_id=original_transaction.id,
            reason="Customer dispute",
            reversed_by_id=test_user.id
        )
        
        assert reversal_transaction.status == TransactionStatus.COMPLETED
        assert reversal_transaction.parent_transaction_id == original_transaction.id
        
        # Check that original transaction is marked as reversed
        db_session.refresh(original_transaction)
        assert original_transaction.status == TransactionStatus.REVERSED
        
        # Check balances after reversal
        db_session.refresh(source_account)
        db_session.refresh(dest_account)
        
        assert source_account.balance == Decimal('1000.00')
        assert dest_account.balance == Decimal('0')


class TestConcurrentTransactions:
    """Test concurrent transaction handling and race conditions."""

    def test_concurrent_withdrawals_same_account(self, db_session: Session, test_user):
        """Test concurrent withdrawals from same account to prevent overdraft."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('500.00')
        )
        
        def withdraw_funds(amount: Decimal, description: str):
            """Worker function for concurrent withdrawal."""
            with get_db_session() as worker_db:
                worker_processor = TransactionProcessor(worker_db)
                try:
                    return worker_processor.process_withdrawal(
                        account_id=account.id,
                        amount=amount,
                        description=description,
                        initiated_by_id=test_user.id
                    )
                except Exception as e:
                    return str(e)
        
        # Execute concurrent withdrawals
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            
            # Each tries to withdraw 300, but total balance is only 500
            futures.append(executor.submit(withdraw_funds, Decimal('300.00'), "Withdrawal 1"))
            futures.append(executor.submit(withdraw_funds, Decimal('300.00'), "Withdrawal 2"))
            futures.append(executor.submit(withdraw_funds, Decimal('300.00'), "Withdrawal 3"))
            
            results = [future.result() for future in futures]
        
        # Only one should succeed, others should fail
        successful_transactions = [r for r in results if isinstance(r, Transaction)]
        failed_attempts = [r for r in results if isinstance(r, str)]
        
        assert len(successful_transactions) == 1
        assert len(failed_attempts) == 2
        
        # Final balance should reflect only the successful withdrawal
        db_session.refresh(account)
        assert account.balance == Decimal('200.00')

    def test_concurrent_transfers(self, db_session: Session, test_user, test_user2):
        """Test concurrent transfers between accounts."""
        processor = TransactionProcessor(db_session)
        
        # Create accounts
        account1 = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('1000.00')
        )
        
        account2 = processor.create_account(
            owner_id=test_user2.id,
            account_type=AccountType.SAVINGS,
            initial_balance=Decimal('1000.00')
        )
        
        def execute_transfer(from_id: int, to_id: int, amount: Decimal):
            """Worker function for concurrent transfers."""
            with get_db_session() as worker_db:
                worker_processor = TransactionProcessor(worker_db)
                try:
                    return worker_processor.process_transfer(
                        from_account_id=from_id,
                        to_account_id=to_id,
                        amount=amount,
                        initiated_by_id=test_user.id
                    )
                except Exception as e:
                    return str(e)
        
        # Execute concurrent bidirectional transfers
        with ThreadPoolExecutor(max_workers=2) as executor:
            future1 = executor.submit(execute_transfer, account1.id, account2.id, Decimal('300.00'))
            future2 = executor.submit(execute_transfer, account2.id, account1.id, Decimal('200.00'))
            
            result1 = future1.result()
            result2 = future2.result()
        
        # Both should succeed (no deadlock)
        assert isinstance(result1, Transaction)
        assert isinstance(result2, Transaction)
        
        # Check final balances
        db_session.refresh(account1)
        db_session.refresh(account2)
        
        assert account1.balance == Decimal('900.00')  # 1000 - 300 + 200
        assert account2.balance == Decimal('1100.00')  # 1000 + 300 - 200


class TestDistributedTransactions:
    """Test distributed transaction patterns."""

    def test_two_phase_commit_success(self, db_session: Session, test_user):
        """Test successful two-phase commit."""
        processor = TransactionProcessor(db_session)
        
        # Create accounts
        accounts = []
        for i in range(3):
            account = processor.create_account(
                owner_id=test_user.id,
                account_type=AccountType.CHECKING,
                initial_balance=Decimal('500.00')
            )
            accounts.append(account)
        
        def transfer_operation(session: Session, from_id: int, to_id: int, amount: Decimal):
            """Individual transfer operation."""
            local_processor = TransactionProcessor(session)
            return local_processor.process_transfer(
                from_account_id=from_id,
                to_account_id=to_id,
                amount=amount,
                initiated_by_id=test_user.id
            )
        
        # Set up distributed transaction
        distributed_tx = DistributedTransaction()
        
        # Add multiple transfer operations
        for i in range(2):
            session = Session(bind=db_session.bind)
            distributed_tx.add_participant(
                session=session,
                operation=transfer_operation,
                from_id=accounts[i].id,
                to_id=accounts[i + 1].id,
                amount=Decimal('100.00')
            )
        
        # Execute distributed transaction
        with distributed_tx.execute():
            assert distributed_tx.state == TwoPhaseCommitState.COMMITTED
        
        # Verify all transfers completed
        for i, account in enumerate(accounts):
            db_session.refresh(account)
            if i == 0:
                assert account.balance == Decimal('400.00')  # Sent 100
            elif i == 1:
                assert account.balance == Decimal('500.00')  # Received 100, sent 100
            else:
                assert account.balance == Decimal('600.00')  # Received 100

    def test_two_phase_commit_failure(self, db_session: Session, test_user):
        """Test two-phase commit with failure and rollback."""
        processor = TransactionProcessor(db_session)
        
        # Create accounts with insufficient funds for all transfers
        source_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('150.00')  # Not enough for 2 * 100
        )
        
        dest_accounts = []
        for i in range(2):
            account = processor.create_account(
                owner_id=test_user.id,
                account_type=AccountType.SAVINGS
            )
            dest_accounts.append(account)
        
        def failing_transfer_operation(session: Session, to_id: int):
            """Transfer operation that will fail due to insufficient funds."""
            local_processor = TransactionProcessor(session)
            return local_processor.process_transfer(
                from_account_id=source_account.id,
                to_account_id=to_id,
                amount=Decimal('100.00'),
                initiated_by_id=test_user.id
            )
        
        # Set up distributed transaction that will fail
        distributed_tx = DistributedTransaction()
        
        for dest_account in dest_accounts:
            session = Session(bind=db_session.bind)
            distributed_tx.add_participant(
                session=session,
                operation=failing_transfer_operation,
                to_id=dest_account.id
            )
        
        # Execute should fail and rollback
        with pytest.raises(TransactionError):
            with distributed_tx.execute():
                pass
        
        assert distributed_tx.state == TwoPhaseCommitState.ABORTED
        
        # Source account balance should remain unchanged
        db_session.refresh(source_account)
        assert source_account.balance == Decimal('150.00')
        
        # Destination accounts should remain empty
        for dest_account in dest_accounts:
            db_session.refresh(dest_account)
            assert dest_account.balance == Decimal('0')


class TestSagaPatterns:
    """Test saga transaction patterns."""

    @pytest.mark.asyncio
    async def test_successful_saga(self, db_session: Session, test_user):
        """Test successful saga execution."""
        processor = TransactionProcessor(db_session)
        
        # Create test accounts
        company_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.MERCHANT,
            initial_balance=Decimal('5000.00')
        )
        
        employee_accounts = []
        for i in range(3):
            account = processor.create_account(
                owner_id=test_user.id,
                account_type=AccountType.CHECKING
            )
            employee_accounts.append(account)
        
        # Execute payroll saga
        result = await transaction_orchestrator.execute_business_transaction(
            transaction_type='batch_payment',
            steps=[
                {
                    'type': 'transfer',
                    'from_account_id': company_account.id,
                    'to_account_id': acc.id,
                    'amount': Decimal('800.00'),
                    'description': 'Monthly salary'
                }
                for acc in employee_accounts
            ],
            user_id=test_user.id
        )
        
        assert result['status'] == 'completed'
        
        # Verify all payments completed
        db_session.refresh(company_account)
        assert company_account.balance == Decimal('2600.00')  # 5000 - (3 * 800)
        
        for account in employee_accounts:
            db_session.refresh(account)
            assert account.balance == Decimal('800.00')

    @pytest.mark.asyncio
    async def test_saga_with_compensation(self, db_session: Session, test_user):
        """Test saga with failure and compensation."""
        processor = TransactionProcessor(db_session)
        
        # Create saga with intentional failure
        saga = SagaOrchestrator()
        
        # Step 1: Successful operation
        async def successful_step(db: Session, **kwargs):
            account = processor.create_account(
                owner_id=test_user.id,
                account_type=AccountType.CHECKING,
                initial_balance=Decimal('100.00')
            )
            return {'account_id': account.id}
        
        async def compensate_account_creation(db: Session, result: Dict):
            account = db.query(Account).filter_by(id=result['account_id']).first()
            if account:
                db.delete(account)
                db.commit()
        
        # Step 2: Failing operation
        async def failing_step(db: Session, **kwargs):
            raise Exception("Simulated failure")
        
        async def failing_compensation(db: Session, result: Dict):
            pass  # No compensation needed for failed step
        
        saga.add_step("create_account", successful_step, compensate_account_creation)
        saga.add_step("failing_operation", failing_step, failing_compensation)
        
        # Execute saga (should fail and compensate)
        result = await saga.execute()
        
        assert result['status'] == 'failed'
        assert result['steps_executed'] == 1
        
        # Verify compensation ran (account should be deleted)
        account_count = db_session.query(Account).filter_by(owner_id=test_user.id).count()
        assert account_count == 0  # Account was compensated (deleted)


class TestAuditAndCompliance:
    """Test audit trail and compliance features."""

    def test_transaction_audit_trail(self, db_session: Session, test_user):
        """Test comprehensive audit trail for transactions."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('1000.00')
        )
        
        # Create transaction
        transaction = processor.process_deposit(
            account_id=account.id,
            amount=Decimal('500.00'),
            initiated_by_id=test_user.id
        )
        
        # Check that audit logs were created
        audit_logs = db_session.query(TransactionLog).filter_by(
            transaction_id=transaction.id
        ).all()
        
        # Should have logs for status changes
        assert len(audit_logs) >= 2  # PENDING -> PROCESSING -> COMPLETED
        
        # Verify audit log structure
        for log in audit_logs:
            assert log.transaction_id == transaction.id
            assert log.changed_by_id == test_user.id
            assert log.changed_at is not None

    def test_balance_snapshots(self, db_session: Session, test_user):
        """Test daily balance snapshot creation."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('1000.00')
        )
        
        # Perform some transactions
        processor.process_deposit(
            account_id=account.id,
            amount=Decimal('200.00'),
            initiated_by_id=test_user.id
        )
        
        processor.process_withdrawal(
            account_id=account.id,
            amount=Decimal('150.00'),
            initiated_by_id=test_user.id
        )
        
        # Create balance snapshot
        snapshots_created = processor.create_balance_snapshots()
        
        assert snapshots_created > 0
        
        # Verify snapshot data
        snapshot = db_session.query(BalanceSnapshot).filter_by(
            account_id=account.id
        ).first()
        
        assert snapshot is not None
        assert snapshot.closing_balance == account.balance
        assert snapshot.transaction_count > 0

    def test_withdrawal_limits(self, db_session: Session, test_user):
        """Test daily and monthly withdrawal limits."""
        processor = TransactionProcessor(db_session)
        
        account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.CHECKING,
            initial_balance=Decimal('10000.00'),
            daily_withdrawal_limit=Decimal('1000.00'),
            monthly_withdrawal_limit=Decimal('5000.00')
        )
        
        # Test daily limit
        # First withdrawal should succeed
        transaction1 = processor.process_withdrawal(
            account_id=account.id,
            amount=Decimal('800.00'),
            initiated_by_id=test_user.id
        )
        assert transaction1.status == TransactionStatus.COMPLETED
        
        # Second withdrawal should fail (exceeds daily limit)
        with pytest.raises(Exception) as exc_info:
            processor.process_withdrawal(
                account_id=account.id,
                amount=Decimal('500.00'),
                initiated_by_id=test_user.id
            )
        
        assert "daily withdrawal limit" in str(exc_info.value).lower()


class TestPerformanceAndScalability:
    """Test performance characteristics of transaction system."""

    def test_bulk_account_creation_performance(self, db_session: Session, test_users):
        """Test performance of bulk account creation."""
        processor = TransactionProcessor(db_session)
        
        start_time = datetime.utcnow()
        
        # Create 100 accounts
        accounts = []
        for i, user in enumerate(test_users[:100]):
            account = processor.create_account(
                owner_id=user.id,
                account_type=AccountType.CHECKING,
                initial_balance=Decimal('100.00')
            )
            accounts.append(account)
        
        end_time = datetime.utcnow()
        processing_time = (end_time - start_time).total_seconds()
        
        # Should complete within reasonable time
        assert processing_time < 10.0  # 10 seconds for 100 accounts
        assert len(accounts) == 100
        
        # Verify all accounts are valid
        for account in accounts:
            assert account.balance == Decimal('100.00')
            assert account.is_active is True

    def test_high_volume_transactions(self, db_session: Session, test_user):
        """Test system behavior under high transaction volume."""
        processor = TransactionProcessor(db_session)
        
        # Create source account with large balance
        source_account = processor.create_account(
            owner_id=test_user.id,
            account_type=AccountType.MERCHANT,
            initial_balance=Decimal('100000.00')
        )
        
        # Create multiple destination accounts
        dest_accounts = []
        for i in range(50):
            account = processor.create_account(
                owner_id=test_user.id,
                account_type=AccountType.CHECKING
            )
            dest_accounts.append(account)
        
        # Prepare large batch of small transactions
        batch_data = []
        for dest_account in dest_accounts:
            batch_data.append({
                'transaction_type': 'transfer',
                'from_account_id': source_account.id,
                'to_account_id': dest_account.id,
                'amount': Decimal('100.00'),
                'description': f'Bulk transfer to {dest_account.account_number}',
                'initiated_by_id': test_user.id
            })
        
        start_time = datetime.utcnow()
        
        # Process batch
        transactions = processor.batch_process_transactions(batch_data)
        
        end_time = datetime.utcnow()
        processing_time = (end_time - start_time).total_seconds()
        
        # Verify results
        assert len(transactions) == 50
        assert all(tx.status == TransactionStatus.COMPLETED for tx in transactions)
        
        # Performance check
        assert processing_time < 30.0  # Should complete within 30 seconds
        
        logger.info(f"Processed {len(transactions)} transactions in {processing_time:.2f} seconds")
        logger.info(f"Average: {processing_time / len(transactions):.4f} seconds per transaction")


# Test fixtures and utilities
@pytest.fixture
def test_user(db_session):
    """Create a test user."""
    from app.models.user import User, UserRole, UserStatus
    
    user = User(
        username="testuser",
        email="test@example.com",
        full_name="Test User",
        role=UserRole.USER,
        status=UserStatus.ACTIVE
    )
    db_session.add(user)
    db_session.commit()
    db_session.refresh(user)
    return user


@pytest.fixture
def test_user2(db_session):
    """Create a second test user."""
    from app.models.user import User, UserRole, UserStatus
    
    user = User(
        username="testuser2",
        email="test2@example.com", 
        full_name="Test User 2",
        role=UserRole.USER,
        status=UserStatus.ACTIVE
    )
    db_session.add(user)
    db_session.commit()
    db_session.refresh(user)
    return user


@pytest.fixture
def test_users(db_session):
    """Create multiple test users for bulk testing."""
    from app.models.user import User, UserRole, UserStatus
    
    users = []
    for i in range(100):
        user = User(
            username=f"testuser{i}",
            email=f"test{i}@example.com",
            full_name=f"Test User {i}",
            role=UserRole.USER,
            status=UserStatus.ACTIVE
        )
        users.append(user)
    
    db_session.add_all(users)
    db_session.commit()
    
    for user in users:
        db_session.refresh(user)
    
    return users
```


### 5.8 Usage Examples and Best Practices

**examples/transaction_examples.py** - Real-World Usage Examples:
```python
"""
Real-world examples demonstrating the transaction management system.
Shows practical patterns for implementing financial operations.
"""

import asyncio
from decimal import Decimal
from datetime import datetime, timedelta
from typing import List, Dict, Any

from sqlalchemy.orm import Session
from app.models.transactions import AccountType, Currency, TransactionType
from app.services.transaction_service import TransactionProcessor
from app.services.advanced_transactions import transaction_orchestrator


class PayrollProcessingExample:
    """Example: Automated payroll processing with proper error handling."""
    
    def __init__(self, db_session: Session):
        self.processor = TransactionProcessor(db_session)
        self.db_session = db_session

    async def process_monthly_payroll(
        self, 
        company_account_id: int,
        payroll_data: List[Dict[str, Any]],
        initiated_by_id: int
    ) -> Dict[str, Any]:
        """
        Process monthly payroll using saga pattern for consistency.
        
        Args:
            company_account_id: Company's payroll account
            payroll_data: List of employee payment information
            initiated_by_id: User initiating the payroll
            
        Returns:
            Processing results with success/failure details
        """
        
        # Validate payroll data
        total_amount = sum(Decimal(str(emp['salary'])) for emp in payroll_data)
        
        company_account = self.db_session.query(Account).filter_by(
            id=company_account_id
        ).first()
        
        if company_account.available_balance < total_amount:
            raise InsufficientFundsError(
                f"Company account has insufficient funds. "
                f"Required: {total_amount}, Available: {company_account.available_balance}"
            )
        
        # Build payroll transaction steps
        steps = []
        for emp in payroll_data:
            steps.append({
                'type': 'transfer',
                'from_account_id': company_account_id,
                'to_account_id': emp['account_id'],
                'amount': Decimal(str(emp['salary'])),
                'description': f"Salary payment - {emp['employee_name']}",
                'metadata': {
                    'employee_id': emp['employee_id'],
                    'period': emp['pay_period'],
                    'department': emp['department']
                }
            })
        
        # Execute payroll using saga orchestration
        result = await transaction_orchestrator.execute_business_transaction(
            transaction_type='payroll_processing',
            steps=steps,
            user_id=initiated_by_id,
            metadata={
                'total_employees': len(payroll_data),
                'total_amount': str(total_amount),
                'processing_date': datetime.utcnow().isoformat()
            }
        )
        
        # Generate payroll report
        if result['status'] == 'completed':
            await self._generate_payroll_report(result['transaction_id'], payroll_data)
        
        return result

    async def _generate_payroll_report(
        self, 
        transaction_id: str, 
        payroll_data: List[Dict]
    ):
        """Generate payroll completion report."""
        
        report_data = {
            'transaction_id': transaction_id,
            'processed_at': datetime.utcnow(),
            'employees_paid': len(payroll_data),
            'total_amount': sum(Decimal(str(emp['salary'])) for emp in payroll_data),
            'details': payroll_data
        }
        
        # In real implementation, this would generate PDF report,
        # send notifications, update payroll system, etc.
        logger.info(f"Payroll processed successfully: {report_data}")
        

class MerchantSettlementExample:
    """Example: Daily merchant settlement processing."""
    
    def __init__(self, db_session: Session):
        self.processor = TransactionProcessor(db_session)
        self.db_session = db_session

    async def process_daily_settlement(
        self, 
        settlement_date: datetime,
        initiated_by_id: int
    ) -> Dict[str, Any]:
        """
        Process daily merchant settlements for all pending transactions.
        
        Demonstrates:
        - Complex querying for settlement calculations
        - Batch processing with proper error handling
        - Fee calculations and deductions
        - Settlement reporting
        """
        
        # Get all pending merchant transactions for settlement
        settlement_window = settlement_date.replace(hour=0, minute=0, second=0)
        next_day = settlement_window + timedelta(days=1)
        
        pending_transactions = self.db_session.query(Transaction).filter(
            Transaction.transaction_type.in_([
                TransactionType.MERCHANT_PAYMENT,
                TransactionType.MERCHANT_REFUND
            ]),
            Transaction.status == TransactionStatus.PENDING_SETTLEMENT,
            Transaction.created_at >= settlement_window,
            Transaction.created_at < next_day
        ).all()
        
        # Group by merchant account
        merchant_settlements = {}
        for transaction in pending_transactions:
            merchant_account_id = transaction.to_account_id
            
            if merchant_account_id not in merchant_settlements:
                merchant_settlements[merchant_account_id] = {
                    'transactions': [],
                    'gross_amount': Decimal('0'),
                    'fee_amount': Decimal('0'),
                    'net_amount': Decimal('0')
                }
            
            settlement = merchant_settlements[merchant_account_id]
            settlement['transactions'].append(transaction)
            settlement['gross_amount'] += transaction.amount
            
            # Calculate processing fee (2.5% for payments, no fee for refunds)
            if transaction.transaction_type == TransactionType.MERCHANT_PAYMENT:
                fee = transaction.amount * Decimal('0.025')
                settlement['fee_amount'] += fee
            
            settlement['net_amount'] = settlement['gross_amount'] - settlement['fee_amount']
        
        # Process settlements
        settlement_results = []
        
        for merchant_account_id, settlement_data in merchant_settlements.items():
            try:
                # Create settlement transaction
                settlement_tx = await self._process_merchant_settlement(
                    merchant_account_id=merchant_account_id,
                    settlement_data=settlement_data,
                    initiated_by_id=initiated_by_id
                )
                
                settlement_results.append({
                    'merchant_account_id': merchant_account_id,
                    'status': 'completed',
                    'settlement_transaction_id': settlement_tx.id,
                    'net_amount': settlement_data['net_amount'],
                    'transactions_count': len(settlement_data['transactions'])
                })
                
                # Mark original transactions as settled
                for tx in settlement_data['transactions']:
                    tx.status = TransactionStatus.SETTLED
                    tx.settled_at = datetime.utcnow()
                
            except Exception as e:
                settlement_results.append({
                    'merchant_account_id': merchant_account_id,
                    'status': 'failed',
                    'error': str(e),
                    'net_amount': settlement_data['net_amount']
                })
        
        self.db_session.commit()
        
        return {
            'settlement_date': settlement_date.date(),
            'total_merchants': len(merchant_settlements),
            'successful_settlements': len([r for r in settlement_results if r['status'] == 'completed']),
            'failed_settlements': len([r for r in settlement_results if r['status'] == 'failed']),
            'details': settlement_results
        }

    async def _process_merchant_settlement(
        self,
        merchant_account_id: int,
        settlement_data: Dict,
        initiated_by_id: int
    ) -> Transaction:
        """Process individual merchant settlement."""
        
        # Get platform fee account
        platform_account = self.db_session.query(Account).filter_by(
            account_type=AccountType.PLATFORM_FEES
        ).first()
        
        if not platform_account:
            raise Exception("Platform fee account not configured")
        
        # Create settlement with fee deduction
        steps = []
        
        # Step 1: Transfer net amount to merchant
        if settlement_data['net_amount'] > 0:
            steps.append({
                'type': 'transfer',
                'from_account_id': platform_account.id,
                'to_account_id': merchant_account_id,
                'amount': settlement_data['net_amount'],
                'description': f"Settlement - Net Amount"
            })
        
        # Step 2: Record fee collection (if any)
        if settlement_data['fee_amount'] > 0:
            steps.append({
                'type': 'internal_transfer',
                'to_account_id': platform_account.id,
                'amount': settlement_data['fee_amount'],
                'description': f"Processing fees collected"
            })
        
        # Execute settlement transaction
        result = await transaction_orchestrator.execute_business_transaction(
            transaction_type='merchant_settlement',
            steps=steps,
            user_id=initiated_by_id,
            metadata={
                'merchant_account_id': merchant_account_id,
                'transactions_count': len(settlement_data['transactions']),
                'gross_amount': str(settlement_data['gross_amount']),
                'fee_amount': str(settlement_data['fee_amount'])
            }
        )
        
        if result['status'] != 'completed':
            raise Exception(f"Settlement failed: {result.get('error')}")
        
        return result['primary_transaction']


class RecurringPaymentExample:
    """Example: Automated recurring payment processing."""
    
    def __init__(self, db_session: Session):
        self.processor = TransactionProcessor(db_session)
        self.db_session = db_session

    async def setup_recurring_payment(
        self,
        payer_account_id: int,
        payee_account_id: int,
        amount: Decimal,
        frequency_days: int,
        description: str,
        initiated_by_id: int
    ) -> Dict[str, Any]:
        """Set up a new recurring payment."""
        
        # Create recurring payment record
        recurring_payment = RecurringPayment(
            payer_account_id=payer_account_id,
            payee_account_id=payee_account_id,
            amount=amount,
            frequency_days=frequency_days,
            description=description,
            is_active=True,
            next_execution_date=datetime.utcnow() + timedelta(days=frequency_days),
            created_by_id=initiated_by_id
        )
        
        self.db_session.add(recurring_payment)
        self.db_session.commit()
        
        return {
            'recurring_payment_id': recurring_payment.id,
            'status': 'active',
            'next_execution': recurring_payment.next_execution_date
        }

    async def process_due_recurring_payments(self) -> List[Dict[str, Any]]:
        """Process all recurring payments that are due."""
        
        current_time = datetime.utcnow()
        
        # Find all due recurring payments
        due_payments = self.db_session.query(RecurringPayment).filter(
            RecurringPayment.is_active == True,
            RecurringPayment.next_execution_date <= current_time
        ).all()
        
        results = []
        
        for recurring_payment in due_payments:
            try:
                # Process the payment
                transaction = self.processor.process_transfer(
                    from_account_id=recurring_payment.payer_account_id,
                    to_account_id=recurring_payment.payee_account_id,
                    amount=recurring_payment.amount,
                    description=f"Recurring: {recurring_payment.description}",
                    initiated_by_id=recurring_payment.created_by_id,
                    metadata={'recurring_payment_id': recurring_payment.id}
                )
                
                # Update next execution date
                recurring_payment.next_execution_date = (
                    current_time + timedelta(days=recurring_payment.frequency_days)
                )
                recurring_payment.last_executed_at = current_time
                recurring_payment.execution_count += 1
                
                results.append({
                    'recurring_payment_id': recurring_payment.id,
                    'status': 'completed',
                    'transaction_id': transaction.id,
                    'amount': transaction.amount,
                    'next_execution': recurring_payment.next_execution_date
                })
                
            except Exception as e:
                # Handle failures (insufficient funds, account issues, etc.)
                recurring_payment.failed_attempts += 1
                
                # Disable after too many failures
                if recurring_payment.failed_attempts >= 3:
                    recurring_payment.is_active = False
                    recurring_payment.deactivated_at = current_time
                    recurring_payment.deactivation_reason = f"Too many failed attempts: {str(e)}"
                else:
                    # Retry later (exponential backoff)
                    retry_delay = 2 ** recurring_payment.failed_attempts
                    recurring_payment.next_execution_date = (
                        current_time + timedelta(hours=retry_delay)
                    )
                
                results.append({
                    'recurring_payment_id': recurring_payment.id,
                    'status': 'failed',
                    'error': str(e),
                    'failed_attempts': recurring_payment.failed_attempts,
                    'retry_at': recurring_payment.next_execution_date if recurring_payment.is_active else None
                })
        
        self.db_session.commit()
        return results


class FraudDetectionExample:
    """Example: Real-time fraud detection and prevention."""
    
    def __init__(self, db_session: Session):
        self.processor = TransactionProcessor(db_session)
        self.db_session = db_session

    def analyze_transaction_risk(
        self, 
        account_id: int, 
        amount: Decimal, 
        transaction_type: TransactionType
    ) -> Dict[str, Any]:
        """
        Analyze transaction for fraud indicators.
        
        Returns risk score and recommended actions.
        """
        
        risk_factors = []
        risk_score = 0
        
        # Factor 1: Transaction amount analysis
        account = self.db_session.query(Account).filter_by(id=account_id).first()
        avg_transaction = self._get_average_transaction_amount(account_id)
        
        if amount > avg_transaction * 5:
            risk_factors.append("Amount significantly higher than average")
            risk_score += 30
        
        # Factor 2: Time-based analysis
        recent_count = self._get_recent_transaction_count(account_id, hours=1)
        if recent_count > 10:
            risk_factors.append("High frequency transactions")
            risk_score += 25
        
        # Factor 3: Account velocity
        daily_total = self._get_daily_transaction_total(account_id)
        if daily_total > account.daily_transaction_limit:
            risk_factors.append("Daily transaction limit exceeded")
            risk_score += 40
        
        # Factor 4: Geographic analysis (if available)
        last_location = self._get_last_transaction_location(account_id)
        if last_location and self._calculate_distance(last_location) > 1000:  # km
            risk_factors.append("Geographic anomaly detected")
            risk_score += 35
        
        # Determine action based on risk score
        if risk_score >= 70:
            action = "block_and_review"
        elif risk_score >= 40:
            action = "require_verification"
        elif risk_score >= 20:
            action = "monitor_closely"
        else:
            action = "proceed"
        
        return {
            'risk_score': risk_score,
            'risk_factors': risk_factors,
            'recommended_action': action,
            'requires_manual_review': risk_score >= 40
        }

    def _get_average_transaction_amount(self, account_id: int) -> Decimal:
        """Get average transaction amount for account over last 30 days."""
        from sqlalchemy import func
        
        thirty_days_ago = datetime.utcnow() - timedelta(days=30)
        
        result = self.db_session.query(
            func.avg(Transaction.amount).label('avg_amount')
        ).filter(
            Transaction.from_account_id == account_id,
            Transaction.created_at >= thirty_days_ago,
            Transaction.status == TransactionStatus.COMPLETED
        ).first()
        
        return result.avg_amount or Decimal('0')

    def _get_recent_transaction_count(self, account_id: int, hours: int) -> int:
        """Get count of transactions in recent hours."""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        count = self.db_session.query(Transaction).filter(
            Transaction.from_account_id == account_id,
            Transaction.created_at >= cutoff_time
        ).count()
        
        return count

    def _get_daily_transaction_total(self, account_id: int) -> Decimal:
        """Get total transaction amount for today."""
        from sqlalchemy import func
        
        today_start = datetime.utcnow().replace(hour=0, minute=0, second=0)
        
        result = self.db_session.query(
            func.sum(Transaction.amount).label('total')
        ).filter(
            Transaction.from_account_id == account_id,
            Transaction.created_at >= today_start,
            Transaction.status == TransactionStatus.COMPLETED
        ).first()
        
        return result.total or Decimal('0')


class FinancialReportingExample:
    """Example: Advanced financial reporting and analytics."""
    
    def __init__(self, db_session: Session):
        self.db_session = db_session

    def generate_account_statement(
        self,
        account_id: int,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """Generate detailed account statement."""
        from sqlalchemy import func, case, and_, or_
        
        # Get account information
        account = self.db_session.query(Account).filter_by(id=account_id).first()
        
        # Get transactions in date range
        transactions = self.db_session.query(Transaction).filter(
            or_(
                Transaction.from_account_id == account_id,
                Transaction.to_account_id == account_id
            ),
            Transaction.created_at >= start_date,
            Transaction.created_at <= end_date,
            Transaction.status == TransactionStatus.COMPLETED
        ).order_by(Transaction.created_at).all()
        
        # Calculate summary statistics
        summary_stats = self.db_session.query(
            func.count(Transaction.id).label('total_transactions'),
            func.sum(
                case(
                    (Transaction.from_account_id == account_id, Transaction.amount),
                    else_=Decimal('0')
                )
            ).label('total_debits'),
            func.sum(
                case(
                    (Transaction.to_account_id == account_id, Transaction.amount),
                    else_=Decimal('0')
                )
            ).label('total_credits'),
            func.avg(Transaction.amount).label('average_amount')
        ).filter(
            or_(
                Transaction.from_account_id == account_id,
                Transaction.to_account_id == account_id
            ),
            Transaction.created_at >= start_date,
            Transaction.created_at <= end_date,
            Transaction.status == TransactionStatus.COMPLETED
        ).first()
        
        # Get opening balance (from snapshot or calculation)
        opening_balance = self._get_opening_balance(account_id, start_date)
        closing_balance = account.balance
        
        # Build statement
        statement = {
            'account': {
                'id': account.id,
                'account_number': account.account_number,
                'account_type': account.account_type.value,
                'owner_id': account.owner_id
            },
            'period': {
                'start_date': start_date.date(),
                'end_date': end_date.date()
            },
            'summary': {
                'opening_balance': opening_balance,
                'closing_balance': closing_balance,
                'total_transactions': summary_stats.total_transactions or 0,
                'total_credits': summary_stats.total_credits or Decimal('0'),
                'total_debits': summary_stats.total_debits or Decimal('0'),
                'average_transaction': summary_stats.average_amount or Decimal('0'),
                'net_change': closing_balance - opening_balance
            },
            'transactions': [
                {
                    'id': tx.id,
                    'date': tx.created_at.date(),
                    'type': tx.transaction_type.value,
                    'description': tx.description,
                    'amount': tx.amount,
                    'direction': 'credit' if tx.to_account_id == account_id else 'debit',
                    'running_balance': self._calculate_running_balance(
                        account_id, tx.created_at, opening_balance
                    ),
                    'reference': tx.reference_id
                }
                for tx in transactions
            ]
        }
        
        return statement

    def _get_opening_balance(self, account_id: int, date: datetime) -> Decimal:
        """Get account balance at start of period."""
        # First, check if we have a balance snapshot close to the date
        closest_snapshot = self.db_session.query(BalanceSnapshot).filter(
            BalanceSnapshot.account_id == account_id,
            BalanceSnapshot.snapshot_date <= date.date()
        ).order_by(BalanceSnapshot.snapshot_date.desc()).first()
        
        if closest_snapshot:
            # Calculate from snapshot + transactions since snapshot
            balance = closest_snapshot.closing_balance
            
            transactions_since = self.db_session.query(Transaction).filter(
                or_(
                    Transaction.from_account_id == account_id,
                    Transaction.to_account_id == account_id
                ),
                Transaction.created_at > closest_snapshot.snapshot_date,
                Transaction.created_at < date,
                Transaction.status == TransactionStatus.COMPLETED
            ).all()
            
            for tx in transactions_since:
                if tx.to_account_id == account_id:
                    balance += tx.amount
                else:
                    balance -= tx.amount
            
            return balance
        else:
            # Calculate from account creation
            return self._calculate_balance_from_inception(account_id, date)

    def _calculate_running_balance(
        self, 
        account_id: int, 
        transaction_date: datetime, 
        opening_balance: Decimal
    ) -> Decimal:
        """Calculate running balance at specific transaction date."""
        from sqlalchemy import func, case, or_
        
        # Get all transactions up to this date
        balance_changes = self.db_session.query(
            func.sum(
                case(
                    (Transaction.to_account_id == account_id, Transaction.amount),
                    (Transaction.from_account_id == account_id, -Transaction.amount),
                    else_=Decimal('0')
                )
            ).label('net_change')
        ).filter(
            or_(
                Transaction.from_account_id == account_id,
                Transaction.to_account_id == account_id
            ),
            Transaction.created_at <= transaction_date,
            Transaction.status == TransactionStatus.COMPLETED
        ).first()
        
        net_change = balance_changes.net_change or Decimal('0')
        return opening_balance + net_change


# Best Practices Documentation
TRANSACTION_BEST_PRACTICES = """
## Transaction Management Best Practices

### 1. Database Design Principles

- **Decimal Precision**: Always use DECIMAL/NUMERIC types for monetary values
- **Constraints**: Implement check constraints for business rules at database level
- **Indexes**: Create appropriate indexes for query performance
- **Audit Trails**: Design comprehensive audit logging from the start

### 2. Concurrency Handling

- **Optimistic Locking**: Use version fields for conflict detection
- **Database Locks**: Use SELECT FOR UPDATE for critical sections
- **Retry Logic**: Implement exponential backoff for transient failures
- **Deadlock Prevention**: Always acquire locks in consistent order

### 3. Error Handling

- **Specific Exceptions**: Create domain-specific exception types
- **Graceful Degradation**: Handle partial failures appropriately
- **Compensation**: Implement compensation logic for complex transactions
- **Monitoring**: Log all failures with sufficient context

### 4. Performance Optimization

- **Batch Processing**: Group related operations for efficiency
- **Connection Pooling**: Configure appropriate connection pool sizes
- **Query Optimization**: Use indexes and efficient query patterns
- **Caching**: Cache frequently accessed reference data

### 5. Security Considerations

- **Authorization**: Verify permissions for every operation
- **Input Validation**: Sanitize and validate all inputs
- **Rate Limiting**: Implement transaction frequency limits
- **Fraud Detection**: Monitor for suspicious patterns

### 6. Compliance and Auditing

- **Complete Audit Trail**: Log all state changes with timestamps
- **Immutable Records**: Never delete financial records
- **Regulatory Reporting**: Design for compliance requirements
- **Data Retention**: Implement appropriate retention policies

### 7. Testing Strategies

- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test complete transaction flows
- **Concurrency Tests**: Verify behavior under concurrent load
- **Performance Tests**: Validate performance under expected load
- **Chaos Engineering**: Test system resilience to failures

### 8. Monitoring and Alerting

- **Transaction Metrics**: Monitor success rates, processing times
- **Error Tracking**: Alert on error rate increases
- **Balance Monitoring**: Detect balance discrepancies quickly
- **Performance Monitoring**: Track query performance and deadlocks
"""
```


---

## Section 6: Real-time Analytics Dashboard System

This section demonstrates building a sophisticated real-time analytics dashboard that showcases advanced SQLAlchemy querying patterns, window functions, materialized views, and real-time data streaming. We'll build a comprehensive analytics platform that monitors user behavior, transaction patterns, and system performance metrics.

### 6.1 Analytics Models and Schema Design

**app/models/analytics.py** - Advanced Analytics Models:
```python
"""
Analytics models demonstrating advanced SQLAlchemy patterns for real-time dashboards.
Features materialized views, window functions, and time-series data handling.
"""

import enum
from datetime import datetime, date, timedelta
from decimal import Decimal
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

from sqlalchemy import (
    Column, Integer, String, DateTime, Date, Boolean, Text,
    ForeignKey, Numeric, Enum, Index, UniqueConstraint,
    CheckConstraint, func, text, event
)
from sqlalchemy.orm import relationship, Session
from sqlalchemy.ext.hybrid import hybrid_property, hybrid_method
from sqlalchemy.dialects.postgresql import UUID, JSONB, ARRAY, TSVECTOR
from sqlalchemy.ext.declarative import declarative_base

from app.core.database import Base
from app.models.user import User
from app.models.transactions import Transaction, Account


class MetricType(enum.Enum):
    """Types of metrics we track."""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIME_SERIES = "time_series"


class AggregationPeriod(enum.Enum):
    """Time periods for metric aggregation."""
    MINUTE = "minute"
    HOUR = "hour"
    DAY = "day"
    WEEK = "week"
    MONTH = "month"
    QUARTER = "quarter"
    YEAR = "year"


class EventCategory(enum.Enum):
    """Categories of events we track."""
    USER_ACTION = "user_action"
    SYSTEM_EVENT = "system_event"
    TRANSACTION = "transaction"
    ERROR = "error"
    PERFORMANCE = "performance"


class Event(Base):
    """
    Base event tracking model for analytics.
    Demonstrates efficient event storage with JSONB for flexible metadata.
    """
    __tablename__ = 'events'
    
    id = Column(Integer, primary_key=True)
    event_id = Column(String(50), unique=True, nullable=False, index=True)
    
    # Event classification
    category = Column(Enum(EventCategory), nullable=False, index=True)
    event_type = Column(String(100), nullable=False, index=True)
    event_name = Column(String(200), nullable=False)
    
    # Timing information
    timestamp = Column(DateTime, nullable=False, default=datetime.utcnow, index=True)
    duration_ms = Column(Integer, nullable=True)  # For timed events
    
    # User and session tracking
    user_id = Column(Integer, ForeignKey('users.id'), nullable=True, index=True)
    session_id = Column(String(100), nullable=True, index=True)
    ip_address = Column(String(45), nullable=True)  # IPv6 compatible
    user_agent = Column(Text, nullable=True)
    
    # Flexible metadata storage
    metadata = Column(JSONB, nullable=True)
    tags = Column(ARRAY(String), nullable=True, index=True)
    
    # Relationships
    user = relationship("User", back_populates="events")
    
    # Performance indexes
    __table_args__ = (
        Index('ix_events_category_type', 'category', 'event_type'),
        Index('ix_events_timestamp_category', 'timestamp', 'category'),
        Index('ix_events_user_timestamp', 'user_id', 'timestamp'),
        Index('ix_events_tags_gin', 'tags', postgresql_using='gin'),
        Index('ix_events_metadata_gin', 'metadata', postgresql_using='gin'),
    )

    @hybrid_property
    def date(self) -> date:
        """Get the date component of the timestamp."""
        return self.timestamp.date()

    @hybrid_method
    def in_time_window(self, start: datetime, end: datetime) -> bool:
        """Check if event falls within time window."""
        return start <= self.timestamp < end

    @in_time_window.expression
    def in_time_window(cls, start: datetime, end: datetime):
        """SQL expression for time window check."""
        return (cls.timestamp >= start) & (cls.timestamp < end)


class MetricDefinition(Base):
    """
    Defines metrics that can be calculated and tracked.
    Enables dynamic metric configuration and calculation rules.
    """
    __tablename__ = 'metric_definitions'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), unique=True, nullable=False)
    display_name = Column(String(200), nullable=False)
    description = Column(Text, nullable=True)
    
    # Metric configuration
    metric_type = Column(Enum(MetricType), nullable=False)
    unit = Column(String(50), nullable=True)
    calculation_query = Column(Text, nullable=True)  # SQL for calculated metrics
    
    # Aggregation settings
    default_aggregation_period = Column(Enum(AggregationPeriod), nullable=False)
    supported_periods = Column(ARRAY(String), nullable=False)
    
    # Configuration
    is_active = Column(Boolean, default=True)
    retention_days = Column(Integer, default=90)
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    metric_values = relationship("MetricValue", back_populates="metric_definition")


class MetricValue(Base):
    """
    Stores calculated metric values with time-series support.
    Optimized for time-series queries and aggregations.
    """
    __tablename__ = 'metric_values'
    
    id = Column(Integer, primary_key=True)
    metric_definition_id = Column(Integer, ForeignKey('metric_definitions.id'), nullable=False)
    
    # Time series data
    timestamp = Column(DateTime, nullable=False, index=True)
    period = Column(Enum(AggregationPeriod), nullable=False)
    
    # Metric values (multiple types to handle different metric kinds)
    value_numeric = Column(Numeric(20, 6), nullable=True)
    value_integer = Column(Integer, nullable=True)
    value_text = Column(String(500), nullable=True)
    value_json = Column(JSONB, nullable=True)
    
    # Statistical measures
    count = Column(Integer, nullable=True)
    sum_value = Column(Numeric(20, 6), nullable=True)
    min_value = Column(Numeric(20, 6), nullable=True)
    max_value = Column(Numeric(20, 6), nullable=True)
    avg_value = Column(Numeric(20, 6), nullable=True)
    
    # Dimensional data
    dimensions = Column(JSONB, nullable=True)  # For grouping and filtering
    
    # Relationships
    metric_definition = relationship("MetricDefinition", back_populates="metric_values")
    
    # Constraints and indexes for performance
    __table_args__ = (
        UniqueConstraint('metric_definition_id', 'timestamp', 'period', 'dimensions'),
        Index('ix_metric_values_timestamp', 'timestamp'),
        Index('ix_metric_values_metric_timestamp', 'metric_definition_id', 'timestamp'),
        Index('ix_metric_values_dimensions_gin', 'dimensions', postgresql_using='gin'),
        CheckConstraint(
            '(value_numeric IS NOT NULL) OR (value_integer IS NOT NULL) OR '
            '(value_text IS NOT NULL) OR (value_json IS NOT NULL)',
            name='check_has_value'
        )
    )


class UserSession(Base):
    """
    Detailed user session tracking for analytics.
    Demonstrates session-based analytics with activity tracking.
    """
    __tablename__ = 'user_sessions'
    
    id = Column(Integer, primary_key=True)
    session_id = Column(String(100), unique=True, nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=True, index=True)
    
    # Session lifecycle
    started_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    ended_at = Column(DateTime, nullable=True)
    last_activity_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    
    # Session characteristics
    ip_address = Column(String(45), nullable=True)
    user_agent = Column(Text, nullable=True)
    device_type = Column(String(50), nullable=True)
    operating_system = Column(String(100), nullable=True)
    browser = Column(String(100), nullable=True)
    
    # Geographic data (if available)
    country_code = Column(String(2), nullable=True)
    city = Column(String(100), nullable=True)
    timezone = Column(String(50), nullable=True)
    
    # Calculated fields
    page_views = Column(Integer, default=0)
    transaction_count = Column(Integer, default=0)
    total_amount_transacted = Column(Numeric(15, 2), default=Decimal('0'))
    
    # Session metadata
    entry_page = Column(String(500), nullable=True)
    exit_page = Column(String(500), nullable=True)
    referrer = Column(String(500), nullable=True)
    
    # Relationships
    user = relationship("User", back_populates="sessions")
    page_views = relationship("PageView", back_populates="session")
    
    @hybrid_property
    def duration_minutes(self) -> Optional[float]:
        """Calculate session duration in minutes."""
        if self.ended_at and self.started_at:
            delta = self.ended_at - self.started_at
            return delta.total_seconds() / 60
        return None

    @duration_minutes.expression
    def duration_minutes(cls):
        """SQL expression for session duration."""
        return func.extract(
            'epoch', 
            func.coalesce(cls.ended_at, func.now()) - cls.started_at
        ) / 60

    @hybrid_property
    def is_active(self) -> bool:
        """Check if session is currently active."""
        if self.ended_at:
            return False
        
        # Consider inactive if no activity for 30 minutes
        inactive_threshold = datetime.utcnow() - timedelta(minutes=30)
        return self.last_activity_at > inactive_threshold

    @is_active.expression
    def is_active(cls):
        """SQL expression for active session check."""
        return (cls.ended_at.is_(None)) & (
            cls.last_activity_at > func.now() - text("INTERVAL '30 minutes'")
        )


class PageView(Base):
    """
    Individual page view tracking for user journey analysis.
    Optimized for real-time analytics queries.
    """
    __tablename__ = 'page_views'
    
    id = Column(Integer, primary_key=True)
    view_id = Column(String(50), unique=True, nullable=False, index=True)
    
    # Session and user context
    session_id = Column(String(100), ForeignKey('user_sessions.session_id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=True, index=True)
    
    # Page information
    url = Column(String(2000), nullable=False)
    page_title = Column(String(500), nullable=True)
    page_type = Column(String(100), nullable=True, index=True)  # dashboard, transaction, profile, etc.
    
    # Timing data
    viewed_at = Column(DateTime, nullable=False, default=datetime.utcnow, index=True)
    time_on_page_seconds = Column(Integer, nullable=True)
    
    # User interaction data
    scroll_depth_percent = Column(Integer, nullable=True)
    clicks_count = Column(Integer, default=0)
    form_submissions = Column(Integer, default=0)
    
    # Technical data
    load_time_ms = Column(Integer, nullable=True)
    referrer = Column(String(2000), nullable=True)
    
    # Flexible metadata
    custom_data = Column(JSONB, nullable=True)
    
    # Relationships
    session = relationship("UserSession", back_populates="page_views")
    user = relationship("User")
    
    # Performance indexes
    __table_args__ = (
        Index('ix_page_views_user_date', 'user_id', func.date('viewed_at')),
        Index('ix_page_views_session_time', 'session_id', 'viewed_at'),
        Index('ix_page_views_page_type_date', 'page_type', func.date('viewed_at')),
    )


class SystemMetrics(Base):
    """
    System performance and health metrics.
    Demonstrates time-series data storage for monitoring.
    """
    __tablename__ = 'system_metrics'
    
    id = Column(Integer, primary_key=True)
    
    # Timing
    timestamp = Column(DateTime, nullable=False, default=datetime.utcnow, index=True)
    collection_interval_seconds = Column(Integer, default=60)
    
    # Database metrics
    active_connections = Column(Integer, nullable=True)
    database_size_mb = Column(Numeric(10, 2), nullable=True)
    slow_query_count = Column(Integer, default=0)
    avg_query_time_ms = Column(Numeric(10, 3), nullable=True)
    
    # Application metrics
    active_user_sessions = Column(Integer, default=0)
    requests_per_minute = Column(Integer, default=0)
    error_rate_percent = Column(Numeric(5, 2), default=Decimal('0'))
    
    # Transaction system metrics
    transactions_per_minute = Column(Integer, default=0)
    pending_transaction_count = Column(Integer, default=0)
    failed_transaction_count = Column(Integer, default=0)
    avg_transaction_processing_ms = Column(Numeric(10, 3), nullable=True)
    
    # Resource utilization
    cpu_usage_percent = Column(Numeric(5, 2), nullable=True)
    memory_usage_percent = Column(Numeric(5, 2), nullable=True)
    disk_usage_percent = Column(Numeric(5, 2), nullable=True)
    
    # Network metrics
    network_in_mbps = Column(Numeric(10, 3), nullable=True)
    network_out_mbps = Column(Numeric(10, 3), nullable=True)
    
    # Custom metrics
    custom_metrics = Column(JSONB, nullable=True)
    
    # Partitioning by date for performance
    __table_args__ = (
        Index('ix_system_metrics_timestamp_desc', timestamp.desc()),
        Index('ix_system_metrics_date', func.date('timestamp')),
    )


class UserActivitySummary(Base):
    """
    Pre-aggregated user activity data for fast dashboard queries.
    Demonstrates materialized view pattern with SQLAlchemy.
    """
    __tablename__ = 'user_activity_summary'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    date = Column(Date, nullable=False, index=True)
    
    # Session metrics
    session_count = Column(Integer, default=0)
    total_session_duration_minutes = Column(Integer, default=0)
    avg_session_duration_minutes = Column(Numeric(8, 2), default=Decimal('0'))
    
    # Page activity
    page_views = Column(Integer, default=0)
    unique_pages_viewed = Column(Integer, default=0)
    total_time_on_site_minutes = Column(Integer, default=0)
    
    # Transaction activity
    transaction_count = Column(Integer, default=0)
    total_transaction_amount = Column(Numeric(15, 2), default=Decimal('0'))
    deposit_count = Column(Integer, default=0)
    withdrawal_count = Column(Integer, default=0)
    transfer_count = Column(Integer, default=0)
    
    # Engagement metrics
    clicks_count = Column(Integer, default=0)
    form_submissions = Column(Integer, default=0)
    feature_usage_count = Column(Integer, default=0)
    
    # Device and location
    device_types = Column(ARRAY(String), nullable=True)
    countries = Column(ARRAY(String), nullable=True)
    
    # Calculated at end of day
    calculated_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    
    # Relationships
    user = relationship("User")
    
    # Unique constraint and indexes
    __table_args__ = (
        UniqueConstraint('user_id', 'date', name='uq_user_activity_date'),
        Index('ix_user_activity_date_user', 'date', 'user_id'),
        Index('ix_user_activity_transaction_count', 'transaction_count'),
    )


class DashboardWidget(Base):
    """
    Configurable dashboard widget definitions.
    Demonstrates dynamic dashboard configuration storage.
    """
    __tablename__ = 'dashboard_widgets'
    
    id = Column(Integer, primary_key=True)
    widget_id = Column(String(50), unique=True, nullable=False)
    
    # Widget identification
    title = Column(String(200), nullable=False)
    description = Column(Text, nullable=True)
    widget_type = Column(String(50), nullable=False)  # chart, table, metric, etc.
    
    # Layout configuration
    position_x = Column(Integer, default=0)
    position_y = Column(Integer, default=0)
    width = Column(Integer, default=4)
    height = Column(Integer, default=3)
    
    # Data configuration
    data_source = Column(String(100), nullable=False)  # SQL query name or endpoint
    refresh_interval_seconds = Column(Integer, default=300)  # 5 minutes default
    
    # Widget configuration
    chart_config = Column(JSONB, nullable=True)  # Chart.js config, etc.
    filter_config = Column(JSONB, nullable=True)  # User-configurable filters
    
    # Permissions and access
    required_permissions = Column(ARRAY(String), nullable=True)
    visible_to_roles = Column(ARRAY(String), nullable=True)
    
    # Lifecycle
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    user_widgets = relationship("UserDashboardWidget", back_populates="widget")


class UserDashboardWidget(Base):
    """
    User-specific widget configurations and customizations.
    Enables personalized dashboard layouts.
    """
    __tablename__ = 'user_dashboard_widgets'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    widget_id = Column(Integer, ForeignKey('dashboard_widgets.id'), nullable=False)
    
    # User customizations
    custom_title = Column(String(200), nullable=True)
    custom_position_x = Column(Integer, nullable=True)
    custom_position_y = Column(Integer, nullable=True)
    custom_width = Column(Integer, nullable=True)
    custom_height = Column(Integer, nullable=True)
    
    # User preferences
    custom_config = Column(JSONB, nullable=True)
    custom_filters = Column(JSONB, nullable=True)
    refresh_interval_override = Column(Integer, nullable=True)
    
    # State
    is_visible = Column(Boolean, default=True)
    is_collapsed = Column(Boolean, default=False)
    
    # Metadata
    added_at = Column(DateTime, default=datetime.utcnow)
    last_accessed_at = Column(DateTime, nullable=True)
    
    # Relationships
    user = relationship("User")
    widget = relationship("DashboardWidget", back_populates="user_widgets")
    
    # Unique constraint
    __table_args__ = (
        UniqueConstraint('user_id', 'widget_id', name='uq_user_widget'),
    )


# Materialized Views for Performance
class TransactionSummaryDaily(Base):
    """
    Materialized view for daily transaction summaries.
    Demonstrates materialized view pattern with SQLAlchemy.
    """
    __tablename__ = 'mv_transaction_summary_daily'
    
    # Note: This would be created as a materialized view in PostgreSQL
    # We define it as a regular table here for SQLAlchemy mapping
    
    date = Column(Date, primary_key=True)
    
    # Transaction counts by type
    total_transactions = Column(Integer, default=0)
    deposit_count = Column(Integer, default=0)
    withdrawal_count = Column(Integer, default=0)
    transfer_count = Column(Integer, default=0)
    
    # Transaction amounts
    total_amount = Column(Numeric(20, 2), default=Decimal('0'))
    deposit_amount = Column(Numeric(20, 2), default=Decimal('0'))
    withdrawal_amount = Column(Numeric(20, 2), default=Decimal('0'))
    transfer_amount = Column(Numeric(20, 2), default=Decimal('0'))
    
    # Status breakdown
    completed_count = Column(Integer, default=0)
    pending_count = Column(Integer, default=0)
    failed_count = Column(Integer, default=0)
    reversed_count = Column(Integer, default=0)
    
    # Performance metrics
    avg_processing_time_ms = Column(Numeric(10, 3), nullable=True)
    max_processing_time_ms = Column(Numeric(10, 3), nullable=True)
    
    # Account activity
    active_accounts = Column(Integer, default=0)
    new_accounts = Column(Integer, default=0)
    
    # Additional metadata
    updated_at = Column(DateTime, default=datetime.utcnow)


# Event handlers for real-time metric updates
@event.listens_for(Transaction, 'after_insert')
def update_metrics_on_transaction(mapper, connection, target):
    """Update real-time metrics when new transaction is created."""
    
    # This would trigger metric recalculation
    # In practice, this might queue a background task
    pass


@event.listens_for(UserSession, 'after_update')
def update_session_metrics(mapper, connection, target):
    """Update session metrics when session is updated."""
    
    # Update real-time user activity metrics
    pass


# Time-series partitioning helpers
def create_time_partitions(table_name: str, start_date: date, end_date: date):
    """
    Create time-based partitions for large tables.
    Useful for events and metrics tables with high volume.
    """
    
    partition_queries = []
    current_date = start_date
    
    while current_date <= end_date:
        partition_name = f"{table_name}_{current_date.strftime('%Y_%m')}"
        start_of_month = current_date.replace(day=1)
        
        # Calculate end of month
        if current_date.month == 12:
            end_of_month = current_date.replace(year=current_date.year + 1, month=1, day=1)
        else:
            end_of_month = current_date.replace(month=current_date.month + 1, day=1)
        
        partition_query = f"""
        CREATE TABLE IF NOT EXISTS {partition_name} 
        PARTITION OF {table_name}
        FOR VALUES FROM ('{start_of_month}') TO ('{end_of_month}');
        """
        
        partition_queries.append(partition_query)
        
        # Move to next month
        if current_date.month == 12:
            current_date = current_date.replace(year=current_date.year + 1, month=1)
        else:
            current_date = current_date.replace(month=current_date.month + 1)
    
    return partition_queries
```


### 6.2 Advanced Query Service for Analytics

**app/services/analytics_service.py** - Advanced Analytics Queries:
```python
"""
Advanced analytics service demonstrating complex SQLAlchemy queries,
window functions, CTEs, and real-time aggregations.
"""

import asyncio
from datetime import datetime, date, timedelta
from decimal import Decimal
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass

from sqlalchemy import (
    func, case, cast, extract, text, and_, or_, not_,
    select, union_all, literal_column, desc, asc
)
from sqlalchemy.orm import Session, aliased
from sqlalchemy.dialects.postgresql import aggregate_order_by
from sqlalchemy.sql import operators
from sqlalchemy.sql.expression import literal

from app.models.analytics import (
    Event, MetricDefinition, MetricValue, UserSession, PageView,
    SystemMetrics, UserActivitySummary, TransactionSummaryDaily,
    EventCategory, MetricType, AggregationPeriod
)
from app.models.transactions import Transaction, Account, TransactionStatus, TransactionType
from app.models.user import User
from app.core.database import get_db_session


@dataclass
class TimeSeriesPoint:
    """Data structure for time series data points."""
    timestamp: datetime
    value: Decimal
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class AnalyticsFilter:
    """Filter configuration for analytics queries."""
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    user_ids: Optional[List[int]] = None
    event_types: Optional[List[str]] = None
    page_types: Optional[List[str]] = None
    transaction_types: Optional[List[TransactionType]] = None
    dimensions: Optional[Dict[str, Any]] = None


class AnalyticsQueryService:
    """
    Advanced analytics query service with complex SQLAlchemy patterns.
    Demonstrates window functions, CTEs, and advanced aggregations.
    """
    
    def __init__(self, db_session: Session):
        self.db_session = db_session

    def get_user_engagement_metrics(
        self, 
        period: AggregationPeriod = AggregationPeriod.DAY,
        days_back: int = 30
    ) -> List[Dict[str, Any]]:
        """
        Get comprehensive user engagement metrics using window functions.
        Demonstrates advanced SQLAlchemy query patterns.
        """
        
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=days_back)
        
        # Build time series with window functions
        base_query = (
            self.db_session.query(
                # Time grouping based on period
                func.date_trunc(period.value, UserActivitySummary.date).label('time_bucket'),
                
                # Basic engagement metrics
                func.sum(UserActivitySummary.session_count).label('total_sessions'),
                func.sum(UserActivitySummary.page_views).label('total_page_views'),
                func.count(func.distinct(UserActivitySummary.user_id)).label('unique_users'),
                
                # Duration metrics
                func.avg(UserActivitySummary.avg_session_duration_minutes).label('avg_session_duration'),
                func.sum(UserActivitySummary.total_time_on_site_minutes).label('total_time_on_site'),
                
                # Transaction engagement
                func.sum(UserActivitySummary.transaction_count).label('total_transactions'),
                func.sum(UserActivitySummary.total_transaction_amount).label('total_transaction_value'),
                
                # Window functions for trends
                func.lag(
                    func.sum(UserActivitySummary.session_count), 1
                ).over(
                    order_by=func.date_trunc(period.value, UserActivitySummary.date)
                ).label('previous_period_sessions'),
                
                func.row_number().over(
                    order_by=func.date_trunc(period.value, UserActivitySummary.date)
                ).label('period_rank')
            )
            .filter(
                UserActivitySummary.date >= start_date.date(),
                UserActivitySummary.date <= end_date.date()
            )
            .group_by(func.date_trunc(period.value, UserActivitySummary.date))
            .order_by(func.date_trunc(period.value, UserActivitySummary.date))
        )
        
        results = base_query.all()
        
        # Process results with calculated fields
        metrics = []
        for row in results:
            # Calculate period-over-period growth
            growth_rate = None
            if row.previous_period_sessions and row.previous_period_sessions > 0:
                growth_rate = (
                    (row.total_sessions - row.previous_period_sessions) / 
                    row.previous_period_sessions * 100
                )
            
            metrics.append({
                'period': row.time_bucket,
                'unique_users': row.unique_users,
                'total_sessions': row.total_sessions,
                'total_page_views': row.total_page_views,
                'avg_session_duration': float(row.avg_session_duration or 0),
                'total_time_on_site': row.total_time_on_site,
                'total_transactions': row.total_transactions,
                'total_transaction_value': float(row.total_transaction_value or 0),
                'session_growth_rate': float(growth_rate or 0),
                'pages_per_session': (
                    float(row.total_page_views / row.total_sessions) 
                    if row.total_sessions > 0 else 0
                )
            })
        
        return metrics

    def get_transaction_analytics_advanced(
        self, 
        filters: AnalyticsFilter
    ) -> Dict[str, Any]:
        """
        Advanced transaction analytics with multiple dimensions.
        Demonstrates complex aggregations and window functions.
        """
        
        # Base transaction query with filters
        base_filter = [Transaction.status == TransactionStatus.COMPLETED]
        
        if filters.start_date:
            base_filter.append(Transaction.created_at >= filters.start_date)
        if filters.end_date:
            base_filter.append(Transaction.created_at <= filters.end_date)
        if filters.transaction_types:
            base_filter.append(Transaction.transaction_type.in_(filters.transaction_types))
        
        # Main analytics query with window functions
        analytics_query = (
            self.db_session.query(
                # Basic metrics
                func.count(Transaction.id).label('total_transactions'),
                func.sum(Transaction.amount).label('total_volume'),
                func.avg(Transaction.amount).label('avg_transaction_size'),
                func.min(Transaction.amount).label('min_transaction'),
                func.max(Transaction.amount).label('max_transaction'),
                
                # Percentiles using window functions
                func.percentile_cont(0.25).within_group(
                    Transaction.amount.asc()
                ).label('p25_transaction_size'),
                func.percentile_cont(0.50).within_group(
                    Transaction.amount.asc()
                ).label('p50_transaction_size'),
                func.percentile_cont(0.75).within_group(
                    Transaction.amount.asc()
                ).label('p75_transaction_size'),
                func.percentile_cont(0.95).within_group(
                    Transaction.amount.asc()
                ).label('p95_transaction_size'),
                
                # Transaction type breakdown
                func.sum(
                    case((Transaction.transaction_type == TransactionType.DEPOSIT, 1), else_=0)
                ).label('deposit_count'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.WITHDRAWAL, 1), else_=0)
                ).label('withdrawal_count'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.TRANSFER, 1), else_=0)
                ).label('transfer_count'),
                
                # Amount breakdown by type
                func.sum(
                    case((Transaction.transaction_type == TransactionType.DEPOSIT, Transaction.amount), else_=0)
                ).label('deposit_volume'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.WITHDRAWAL, Transaction.amount), else_=0)
                ).label('withdrawal_volume'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.TRANSFER, Transaction.amount), else_=0)
                ).label('transfer_volume'),
                
                # Unique accounts involved
                func.count(func.distinct(Transaction.from_account_id)).label('unique_source_accounts'),
                func.count(func.distinct(Transaction.to_account_id)).label('unique_dest_accounts'),
                func.count(func.distinct(
                    func.coalesce(Transaction.from_account_id, Transaction.to_account_id)
                )).label('total_unique_accounts')
            )
            .filter(*base_filter)
        )
        
        result = analytics_query.first()
        
        # Time-series breakdown
        time_series = self._get_transaction_time_series(filters)
        
        # Top accounts by transaction volume
        top_accounts = self._get_top_accounts_by_volume(filters, limit=10)
        
        # Transaction patterns analysis
        patterns = self._analyze_transaction_patterns(filters)
        
        return {
            'summary': {
                'total_transactions': result.total_transactions,
                'total_volume': float(result.total_volume or 0),
                'avg_transaction_size': float(result.avg_transaction_size or 0),
                'min_transaction': float(result.min_transaction or 0),
                'max_transaction': float(result.max_transaction or 0),
                'unique_accounts': result.total_unique_accounts
            },
            'percentiles': {
                'p25': float(result.p25_transaction_size or 0),
                'p50': float(result.p50_transaction_size or 0),
                'p75': float(result.p75_transaction_size or 0),
                'p95': float(result.p95_transaction_size or 0)
            },
            'by_type': {
                'deposits': {
                    'count': result.deposit_count,
                    'volume': float(result.deposit_volume or 0)
                },
                'withdrawals': {
                    'count': result.withdrawal_count,
                    'volume': float(result.withdrawal_volume or 0)
                },
                'transfers': {
                    'count': result.transfer_count,
                    'volume': float(result.transfer_volume or 0)
                }
            },
            'time_series': time_series,
            'top_accounts': top_accounts,
            'patterns': patterns
        }

    def _get_transaction_time_series(
        self, 
        filters: AnalyticsFilter,
        interval: str = 'hour'
    ) -> List[Dict[str, Any]]:
        """Get transaction volume over time using window functions."""
        
        base_filter = [Transaction.status == TransactionStatus.COMPLETED]
        
        if filters.start_date:
            base_filter.append(Transaction.created_at >= filters.start_date)
        if filters.end_date:
            base_filter.append(Transaction.created_at <= filters.end_date)
        
        # Time series query with rolling averages
        time_series_query = (
            self.db_session.query(
                func.date_trunc(interval, Transaction.created_at).label('time_bucket'),
                func.count(Transaction.id).label('transaction_count'),
                func.sum(Transaction.amount).label('total_volume'),
                func.avg(Transaction.amount).label('avg_amount'),
                
                # Rolling 24-hour average using window function
                func.avg(func.count(Transaction.id)).over(
                    order_by=func.date_trunc(interval, Transaction.created_at),
                    rows=(23, 0)  # 24 hour window for hourly data
                ).label('rolling_avg_count'),
                
                func.avg(func.sum(Transaction.amount)).over(
                    order_by=func.date_trunc(interval, Transaction.created_at),
                    rows=(23, 0)
                ).label('rolling_avg_volume')
            )
            .filter(*base_filter)
            .group_by(func.date_trunc(interval, Transaction.created_at))
            .order_by(func.date_trunc(interval, Transaction.created_at))
        )
        
        results = time_series_query.all()
        
        return [
            {
                'timestamp': row.time_bucket,
                'transaction_count': row.transaction_count,
                'total_volume': float(row.total_volume or 0),
                'avg_amount': float(row.avg_amount or 0),
                'rolling_avg_count': float(row.rolling_avg_count or 0),
                'rolling_avg_volume': float(row.rolling_avg_volume or 0)
            }
            for row in results
        ]

    def _get_top_accounts_by_volume(
        self, 
        filters: AnalyticsFilter, 
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """Get top accounts by transaction volume with ranking."""
        
        # Subquery for account transaction summaries
        account_summary = (
            self.db_session.query(
                func.coalesce(Transaction.from_account_id, Transaction.to_account_id).label('account_id'),
                func.count(Transaction.id).label('transaction_count'),
                func.sum(Transaction.amount).label('total_volume'),
                func.avg(Transaction.amount).label('avg_transaction'),
                
                # Count by transaction type
                func.sum(
                    case((Transaction.transaction_type == TransactionType.DEPOSIT, 1), else_=0)
                ).label('deposits'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.WITHDRAWAL, 1), else_=0)
                ).label('withdrawals'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.TRANSFER, 1), else_=0)
                ).label('transfers')
            )
            .filter(
                Transaction.status == TransactionStatus.COMPLETED,
                Transaction.created_at >= (filters.start_date or datetime.utcnow() - timedelta(days=30))
            )
            .group_by(func.coalesce(Transaction.from_account_id, Transaction.to_account_id))
            .subquery()
        )
        
        # Main query with account details and ranking
        top_accounts_query = (
            self.db_session.query(
                Account.id,
                Account.account_number,
                Account.account_type,
                User.username,
                User.full_name,
                account_summary.c.transaction_count,
                account_summary.c.total_volume,
                account_summary.c.avg_transaction,
                account_summary.c.deposits,
                account_summary.c.withdrawals,
                account_summary.c.transfers,
                
                # Ranking
                func.row_number().over(
                    order_by=account_summary.c.total_volume.desc()
                ).label('volume_rank'),
                
                func.row_number().over(
                    order_by=account_summary.c.transaction_count.desc()
                ).label('count_rank')
            )
            .join(account_summary, Account.id == account_summary.c.account_id)
            .join(User, Account.owner_id == User.id)
            .order_by(account_summary.c.total_volume.desc())
            .limit(limit)
        )
        
        results = top_accounts_query.all()
        
        return [
            {
                'account_id': row.id,
                'account_number': row.account_number,
                'account_type': row.account_type.value,
                'owner': {
                    'username': row.username,
                    'full_name': row.full_name
                },
                'metrics': {
                    'transaction_count': row.transaction_count,
                    'total_volume': float(row.total_volume),
                    'avg_transaction': float(row.avg_transaction),
                    'deposits': row.deposits,
                    'withdrawals': row.withdrawals,
                    'transfers': row.transfers
                },
                'rankings': {
                    'volume_rank': row.volume_rank,
                    'count_rank': row.count_rank
                }
            }
            for row in results
        ]

    def _analyze_transaction_patterns(self, filters: AnalyticsFilter) -> Dict[str, Any]:
        """Analyze transaction patterns using advanced SQL patterns."""
        
        # Hourly distribution analysis
        hourly_distribution = (
            self.db_session.query(
                extract('hour', Transaction.created_at).label('hour'),
                func.count(Transaction.id).label('transaction_count'),
                func.sum(Transaction.amount).label('total_volume'),
                func.avg(Transaction.amount).label('avg_amount')
            )
            .filter(
                Transaction.status == TransactionStatus.COMPLETED,
                Transaction.created_at >= (filters.start_date or datetime.utcnow() - timedelta(days=7))
            )
            .group_by(extract('hour', Transaction.created_at))
            .order_by(extract('hour', Transaction.created_at))
        ).all()
        
        # Day of week analysis
        dow_analysis = (
            self.db_session.query(
                extract('dow', Transaction.created_at).label('day_of_week'),
                func.count(Transaction.id).label('transaction_count'),
                func.sum(Transaction.amount).label('total_volume')
            )
            .filter(
                Transaction.status == TransactionStatus.COMPLETED,
                Transaction.created_at >= (filters.start_date or datetime.utcnow() - timedelta(days=30))
            )
            .group_by(extract('dow', Transaction.created_at))
            .order_by(extract('dow', Transaction.created_at))
        ).all()
        
        # Transaction size distribution
        size_distribution = (
            self.db_session.query(
                case(
                    (Transaction.amount <= 50, 'Small (≤$50)'),
                    (Transaction.amount <= 500, 'Medium ($50-$500)'),
                    (Transaction.amount <= 5000, 'Large ($500-$5K)'),
                    else_='Very Large (>$5K)'
                ).label('size_category'),
                func.count(Transaction.id).label('count'),
                func.sum(Transaction.amount).label('volume'),
                func.round(
                    func.count(Transaction.id) * 100.0 / 
                    func.sum(func.count(Transaction.id)).over(), 2
                ).label('count_percentage'),
                func.round(
                    func.sum(Transaction.amount) * 100.0 / 
                    func.sum(func.sum(Transaction.amount)).over(), 2
                ).label('volume_percentage')
            )
            .filter(
                Transaction.status == TransactionStatus.COMPLETED,
                Transaction.created_at >= (filters.start_date or datetime.utcnow() - timedelta(days=30))
            )
            .group_by(
                case(
                    (Transaction.amount <= 50, 'Small (≤$50)'),
                    (Transaction.amount <= 500, 'Medium ($50-$500)'),
                    (Transaction.amount <= 5000, 'Large ($500-$5K)'),
                    else_='Very Large (>$5K)'
                )
            )
        ).all()
        
        return {
            'hourly_distribution': [
                {
                    'hour': int(row.hour),
                    'transaction_count': row.transaction_count,
                    'total_volume': float(row.total_volume),
                    'avg_amount': float(row.avg_amount)
                }
                for row in hourly_distribution
            ],
            'day_of_week_analysis': [
                {
                    'day_of_week': int(row.day_of_week),
                    'transaction_count': row.transaction_count,
                    'total_volume': float(row.total_volume)
                }
                for row in dow_analysis
            ],
            'size_distribution': [
                {
                    'category': row.size_category,
                    'count': row.count,
                    'volume': float(row.volume),
                    'count_percentage': float(row.count_percentage),
                    'volume_percentage': float(row.volume_percentage)
                }
                for row in size_distribution
            ]
        }

    def get_user_journey_analysis(
        self, 
        user_id: Optional[int] = None,
        days_back: int = 7
    ) -> Dict[str, Any]:
        """
        Analyze user journeys through the application.
        Demonstrates advanced session and page view analytics.
        """
        
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=days_back)
        
        base_filter = [
            UserSession.started_at >= start_date,
            UserSession.started_at <= end_date
        ]
        
        if user_id:
            base_filter.append(UserSession.user_id == user_id)
        
        # Session funnel analysis
        funnel_query = (
            self.db_session.query(
                UserSession.id.label('session_id'),
                UserSession.user_id,
                UserSession.started_at,
                UserSession.duration_minutes,
                
                # Page view sequence using window functions
                func.string_agg(
                    PageView.page_type,
                    ' -> '
                ).over(
                    partition_by=UserSession.id,
                    order_by=PageView.viewed_at,
                    rows=(None, None)
                ).label('page_journey'),
                
                # Session metrics
                func.count(PageView.id).over(
                    partition_by=UserSession.id
                ).label('total_page_views'),
                
                func.sum(PageView.time_on_page_seconds).over(
                    partition_by=UserSession.id
                ).label('total_engagement_time'),
                
                # Conversion indicators
                func.bool_or(
                    PageView.page_type == 'transaction'
                ).over(
                    partition_by=UserSession.id
                ).label('visited_transaction_page'),
                
                func.count(
                    case((PageView.form_submissions > 0, 1), else_=None)
                ).over(
                    partition_by=UserSession.id
                ).label('form_submissions_in_session')
            )
            .outerjoin(PageView, UserSession.session_id == PageView.session_id)
            .filter(*base_filter)
        )
        
        # Common entry pages
        entry_pages = (
            self.db_session.query(
                UserSession.entry_page,
                func.count(UserSession.id).label('sessions'),
                func.avg(UserSession.duration_minutes).label('avg_duration'),
                func.count(
                    case((UserSession.transaction_count > 0, 1), else_=None)
                ).label('converting_sessions')
            )
            .filter(*base_filter)
            .group_by(UserSession.entry_page)
            .order_by(func.count(UserSession.id).desc())
            .limit(10)
        ).all()
        
        # Exit page analysis
        exit_pages = (
            self.db_session.query(
                UserSession.exit_page,
                func.count(UserSession.id).label('sessions'),
                func.avg(UserSession.duration_minutes).label('avg_duration_before_exit')
            )
            .filter(
                *base_filter,
                UserSession.exit_page.isnot(None)
            )
            .group_by(UserSession.exit_page)
            .order_by(func.count(UserSession.id).desc())
            .limit(10)
        ).all()
        
        return {
            'entry_pages': [
                {
                    'page': row.entry_page,
                    'sessions': row.sessions,
                    'avg_duration': float(row.avg_duration or 0),
                    'conversion_rate': float(row.converting_sessions / row.sessions * 100)
                }
                for row in entry_pages
            ],
            'exit_pages': [
                {
                    'page': row.exit_page,
                    'sessions': row.sessions,
                    'avg_duration_before_exit': float(row.avg_duration_before_exit or 0)
                }
                for row in exit_pages
            ]
        }

    def get_cohort_analysis(
        self,
        cohort_period: AggregationPeriod = AggregationPeriod.MONTH,
        analysis_period: AggregationPeriod = AggregationPeriod.WEEK
    ) -> Dict[str, Any]:
        """
        Perform cohort analysis for user retention.
        Demonstrates advanced window functions and date arithmetic.
        """
        
        # Define cohort start (user registration month)
        user_cohorts = (
            self.db_session.query(
                User.id.label('user_id'),
                func.date_trunc(cohort_period.value, User.created_at).label('cohort_month'),
                User.created_at
            )
            .subquery()
        )
        
        # User activity by period
        user_activity = (
            self.db_session.query(
                UserActivitySummary.user_id,
                func.date_trunc(analysis_period.value, UserActivitySummary.date).label('activity_period'),
                func.sum(UserActivitySummary.session_count).label('sessions'),
                func.sum(UserActivitySummary.transaction_count).label('transactions')
            )
            .group_by(
                UserActivitySummary.user_id,
                func.date_trunc(analysis_period.value, UserActivitySummary.date)
            )
            .subquery()
        )
        
        # Cohort analysis query
        cohort_query = (
            self.db_session.query(
                user_cohorts.c.cohort_month,
                user_activity.c.activity_period,
                
                # Calculate period offset from cohort start
                func.extract(
                    'epoch',
                    user_activity.c.activity_period - user_cohorts.c.cohort_month
                ).label('period_offset'),
                
                # Cohort metrics
                func.count(func.distinct(user_cohorts.c.user_id)).label('cohort_size'),
                func.count(func.distinct(user_activity.c.user_id)).label('active_users'),
                func.sum(user_activity.c.sessions).label('total_sessions'),
                func.sum(user_activity.c.transactions).label('total_transactions'),
                
                # Retention rate calculation
                func.round(
                    func.count(func.distinct(user_activity.c.user_id)) * 100.0 /
                    func.count(func.distinct(user_cohorts.c.user_id)), 2
                ).label('retention_rate')
            )
            .outerjoin(
                user_activity,
                and_(
                    user_cohorts.c.user_id == user_activity.c.user_id,
                    user_activity.c.activity_period >= user_cohorts.c.cohort_month
                )
            )
            .group_by(
                user_cohorts.c.cohort_month,
                user_activity.c.activity_period
            )
            .order_by(
                user_cohorts.c.cohort_month,
                user_activity.c.activity_period
            )
        )
        
        results = cohort_query.all()
        
        # Group results by cohort
        cohorts = {}
        for row in results:
            cohort_key = row.cohort_month.strftime('%Y-%m')
            if cohort_key not in cohorts:
                cohorts[cohort_key] = {
                    'cohort_month': row.cohort_month,
                    'cohort_size': row.cohort_size,
                    'periods': []
                }
            
            if row.activity_period:  # Only add periods with activity
                cohorts[cohort_key]['periods'].append({
                    'period': row.activity_period,
                    'period_offset': int(row.period_offset or 0),
                    'active_users': row.active_users,
                    'retention_rate': float(row.retention_rate or 0),
                    'avg_sessions': float(row.total_sessions / row.active_users) if row.active_users > 0 else 0,
                    'avg_transactions': float(row.total_transactions / row.active_users) if row.active_users > 0 else 0
                })
        
        return {
            'cohorts': list(cohorts.values()),
            'analysis_config': {
                'cohort_period': cohort_period.value,
                'analysis_period': analysis_period.value
            }
        }

    def get_real_time_dashboard_data(self) -> Dict[str, Any]:
        """
        Get real-time dashboard data with current system state.
        Optimized for frequent polling with minimal latency.
        """
        
        now = datetime.utcnow()
        last_hour = now - timedelta(hours=1)
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # Real-time counters using efficient queries
        current_stats = (
            self.db_session.query(
                # Active sessions count
                func.count(
                    case((UserSession.is_active == True, UserSession.id), else_=None)
                ).label('active_sessions'),
                
                # Transactions in last hour
                func.count(
                    case((Transaction.created_at >= last_hour, Transaction.id), else_=None)
                ).label('transactions_last_hour'),
                
                # Today's totals
                func.count(
                    case((Transaction.created_at >= today_start, Transaction.id), else_=None)
                ).label('transactions_today'),
                func.sum(
                    case((Transaction.created_at >= today_start, Transaction.amount), else_=0)
                ).label('volume_today'),
                
                # System health indicators
                func.avg(
                    case((Transaction.created_at >= last_hour, Transaction.processing_time_ms), else_=None)
                ).label('avg_processing_time'),
                
                func.count(
                    case((
                        and_(
                            Transaction.created_at >= last_hour,
                            Transaction.status == TransactionStatus.FAILED
                        ), 
                        Transaction.id
                    ), else_=None)
                ).label('failed_transactions_hour')
            )
            .outerjoin(UserSession)
            .outerjoin(Transaction)
        ).first()
        
        # Get latest system metrics
        latest_system_metrics = (
            self.db_session.query(SystemMetrics)
            .order_by(SystemMetrics.timestamp.desc())
            .first()
        )
        
        # Calculate error rate
        error_rate = 0
        if current_stats.transactions_last_hour > 0:
            error_rate = (
                current_stats.failed_transactions_hour / 
                current_stats.transactions_last_hour * 100
            )
        
        return {
            'timestamp': now,
            'real_time_metrics': {
                'active_sessions': current_stats.active_sessions,
                'transactions_last_hour': current_stats.transactions_last_hour,
                'transactions_today': current_stats.transactions_today,
                'volume_today': float(current_stats.volume_today or 0),
                'avg_processing_time_ms': float(current_stats.avg_processing_time or 0),
                'error_rate_percent': float(error_rate)
            },
            'system_health': {
                'database_connections': latest_system_metrics.active_connections if latest_system_metrics else 0,
                'cpu_usage': float(latest_system_metrics.cpu_usage_percent or 0) if latest_system_metrics else 0,
                'memory_usage': float(latest_system_metrics.memory_usage_percent or 0) if latest_system_metrics else 0,
                'last_updated': latest_system_metrics.timestamp if latest_system_metrics else None
            }
        }

    def execute_custom_metric_query(
        self,
        metric_definition: MetricDefinition,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Execute custom metric calculation query.
        Demonstrates dynamic query execution with SQLAlchemy.
        """
        
        if not metric_definition.calculation_query:
            raise ValueError(f"Metric {metric_definition.name} has no calculation query defined")
        
        # Build parameterized query
        query_text = metric_definition.calculation_query
        
        # Apply filters if provided
        params = {}
        if filters:
            for key, value in filters.items():
                if isinstance(value, datetime):
                    params[key] = value
                elif isinstance(value, list):
                    # Handle list parameters for IN clauses
                    params[key] = tuple(value)
                else:
                    params[key] = value
        
        # Execute raw SQL with parameters
        result = self.db_session.execute(text(query_text), params)
        
        # Convert to list of dictionaries
        columns = result.keys()
        rows = result.fetchall()
        
        return [dict(zip(columns, row)) for row in rows]


class MaterializedViewManager:
    """
    Manages materialized views for improved analytics performance.
    Demonstrates materialized view refresh patterns.
    """
    
    def __init__(self, db_session: Session):
        self.db_session = db_session

    def refresh_daily_transaction_summary(self, target_date: Optional[date] = None):
        """
        Refresh daily transaction summary materialized view.
        Can refresh specific date or all recent data.
        """
        
        if target_date is None:
            target_date = datetime.utcnow().date()
        
        # Delete existing data for target date
        self.db_session.query(TransactionSummaryDaily).filter_by(
            date=target_date
        ).delete()
        
        # Calculate fresh summary data
        summary_data = (
            self.db_session.query(
                func.date(Transaction.created_at).label('transaction_date'),
                func.count(Transaction.id).label('total_transactions'),
                
                # Count by type
                func.sum(
                    case((Transaction.transaction_type == TransactionType.DEPOSIT, 1), else_=0)
                ).label('deposit_count'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.WITHDRAWAL, 1), else_=0)
                ).label('withdrawal_count'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.TRANSFER, 1), else_=0)
                ).label('transfer_count'),
                
                # Amount by type
                func.sum(
                    case((Transaction.transaction_type == TransactionType.DEPOSIT, Transaction.amount), else_=0)
                ).label('deposit_amount'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.WITHDRAWAL, Transaction.amount), else_=0)
                ).label('withdrawal_amount'),
                func.sum(
                    case((Transaction.transaction_type == TransactionType.TRANSFER, Transaction.amount), else_=0)
                ).label('transfer_amount'),
                func.sum(Transaction.amount).label('total_amount'),
                
                # Status breakdown
                func.sum(
                    case((Transaction.status == TransactionStatus.COMPLETED, 1), else_=0)
                ).label('completed_count'),
                func.sum(
                    case((Transaction.status == TransactionStatus.PENDING, 1), else_=0)
                ).label('pending_count'),
                func.sum(
                    case((Transaction.status == TransactionStatus.FAILED, 1), else_=0)
                ).label('failed_count'),
                
                # Performance metrics
                func.avg(Transaction.processing_time_ms).label('avg_processing_time_ms'),
                func.max(Transaction.processing_time_ms).label('max_processing_time_ms'),
                
                # Active accounts
                func.count(func.distinct(
                    func.coalesce(Transaction.from_account_id, Transaction.to_account_id)
                )).label('active_accounts')
            )
            .filter(func.date(Transaction.created_at) == target_date)
            .group_by(func.date(Transaction.created_at))
        ).first()
        
        if summary_data and summary_data.transaction_date:
            # Create new summary record
            summary = TransactionSummaryDaily(
                date=summary_data.transaction_date,
                total_transactions=summary_data.total_transactions,
                deposit_count=summary_data.deposit_count,
                withdrawal_count=summary_data.withdrawal_count,
                transfer_count=summary_data.transfer_count,
                total_amount=summary_data.total_amount,
                deposit_amount=summary_data.deposit_amount,
                withdrawal_amount=summary_data.withdrawal_amount,
                transfer_amount=summary_data.transfer_amount,
                completed_count=summary_data.completed_count,
                pending_count=summary_data.pending_count,
                failed_count=summary_data.failed_count,
                avg_processing_time_ms=summary_data.avg_processing_time_ms,
                max_processing_time_ms=summary_data.max_processing_time_ms,
                active_accounts=summary_data.active_accounts,
                updated_at=datetime.utcnow()
            )
            
            self.db_session.add(summary)
            self.db_session.commit()
            
            return summary
        
        return None

    def refresh_user_activity_summaries(self, target_date: Optional[date] = None):
        """
        Refresh user activity summaries for specified date.
        Calculates comprehensive daily user metrics.
        """
        
        if target_date is None:
            target_date = (datetime.utcnow() - timedelta(days=1)).date()
        
        # Delete existing summaries for target date
        self.db_session.query(UserActivitySummary).filter_by(
            date=target_date
        ).delete()
        
        # Calculate user activity summaries
        start_datetime = datetime.combine(target_date, datetime.min.time())
        end_datetime = start_datetime + timedelta(days=1)
        
        user_summaries = (
            self.db_session.query(
                UserSession.user_id,
                literal(target_date).label('summary_date'),
                
                # Session metrics
                func.count(func.distinct(UserSession.id)).label('session_count'),
                func.sum(UserSession.duration_minutes).label('total_duration'),
                func.avg(UserSession.duration_minutes).label('avg_duration'),
                
                # Page view metrics
                func.count(PageView.id).label('page_views'),
                func.count(func.distinct(PageView.page_type)).label('unique_pages'),
                func.sum(PageView.time_on_page_seconds).label('total_time_on_site'),
                
                # Transaction metrics
                func.sum(UserSession.transaction_count).label('transactions'),
                func.sum(UserSession.total_amount_transacted).label('total_amount'),
                
                # Engagement metrics
                func.sum(PageView.clicks_count).label('total_clicks'),
                func.sum(PageView.form_submissions).label('total_submissions'),
                
                # Device and location arrays
                func.array_agg(func.distinct(UserSession.device_type)).label('device_types'),
                func.array_agg(func.distinct(UserSession.country_code)).label('countries')
            )
            .outerjoin(PageView, UserSession.session_id == PageView.session_id)
            .filter(
                UserSession.started_at >= start_datetime,
                UserSession.started_at < end_datetime,
                UserSession.user_id.isnot(None)
            )
            .group_by(UserSession.user_id)
        )
        
        # Insert new summaries
        summaries_to_add = []
        for row in user_summaries:
            summary = UserActivitySummary(
                user_id=row.user_id,
                date=target_date,
                session_count=row.session_count,
                total_session_duration_minutes=int(row.total_duration or 0),
                avg_session_duration_minutes=row.avg_duration or Decimal('0'),
                page_views=row.page_views or 0,
                unique_pages_viewed=row.unique_pages or 0,
                total_time_on_site_minutes=int((row.total_time_on_site or 0) / 60),
                transaction_count=row.transactions or 0,
                total_transaction_amount=row.total_amount or Decimal('0'),
                clicks_count=row.total_clicks or 0,
                form_submissions=row.total_submissions or 0,
                device_types=row.device_types or [],
                countries=row.countries or [],
                calculated_at=datetime.utcnow()
            )
            summaries_to_add.append(summary)
        
        self.db_session.add_all(summaries_to_add)
        self.db_session.commit()
        
        return len(summaries_to_add)
```


### 6.3 Real-time Data Streaming and Event Processing

**app/services/real_time_analytics.py** - Real-time Analytics Service:
```python
"""
Real-time analytics service demonstrating event streaming patterns,
real-time aggregations, and live dashboard updates.
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Dict, Any, List, Optional, Callable, AsyncGenerator
from dataclasses import dataclass, asdict
from contextlib import asynccontextmanager

import redis
from sqlalchemy import text, func
from sqlalchemy.orm import Session
from fastapi import WebSocket, WebSocketDisconnect

from app.models.analytics import (
    Event, SystemMetrics, UserSession, PageView,
    EventCategory, MetricType
)
from app.models.transactions import Transaction, TransactionStatus
from app.core.database import get_db_session
from app.core.config import settings


@dataclass
class RealTimeMetric:
    """Structure for real-time metric data."""
    metric_name: str
    value: float
    timestamp: datetime
    dimensions: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class AlertThreshold:
    """Configuration for metric alerts."""
    metric_name: str
    threshold_value: float
    comparison_operator: str  # gt, lt, gte, lte, eq
    severity: str  # low, medium, high, critical
    cooldown_minutes: int = 5


class RealTimeAnalyticsService:
    """
    Real-time analytics service with WebSocket support and Redis streaming.
    Demonstrates real-time data processing patterns with SQLAlchemy.
    """
    
    def __init__(self, db_session: Session, redis_client: Optional[redis.Redis] = None):
        self.db_session = db_session
        self.redis_client = redis_client or redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            decode_responses=True
        )
        self.active_connections: List[WebSocket] = []
        self.metric_cache = {}
        self.alert_thresholds = self._load_alert_thresholds()

    async def start_real_time_processing(self):
        """Start real-time analytics processing loops."""
        
        # Start multiple concurrent processing tasks
        await asyncio.gather(
            self._process_transaction_stream(),
            self._calculate_real_time_metrics(),
            self._monitor_system_health(),
            self._process_user_activity_stream(),
            return_exceptions=True
        )

    async def _process_transaction_stream(self):
        """Process real-time transaction events."""
        
        while True:
            try:
                # Get recent transactions (last 10 seconds)
                cutoff_time = datetime.utcnow() - timedelta(seconds=10)
                
                recent_transactions = (
                    self.db_session.query(Transaction)
                    .filter(
                        Transaction.created_at >= cutoff_time,
                        Transaction.status.in_([
                            TransactionStatus.COMPLETED,
                            TransactionStatus.FAILED
                        ])
                    )
                    .order_by(Transaction.created_at.desc())
                ).all()
                
                if recent_transactions:
                    # Calculate real-time metrics
                    metrics = await self._calculate_transaction_metrics(recent_transactions)
                    
                    # Broadcast to connected clients
                    await self._broadcast_metrics("transaction_stream", metrics)
                    
                    # Store in Redis for fast access
                    await self._cache_metrics("recent_transactions", metrics)
                
                await asyncio.sleep(5)  # Process every 5 seconds
                
            except Exception as e:
                logger.error(f"Error in transaction stream processing: {e}")
                await asyncio.sleep(10)  # Back off on error

    async def _calculate_transaction_metrics(
        self, 
        transactions: List[Transaction]
    ) -> Dict[str, Any]:
        """Calculate metrics from recent transactions."""
        
        if not transactions:
            return {}
        
        total_count = len(transactions)
        total_amount = sum(tx.amount for tx in transactions)
        completed_count = len([tx for tx in transactions if tx.status == TransactionStatus.COMPLETED])
        failed_count = len([tx for tx in transactions if tx.status == TransactionStatus.FAILED])
        
        # Calculate processing time statistics
        processing_times = [tx.processing_time_ms for tx in transactions if tx.processing_time_ms]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        # Transaction type breakdown
        type_counts = {}
        for tx in transactions:
            tx_type = tx.transaction_type.value
            type_counts[tx_type] = type_counts.get(tx_type, 0) + 1
        
        return {
            'timestamp': datetime.utcnow(),
            'window_size_seconds': 10,
            'total_transactions': total_count,
            'total_volume': float(total_amount),
            'avg_amount': float(total_amount / total_count) if total_count > 0 else 0,
            'success_rate': float(completed_count / total_count * 100) if total_count > 0 else 0,
            'failure_rate': float(failed_count / total_count * 100) if total_count > 0 else 0,
            'avg_processing_time_ms': avg_processing_time,
            'transactions_per_second': total_count / 10,
            'type_breakdown': type_counts
        }

    async def _calculate_real_time_metrics(self):
        """Calculate and update real-time system metrics."""
        
        while True:
            try:
                now = datetime.utcnow()
                
                # Query current system state
                system_stats = await self._get_current_system_stats()
                user_stats = await self._get_current_user_stats()
                transaction_stats = await self._get_current_transaction_stats()
                
                # Combine all metrics
                metrics = {
                    'timestamp': now,
                    'system': system_stats,
                    'users': user_stats,
                    'transactions': transaction_stats
                }
                
                # Check for alerts
                alerts = await self._check_alert_thresholds(metrics)
                if alerts:
                    await self._broadcast_alerts(alerts)
                
                # Broadcast metrics to connected clients
                await self._broadcast_metrics("real_time_metrics", metrics)
                
                # Cache for dashboard widgets
                await self._cache_metrics("current_system_state", metrics)
                
                await asyncio.sleep(30)  # Update every 30 seconds
                
            except Exception as e:
                logger.error(f"Error in real-time metrics calculation: {e}")
                await asyncio.sleep(60)

    async def _get_current_system_stats(self) -> Dict[str, Any]:
        """Get current system performance statistics."""
        
        # Database connection stats
        db_stats = self.db_session.execute(text("""
            SELECT 
                count(*) as total_connections,
                count(*) FILTER (WHERE state = 'active') as active_connections,
                count(*) FILTER (WHERE state = 'idle') as idle_connections
            FROM pg_stat_activity 
            WHERE datname = current_database()
        """)).first()
        
        # Database size
        db_size = self.db_session.execute(text("""
            SELECT pg_size_pretty(pg_database_size(current_database())) as size,
                   pg_database_size(current_database()) / 1024 / 1024 as size_mb
        """)).first()
        
        # Recent slow queries
        slow_queries = self.db_session.execute(text("""
            SELECT count(*) as slow_query_count
            FROM pg_stat_statements 
            WHERE mean_exec_time > 1000  -- Queries taking more than 1 second
        """)).first()
        
        return {
            'database': {
                'total_connections': db_stats.total_connections if db_stats else 0,
                'active_connections': db_stats.active_connections if db_stats else 0,
                'idle_connections': db_stats.idle_connections if db_stats else 0,
                'size_mb': float(db_size.size_mb if db_size else 0),
                'slow_queries': slow_queries.slow_query_count if slow_queries else 0
            }
        }

    async def _get_current_user_stats(self) -> Dict[str, Any]:
        """Get current user activity statistics."""
        
        now = datetime.utcnow()
        last_hour = now - timedelta(hours=1)
        
        stats = (
            self.db_session.query(
                # Active sessions
                func.count(
                    case((UserSession.is_active == True, UserSession.id), else_=None)
                ).label('active_sessions'),
                
                # New sessions in last hour
                func.count(
                    case((UserSession.started_at >= last_hour, UserSession.id), else_=None)
                ).label('new_sessions_hour'),
                
                # Page views in last hour
                func.count(
                    case((PageView.viewed_at >= last_hour, PageView.id), else_=None)
                ).label('page_views_hour'),
                
                # Unique users active in last hour
                func.count(func.distinct(
                    case((UserSession.last_activity_at >= last_hour, UserSession.user_id), else_=None)
                )).label('active_users_hour')
            )
            .outerjoin(PageView, UserSession.session_id == PageView.session_id)
        ).first()
        
        return {
            'active_sessions': stats.active_sessions or 0,
            'new_sessions_hour': stats.new_sessions_hour or 0,
            'page_views_hour': stats.page_views_hour or 0,
            'active_users_hour': stats.active_users_hour or 0,
            'avg_pages_per_session': (
                float(stats.page_views_hour / stats.active_sessions)
                if stats.active_sessions and stats.active_sessions > 0 else 0
            )
        }

    async def _get_current_transaction_stats(self) -> Dict[str, Any]:
        """Get current transaction system statistics."""
        
        now = datetime.utcnow()
        last_hour = now - timedelta(hours=1)
        last_minute = now - timedelta(minutes=1)
        
        stats = (
            self.db_session.query(
                # Counts by status
                func.count(
                    case((Transaction.status == TransactionStatus.PENDING, Transaction.id), else_=None)
                ).label('pending_count'),
                func.count(
                    case((Transaction.status == TransactionStatus.PROCESSING, Transaction.id), else_=None)
                ).label('processing_count'),
                
                # Recent activity
                func.count(
                    case((Transaction.created_at >= last_hour, Transaction.id), else_=None)
                ).label('transactions_hour'),
                func.count(
                    case((Transaction.created_at >= last_minute, Transaction.id), else_=None)
                ).label('transactions_minute'),
                
                # Volume metrics
                func.sum(
                    case((Transaction.created_at >= last_hour, Transaction.amount), else_=0)
                ).label('volume_hour'),
                
                # Performance metrics
                func.avg(
                    case((Transaction.created_at >= last_hour, Transaction.processing_time_ms), else_=None)
                ).label('avg_processing_time_hour'),
                
                # Error rates
                func.count(
                    case((
                        and_(
                            Transaction.created_at >= last_hour,
                            Transaction.status == TransactionStatus.FAILED
                        ), 
                        Transaction.id
                    ), else_=None)
                ).label('failed_transactions_hour')
            )
        ).first()
        
        # Calculate derived metrics
        error_rate = 0
        tps = 0  # Transactions per second
        
        if stats.transactions_hour and stats.transactions_hour > 0:
            error_rate = stats.failed_transactions_hour / stats.transactions_hour * 100
        
        if stats.transactions_minute:
            tps = stats.transactions_minute / 60
        
        return {
            'pending_transactions': stats.pending_count or 0,
            'processing_transactions': stats.processing_count or 0,
            'transactions_per_hour': stats.transactions_hour or 0,
            'transactions_per_second': round(tps, 2),
            'volume_per_hour': float(stats.volume_hour or 0),
            'avg_processing_time_ms': float(stats.avg_processing_time_hour or 0),
            'error_rate_percent': round(error_rate, 2),
            'system_load': self._calculate_system_load(stats)
        }

    def _calculate_system_load(self, stats) -> str:
        """Calculate overall system load indicator."""
        
        load_score = 0
        
        # Transaction load factors
        if stats.transactions_minute:
            if stats.transactions_minute > 100:
                load_score += 3
            elif stats.transactions_minute > 50:
                load_score += 2
            elif stats.transactions_minute > 20:
                load_score += 1
        
        # Processing time factor
        if stats.avg_processing_time_hour:
            if stats.avg_processing_time_hour > 1000:  # > 1 second
                load_score += 3
            elif stats.avg_processing_time_hour > 500:
                load_score += 2
            elif stats.avg_processing_time_hour > 200:
                load_score += 1
        
        # Pending transactions factor
        if stats.pending_count:
            if stats.pending_count > 100:
                load_score += 2
            elif stats.pending_count > 50:
                load_score += 1
        
        # Map score to load level
        if load_score >= 7:
            return "high"
        elif load_score >= 4:
            return "medium"
        elif load_score >= 2:
            return "low"
        else:
            return "minimal"

    async def _check_alert_thresholds(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check metrics against configured alert thresholds."""
        
        alerts = []
        
        for threshold in self.alert_thresholds:
            try:
                # Extract metric value using dot notation
                value = self._get_nested_value(metrics, threshold.metric_name)
                
                if value is None:
                    continue
                
                # Check threshold condition
                should_alert = False
                if threshold.comparison_operator == 'gt':
                    should_alert = value > threshold.threshold_value
                elif threshold.comparison_operator == 'gte':
                    should_alert = value >= threshold.threshold_value
                elif threshold.comparison_operator == 'lt':
                    should_alert = value < threshold.threshold_value
                elif threshold.comparison_operator == 'lte':
                    should_alert = value <= threshold.threshold_value
                elif threshold.comparison_operator == 'eq':
                    should_alert = value == threshold.threshold_value
                
                if should_alert:
                    # Check cooldown period
                    cooldown_key = f"alert_cooldown:{threshold.metric_name}"
                    if not self.redis_client.exists(cooldown_key):
                        
                        alerts.append({
                            'metric_name': threshold.metric_name,
                            'current_value': value,
                            'threshold_value': threshold.threshold_value,
                            'severity': threshold.severity,
                            'timestamp': datetime.utcnow(),
                            'message': f"{threshold.metric_name} is {value} "
                                     f"({threshold.comparison_operator} {threshold.threshold_value})"
                        })
                        
                        # Set cooldown
                        self.redis_client.setex(
                            cooldown_key,
                            threshold.cooldown_minutes * 60,
                            "1"
                        )
                
            except Exception as e:
                logger.error(f"Error checking threshold for {threshold.metric_name}: {e}")
        
        return alerts

    def _get_nested_value(self, data: Dict[str, Any], path: str) -> Optional[float]:
        """Get nested dictionary value using dot notation (e.g., 'system.cpu_usage')."""
        
        keys = path.split('.')
        current = data
        
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return None
        
        try:
            return float(current)
        except (ValueError, TypeError):
            return None

    async def _process_user_activity_stream(self):
        """Process real-time user activity events."""
        
        while True:
            try:
                # Get recent page views and session updates
                cutoff_time = datetime.utcnow() - timedelta(seconds=30)
                
                # Recent page views
                recent_page_views = (
                    self.db_session.query(PageView)
                    .filter(PageView.viewed_at >= cutoff_time)
                    .order_by(PageView.viewed_at.desc())
                ).all()
                
                if recent_page_views:
                    # Calculate real-time user engagement
                    engagement_metrics = {
                        'timestamp': datetime.utcnow(),
                        'recent_page_views': len(recent_page_views),
                        'unique_users': len(set(pv.user_id for pv in recent_page_views if pv.user_id)),
                        'page_types': {},
                        'avg_load_time': 0
                    }
                    
                    # Analyze page types and performance
                    page_type_counts = {}
                    load_times = []
                    
                    for pv in recent_page_views:
                        if pv.page_type:
                            page_type_counts[pv.page_type] = page_type_counts.get(pv.page_type, 0) + 1
                        
                        if pv.load_time_ms:
                            load_times.append(pv.load_time_ms)
                    
                    engagement_metrics['page_types'] = page_type_counts
                    if load_times:
                        engagement_metrics['avg_load_time'] = sum(load_times) / len(load_times)
                    
                    # Broadcast user activity metrics
                    await self._broadcast_metrics("user_activity", engagement_metrics)
                
                await asyncio.sleep(15)  # Process every 15 seconds
                
            except Exception as e:
                logger.error(f"Error in user activity stream processing: {e}")
                await asyncio.sleep(30)

    async def _monitor_system_health(self):
        """Monitor system health metrics and create alerts."""
        
        while True:
            try:
                # Collect system metrics
                system_metrics = SystemMetrics()
                
                # Database metrics
                db_stats = self.db_session.execute(text("""
                    SELECT 
                        count(*) as connections,
                        pg_database_size(current_database()) / 1024 / 1024 as size_mb
                    FROM pg_stat_activity 
                    WHERE datname = current_database()
                """)).first()
                
                system_metrics.timestamp = datetime.utcnow()
                system_metrics.active_connections = db_stats.connections if db_stats else 0
                system_metrics.database_size_mb = db_stats.size_mb if db_stats else 0
                
                # Transaction system health
                pending_count = self.db_session.query(Transaction).filter_by(
                    status=TransactionStatus.PENDING
                ).count()
                
                failed_count_hour = self.db_session.query(Transaction).filter(
                    Transaction.status == TransactionStatus.FAILED,
                    Transaction.created_at >= datetime.utcnow() - timedelta(hours=1)
                ).count()
                
                system_metrics.pending_transaction_count = pending_count
                system_metrics.failed_transaction_count = failed_count_hour
                
                # Store system metrics
                self.db_session.add(system_metrics)
                self.db_session.commit()
                
                # Create health summary
                health_summary = {
                    'timestamp': system_metrics.timestamp,
                    'database_health': {
                        'connections': system_metrics.active_connections,
                        'size_mb': float(system_metrics.database_size_mb),
                        'status': 'healthy' if system_metrics.active_connections < 100 else 'warning'
                    },
                    'transaction_health': {
                        'pending_count': pending_count,
                        'failed_count_hour': failed_count_hour,
                        'status': 'healthy' if pending_count < 50 else 'warning'
                    }
                }
                
                await self._broadcast_metrics("system_health", health_summary)
                
                await asyncio.sleep(60)  # Monitor every minute
                
            except Exception as e:
                logger.error(f"Error in system health monitoring: {e}")
                await asyncio.sleep(120)

    async def _broadcast_metrics(self, metric_type: str, data: Dict[str, Any]):
        """Broadcast metrics to all connected WebSocket clients."""
        
        if not self.active_connections:
            return
        
        message = {
            'type': metric_type,
            'data': data,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        # Convert datetime objects to ISO strings for JSON serialization
        json_message = json.dumps(message, default=str)
        
        # Send to all active connections
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_text(json_message)
            except WebSocketDisconnect:
                disconnected.append(connection)
            except Exception as e:
                logger.error(f"Error sending WebSocket message: {e}")
                disconnected.append(connection)
        
        # Remove disconnected clients
        for conn in disconnected:
            self.active_connections.remove(conn)

    async def _broadcast_alerts(self, alerts: List[Dict[str, Any]]):
        """Broadcast alerts to connected clients."""
        
        for alert in alerts:
            await self._broadcast_metrics("alert", alert)
            
            # Also store alert in Redis for persistence
            alert_key = f"alert:{alert['metric_name']}:{int(time.time())}"
            self.redis_client.setex(
                alert_key,
                3600,  # Store for 1 hour
                json.dumps(alert, default=str)
            )

    async def _cache_metrics(self, cache_key: str, data: Dict[str, Any]):
        """Cache metrics in Redis for fast access."""
        
        try:
            serialized_data = json.dumps(data, default=str)
            self.redis_client.setex(
                f"metrics:{cache_key}",
                300,  # Cache for 5 minutes
                serialized_data
            )
        except Exception as e:
            logger.error(f"Error caching metrics {cache_key}: {e}")

    def _load_alert_thresholds(self) -> List[AlertThreshold]:
        """Load alert thresholds from configuration."""
        
        return [
            AlertThreshold(
                metric_name="transactions.error_rate_percent",
                threshold_value=5.0,
                comparison_operator="gte",
                severity="high",
                cooldown_minutes=5
            ),
            AlertThreshold(
                metric_name="transactions.avg_processing_time_ms",
                threshold_value=2000.0,
                comparison_operator="gte",
                severity="medium",
                cooldown_minutes=10
            ),
            AlertThreshold(
                metric_name="transactions.pending_transactions",
                threshold_value=100,
                comparison_operator="gte",
                severity="medium",
                cooldown_minutes=15
            ),
            AlertThreshold(
                metric_name="system.database.connections",
                threshold_value=200,
                comparison_operator="gte",
                severity="critical",
                cooldown_minutes=5
            ),
            AlertThreshold(
                metric_name="users.active_sessions",
                threshold_value=1000,
                comparison_operator="gte",
                severity="low",
                cooldown_minutes=30
            )
        ]

    # WebSocket connection management
    async def connect_websocket(self, websocket: WebSocket):
        """Add new WebSocket connection for real-time updates."""
        await websocket.accept()
        self.active_connections.append(websocket)
        
        # Send initial dashboard data
        initial_data = await self._get_cached_metrics("current_system_state")
        if initial_data:
            await websocket.send_text(json.dumps({
                'type': 'initial_data',
                'data': initial_data
            }, default=str))

    def disconnect_websocket(self, websocket: WebSocket):
        """Remove WebSocket connection."""
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)

    async def _get_cached_metrics(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Retrieve cached metrics from Redis."""
        
        try:
            cached_data = self.redis_client.get(f"metrics:{cache_key}")
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            logger.error(f"Error retrieving cached metrics {cache_key}: {e}")
        
        return None


class EventStreamProcessor:
    """
    Process events in real-time for immediate analytics updates.
    Demonstrates event-driven architecture with SQLAlchemy.
    """
    
    def __init__(self, db_session: Session, redis_client: redis.Redis):
        self.db_session = db_session
        self.redis_client = redis_client

    async def process_event_stream(self, stream_name: str = "analytics_events"):
        """Process events from Redis stream."""
        
        consumer_group = "analytics_processors"
        consumer_name = f"processor_{id(self)}"
        
        # Create consumer group if it doesn't exist
        try:
            self.redis_client.xgroup_create(stream_name, consumer_group, id='0', mkstream=True)
        except redis.exceptions.ResponseError:
            pass  # Group already exists
        
        while True:
            try:
                # Read from stream
                messages = self.redis_client.xreadgroup(
                    consumer_group,
                    consumer_name,
                    {stream_name: '>'},
                    count=10,
                    block=1000  # Block for 1 second
                )
                
                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        await self._process_single_event(msg_id, fields)
                        
                        # Acknowledge message processing
                        self.redis_client.xack(stream_name, consumer_group, msg_id)
                
            except Exception as e:
                logger.error(f"Error processing event stream: {e}")
                await asyncio.sleep(5)

    async def _process_single_event(self, msg_id: str, fields: Dict[str, str]):
        """Process a single event from the stream."""
        
        try:
            event_type = fields.get('event_type')
            event_data = json.loads(fields.get('data', '{}'))
            
            if event_type == 'transaction_completed':
                await self._handle_transaction_event(event_data)
            elif event_type == 'user_action':
                await self._handle_user_action_event(event_data)
            elif event_type == 'system_metric':
                await self._handle_system_metric_event(event_data)
            
        except Exception as e:
            logger.error(f"Error processing event {msg_id}: {e}")

    async def _handle_transaction_event(self, event_data: Dict[str, Any]):
        """Handle real-time transaction completion events."""
        
        transaction_id = event_data.get('transaction_id')
        
        if transaction_id:
            # Update real-time transaction counters
            await self._increment_counter(
                f"transactions:completed:{datetime.utcnow().strftime('%Y%m%d%H%M')}"
            )
            
            # Update transaction volume tracking
            amount = Decimal(str(event_data.get('amount', '0')))
            await self._increment_counter(
                f"volume:completed:{datetime.utcnow().strftime('%Y%m%d%H%M')}",
                float(amount)
            )

    async def _handle_user_action_event(self, event_data: Dict[str, Any]):
        """Handle real-time user action events."""
        
        action_type = event_data.get('action_type')
        user_id = event_data.get('user_id')
        
        if action_type and user_id:
            # Track user engagement
            await self._increment_counter(
                f"user_actions:{action_type}:{datetime.utcnow().strftime('%Y%m%d%H')}"
            )
            
            # Update user activity tracking
            await self._track_user_activity(user_id, action_type, event_data)

    async def _increment_counter(self, counter_key: str, increment: float = 1.0):
        """Increment Redis counter with expiration."""
        
        pipe = self.redis_client.pipeline()
        pipe.incrbyfloat(counter_key, increment)
        pipe.expire(counter_key, 86400)  # Expire after 24 hours
        pipe.execute()

    async def _track_user_activity(self, user_id: int, action_type: str, event_data: Dict[str, Any]):
        """Track individual user activity for real-time personalization."""
        
        activity_key = f"user_activity:{user_id}:{datetime.utcnow().strftime('%Y%m%d')}"
        
        activity_data = {
            'last_action': action_type,
            'last_action_time': datetime.utcnow().isoformat(),
            'page_type': event_data.get('page_type'),
            'session_id': event_data.get('session_id')
        }
        
        self.redis_client.hset(activity_key, mapping=activity_data)
        self.redis_client.expire(activity_key, 86400)  # Expire after 24 hours


# Factory function for service initialization
def create_real_time_analytics_service(db_session: Session) -> RealTimeAnalyticsService:
    """Create and configure real-time analytics service."""
    
    redis_client = redis.Redis(
        host=settings.REDIS_HOST,
        port=settings.REDIS_PORT,
        decode_responses=True
    )
    
    return RealTimeAnalyticsService(db_session, redis_client)


# Utility functions for event publishing
async def publish_event(
    event_type: str,
    event_data: Dict[str, Any],
    stream_name: str = "analytics_events"
):
    """Publish event to Redis stream for real-time processing."""
    
    redis_client = redis.Redis(
        host=settings.REDIS_HOST,
        port=settings.REDIS_PORT,
        decode_responses=True
    )
    
    message = {
        'event_type': event_type,
        'data': json.dumps(event_data, default=str),
        'timestamp': datetime.utcnow().isoformat()
    }
    
    try:
        redis_client.xadd(stream_name, message)
    except Exception as e:
        logger.error(f"Error publishing event: {e}")


# Event decorators for automatic event publishing
def track_transaction_event(event_type: str):
    """Decorator to automatically track transaction events."""
    
    def decorator(func):
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                
                # Publish success event
                await publish_event('transaction_completed', {
                    'function_name': func.__name__,
                    'transaction_id': getattr(result, 'id', None),
                    'amount': getattr(result, 'amount', None),
                    'processing_time_ms': (time.time() - start_time) * 1000,
                    'status': 'success'
                })
                
                return result
                
            except Exception as e:
                # Publish failure event
                await publish_event('transaction_failed', {
                    'function_name': func.__name__,
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'processing_time_ms': (time.time() - start_time) * 1000,
                    'status': 'failure'
                })
                raise
        
        return wrapper
    return decorator
```


### 6.4 Dashboard API Endpoints and Widget Management

**app/schemas/dashboard.py** - Dashboard and Widget Schemas:
```python
"""
Dashboard and widget schemas for real-time analytics dashboard.
Demonstrates complex validation and dynamic configuration patterns.
"""

from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union
from enum import Enum
from uuid import UUID

from pydantic import BaseModel, Field, validator, root_validator

from app.schemas.common import BaseSchema, TimestampMixin, UUIDMixin


class WidgetType(str, Enum):
    """Widget type enumeration."""
    METRIC_CARD = "metric_card"
    CHART_LINE = "chart_line"
    CHART_BAR = "chart_bar"
    CHART_PIE = "chart_pie"
    TABLE = "table"
    HEATMAP = "heatmap"
    GAUGE = "gauge"
    PROGRESS_BAR = "progress_bar"


class ChartTimeRange(str, Enum):
    """Chart time range options."""
    LAST_HOUR = "1h"
    LAST_DAY = "24h"
    LAST_WEEK = "7d"
    LAST_MONTH = "30d"
    LAST_QUARTER = "90d"
    CUSTOM = "custom"


class RefreshInterval(int, Enum):
    """Widget refresh interval in seconds."""
    REAL_TIME = 5
    FAST = 30
    NORMAL = 60
    SLOW = 300
    MANUAL = 0


# Widget Configuration Schemas
class BaseWidgetConfig(BaseSchema):
    """Base widget configuration."""
    title: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    refresh_interval: RefreshInterval = RefreshInterval.NORMAL
    height: int = Field(default=300, ge=100, le=1000)
    width: int = Field(default=400, ge=200, le=2000)


class MetricCardConfig(BaseWidgetConfig):
    """Configuration for metric card widgets."""
    metric_query: str = Field(..., description="SQL query or metric identifier")
    format_type: str = Field(default="number", regex="^(number|currency|percentage|duration)$")
    precision: int = Field(default=2, ge=0, le=6)
    show_trend: bool = Field(default=True)
    trend_period: ChartTimeRange = ChartTimeRange.LAST_DAY
    alert_thresholds: Optional[Dict[str, float]] = Field(default=None)


class ChartConfig(BaseWidgetConfig):
    """Configuration for chart widgets."""
    chart_type: WidgetType = Field(..., description="Type of chart")
    time_range: ChartTimeRange = ChartTimeRange.LAST_DAY
    custom_start_date: Optional[datetime] = None
    custom_end_date: Optional[datetime] = None
    group_by: Optional[str] = Field(None, description="Field to group data by")
    aggregation: str = Field(default="sum", regex="^(sum|avg|count|min|max)$")
    show_legend: bool = Field(default=True)
    color_scheme: str = Field(default="default")

    @validator('custom_end_date')
    def validate_custom_dates(cls, v, values):
        if values.get('time_range') == ChartTimeRange.CUSTOM:
            if not v or not values.get('custom_start_date'):
                raise ValueError('Custom start and end dates required for custom time range')
            if v <= values.get('custom_start_date'):
                raise ValueError('End date must be after start date')
        return v


class TableConfig(BaseWidgetConfig):
    """Configuration for table widgets."""
    columns: List[Dict[str, str]] = Field(..., min_items=1)
    sortable: bool = Field(default=True)
    filterable: bool = Field(default=True)
    pagination: bool = Field(default=True)
    page_size: int = Field(default=20, ge=5, le=100)
    export_enabled: bool = Field(default=True)

    @validator('columns')
    def validate_columns(cls, v):
        required_keys = {'field', 'title'}
        for col in v:
            if not all(key in col for key in required_keys):
                raise ValueError('Each column must have field and title')
        return v


class GaugeConfig(BaseWidgetConfig):
    """Configuration for gauge widgets."""
    min_value: float = Field(default=0)
    max_value: float = Field(default=100)
    target_value: Optional[float] = None
    warning_threshold: Optional[float] = None
    critical_threshold: Optional[float] = None
    unit: str = Field(default="", max_length=10)


# Dashboard Schemas
class DashboardBase(BaseSchema):
    """Base dashboard schema."""
    name: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=1000)
    is_public: bool = Field(default=False)
    is_default: bool = Field(default=False)
    layout_config: Optional[Dict[str, Any]] = Field(default=None)


class DashboardCreate(DashboardBase):
    """Schema for dashboard creation."""
    pass


class DashboardUpdate(BaseSchema):
    """Schema for dashboard updates."""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=1000)
    is_public: Optional[bool] = None
    layout_config: Optional[Dict[str, Any]] = None


class DashboardResponse(DashboardBase, TimestampMixin, UUIDMixin):
    """Dashboard response schema."""
    owner_id: int
    widget_count: int = Field(default=0)


# Widget Schemas
class WidgetBase(BaseSchema):
    """Base widget schema."""
    name: str = Field(..., min_length=1, max_length=100)
    widget_type: WidgetType
    position_x: int = Field(default=0, ge=0)
    position_y: int = Field(default=0, ge=0)
    is_visible: bool = Field(default=True)


class WidgetCreate(WidgetBase):
    """Schema for widget creation."""
    dashboard_id: int = Field(..., gt=0)
    configuration: Union[
        MetricCardConfig,
        ChartConfig,
        TableConfig,
        GaugeConfig
    ] = Field(..., description="Widget-specific configuration")

    @root_validator
    def validate_config_type(cls, values):
        widget_type = values.get('widget_type')
        config = values.get('configuration')
        
        if widget_type == WidgetType.METRIC_CARD and not isinstance(config, MetricCardConfig):
            raise ValueError('MetricCardConfig required for metric card widgets')
        elif widget_type in [WidgetType.CHART_LINE, WidgetType.CHART_BAR, WidgetType.CHART_PIE] and not isinstance(config, ChartConfig):
            raise ValueError('ChartConfig required for chart widgets')
        elif widget_type == WidgetType.TABLE and not isinstance(config, TableConfig):
            raise ValueError('TableConfig required for table widgets')
        elif widget_type == WidgetType.GAUGE and not isinstance(config, GaugeConfig):
            raise ValueError('GaugeConfig required for gauge widgets')
        
        return values


class WidgetUpdate(BaseSchema):
    """Schema for widget updates."""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    position_x: Optional[int] = Field(None, ge=0)
    position_y: Optional[int] = Field(None, ge=0)
    is_visible: Optional[bool] = None
    configuration: Optional[Dict[str, Any]] = None


class WidgetResponse(WidgetBase, TimestampMixin, UUIDMixin):
    """Widget response schema."""
    dashboard_id: int
    configuration: Dict[str, Any]


class WidgetDataResponse(BaseSchema):
    """Widget data response schema."""
    widget_id: int
    widget_type: WidgetType
    data: Union[Dict[str, Any], List[Dict[str, Any]]]
    metadata: Optional[Dict[str, Any]] = None
    last_updated: datetime
    refresh_interval: int


# Dashboard Layout Schemas
class LayoutPosition(BaseSchema):
    """Widget position in dashboard layout."""
    widget_id: int
    x: int = Field(ge=0)
    y: int = Field(ge=0)
    width: int = Field(ge=1)
    height: int = Field(ge=1)


class DashboardLayout(BaseSchema):
    """Complete dashboard layout configuration."""
    grid_columns: int = Field(default=12, ge=6, le=24)
    grid_rows: int = Field(default=20, ge=10, le=50)
    widget_positions: List[LayoutPosition]


class DashboardLayoutUpdate(BaseSchema):
    """Schema for updating dashboard layout."""
    layout_config: DashboardLayout


# Real-time Data Schemas
class RealTimeUpdate(BaseSchema):
    """Real-time dashboard update."""
    dashboard_id: int
    widget_updates: List[WidgetDataResponse]
    timestamp: datetime


class MetricAlert(BaseSchema):
    """Metric alert schema."""
    widget_id: int
    metric_name: str
    current_value: float
    threshold_value: float
    severity: str = Field(regex="^(low|medium|high|critical)$")
    message: str
    triggered_at: datetime


# Filter and Query Schemas
class DashboardFilter(BaseSchema):
    """Dashboard-level filters."""
    time_range: ChartTimeRange = ChartTimeRange.LAST_DAY
    custom_start_date: Optional[datetime] = None
    custom_end_date: Optional[datetime] = None
    user_segment: Optional[str] = None
    transaction_type: Optional[str] = None
    custom_filters: Optional[Dict[str, Any]] = None


class WidgetQuery(BaseSchema):
    """Widget data query parameters."""
    widget_id: int
    force_refresh: bool = Field(default=False)
    filters: Optional[DashboardFilter] = None


# Analytics Export Schemas
class ExportRequest(BaseSchema):
    """Data export request schema."""
    dashboard_id: Optional[int] = None
    widget_ids: Optional[List[int]] = None
    format_type: str = Field(default="csv", regex="^(csv|json|excel)$")
    time_range: ChartTimeRange = ChartTimeRange.LAST_DAY
    custom_start_date: Optional[datetime] = None
    custom_end_date: Optional[datetime] = None
    include_metadata: bool = Field(default=True)


class ExportResponse(BaseSchema):
    """Export response schema."""
    export_id: str
    download_url: str
    file_size_mb: float
    expires_at: datetime
    record_count: int


# Dashboard Sharing Schemas
class DashboardShare(BaseSchema):
    """Dashboard sharing configuration."""
    dashboard_id: int
    share_type: str = Field(regex="^(public|private|link)$")
    expires_at: Optional[datetime] = None
    allowed_users: Optional[List[int]] = None
    permissions: List[str] = Field(default=["view"])


class SharedDashboardResponse(BaseSchema):
    """Shared dashboard access response."""
    share_token: str
    dashboard: DashboardResponse
    permissions: List[str]
    expires_at: Optional[datetime]
```

**app/services/dashboard_service.py** - Dashboard Management Service:
```python
"""
Dashboard management service providing widget operations,
data fetching, and real-time updates for analytics dashboards.
"""

import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Union
from decimal import Decimal

from sqlalchemy import text, func, and_, or_, desc, asc
from sqlalchemy.orm import Session, joinedload
from sqlalchemy.exc import SQLAlchemyError

from app.models.analytics import DashboardWidget, Event, SystemMetrics
from app.models.transactions import Transaction, Account, TransactionStatus
from app.models.user import User, UserSession
from app.schemas.dashboard import (
    WidgetType, ChartTimeRange, DashboardFilter,
    WidgetDataResponse, MetricAlert
)
from app.services.analytics_query_service import AnalyticsQueryService
from app.core.exceptions import ValidationError, NotFoundError


class DashboardService:
    """
    Comprehensive dashboard management service with real-time data processing.
    Demonstrates advanced SQLAlchemy patterns for analytics dashboards.
    """
    
    def __init__(self, db_session: Session):
        self.db_session = db_session
        self.analytics_service = AnalyticsQueryService(db_session)

    async def get_dashboard_data(
        self,
        dashboard_id: int,
        filters: Optional[DashboardFilter] = None,
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get complete dashboard data with all widgets."""
        
        # Get dashboard and widgets
        dashboard = (
            self.db_session.query(DashboardWidget)
            .filter(DashboardWidget.dashboard_id == dashboard_id)
            .options(joinedload(DashboardWidget.dashboard))
            .all()
        )
        
        if not dashboard:
            raise NotFoundError(f"Dashboard {dashboard_id} not found")
        
        # Process each widget
        widget_data = []
        for widget in dashboard:
            try:
                data = await self._get_widget_data(widget, filters, user_id)
                widget_data.append(data)
            except Exception as e:
                # Log error but continue with other widgets
                logger.error(f"Error loading widget {widget.id}: {e}")
                widget_data.append({
                    'widget_id': widget.id,
                    'error': str(e),
                    'last_updated': datetime.utcnow()
                })
        
        return {
            'dashboard_id': dashboard_id,
            'widgets': widget_data,
            'filters_applied': filters.dict() if filters else None,
            'last_updated': datetime.utcnow(),
            'total_widgets': len(widget_data)
        }

    async def _get_widget_data(
        self,
        widget: DashboardWidget,
        filters: Optional[DashboardFilter] = None,
        user_id: Optional[int] = None
    ) -> WidgetDataResponse:
        """Get data for a specific widget based on its type and configuration."""
        
        widget_config = widget.configuration or {}
        widget_type = WidgetType(widget.widget_type)
        
        # Determine time range for queries
        time_range = self._resolve_time_range(filters, widget_config)
        
        try:
            if widget_type == WidgetType.METRIC_CARD:
                data = await self._get_metric_card_data(widget, time_range, user_id)
            elif widget_type in [WidgetType.CHART_LINE, WidgetType.CHART_BAR, WidgetType.CHART_PIE]:
                data = await self._get_chart_data(widget, time_range, user_id)
            elif widget_type == WidgetType.TABLE:
                data = await self._get_table_data(widget, time_range, user_id)
            elif widget_type == WidgetType.GAUGE:
                data = await self._get_gauge_data(widget, time_range, user_id)
            elif widget_type == WidgetType.HEATMAP:
                data = await self._get_heatmap_data(widget, time_range, user_id)
            else:
                raise ValueError(f"Unsupported widget type: {widget_type}")
            
            return WidgetDataResponse(
                widget_id=widget.id,
                widget_type=widget_type,
                data=data,
                metadata=self._get_widget_metadata(widget, time_range),
                last_updated=datetime.utcnow(),
                refresh_interval=widget_config.get('refresh_interval', 60)
            )
            
        except Exception as e:
            logger.error(f"Error getting data for widget {widget.id}: {e}")
            raise

    async def _get_metric_card_data(
        self,
        widget: DashboardWidget,
        time_range: Tuple[datetime, datetime],
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get data for metric card widgets."""
        
        config = widget.configuration
        metric_name = config.get('metric_name', 'total_transactions')
        start_time, end_time = time_range
        
        # Main metric value
        current_value = await self._calculate_metric_value(
            metric_name, start_time, end_time, user_id
        )
        
        # Trend calculation if enabled
        trend_data = None
        if config.get('show_trend', True):
            trend_period = config.get('trend_period', '24h')
            trend_data = await self._calculate_trend(
                metric_name, current_value, trend_period, user_id
            )
        
        # Format value based on configuration
        formatted_value = self._format_metric_value(
            current_value,
            config.get('format_type', 'number'),
            config.get('precision', 2)
        )
        
        # Check alert thresholds
        alerts = self._check_metric_alerts(
            widget.id, metric_name, current_value, config.get('alert_thresholds')
        )
        
        return {
            'current_value': current_value,
            'formatted_value': formatted_value,
            'trend': trend_data,
            'alerts': alerts,
            'time_range': {
                'start': start_time,
                'end': end_time
            }
        }

    async def _get_chart_data(
        self,
        widget: DashboardWidget,
        time_range: Tuple[datetime, datetime],
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get data for chart widgets (line, bar, pie)."""
        
        config = widget.configuration
        chart_type = config.get('chart_type', 'line')
        metric_name = config.get('metric_name', 'transaction_volume')
        group_by = config.get('group_by', 'hour')
        aggregation = config.get('aggregation', 'sum')
        
        start_time, end_time = time_range
        
        # Build time series query based on grouping
        if group_by == 'hour':
            time_bucket = "date_trunc('hour', created_at)"
            time_format = '%Y-%m-%d %H:00'
        elif group_by == 'day':
            time_bucket = "date_trunc('day', created_at)"
            time_format = '%Y-%m-%d'
        elif group_by == 'week':
            time_bucket = "date_trunc('week', created_at)"
            time_format = '%Y-W%U'
        else:
            time_bucket = "date_trunc('day', created_at)"
            time_format = '%Y-%m-%d'
        
        # Execute time series query
        if metric_name == 'transaction_volume':
            query_result = self.db_session.execute(text(f"""
                SELECT 
                    {time_bucket} as time_bucket,
                    {aggregation}(amount) as value,
                    count(*) as transaction_count
                FROM transactions 
                WHERE created_at >= :start_time 
                    AND created_at <= :end_time
                    AND status = 'completed'
                    {f'AND user_id = :user_id' if user_id else ''}
                GROUP BY time_bucket
                ORDER BY time_bucket
            """), {
                'start_time': start_time,
                'end_time': end_time,
                'user_id': user_id
            })
        elif metric_name == 'user_activity':
            query_result = self.db_session.execute(text(f"""
                SELECT 
                    {time_bucket} as time_bucket,
                    count(distinct user_id) as value,
                    count(*) as total_events
                FROM events 
                WHERE created_at >= :start_time 
                    AND created_at <= :end_time
                    AND category = 'user_activity'
                GROUP BY time_bucket
                ORDER BY time_bucket
            """), {
                'start_time': start_time,
                'end_time': end_time
            })
        else:
            # Generic metric query
            query_result = await self._execute_custom_metric_query(
                metric_name, time_bucket, aggregation, start_time, end_time, user_id
            )
        
        # Process results for chart format
        chart_data = []
        labels = []
        values = []
        
        for row in query_result:
            time_label = row.time_bucket.strftime(time_format)
            labels.append(time_label)
            values.append(float(row.value or 0))
            
            chart_data.append({
                'x': time_label,
                'y': float(row.value or 0),
                'timestamp': row.time_bucket,
                'metadata': getattr(row, 'transaction_count', None) or getattr(row, 'total_events', None)
            })
        
        return {
            'chart_type': chart_type,
            'data': chart_data,
            'labels': labels,
            'values': values,
            'summary': {
                'total_points': len(chart_data),
                'max_value': max(values) if values else 0,
                'min_value': min(values) if values else 0,
                'avg_value': sum(values) / len(values) if values else 0
            }
        }

    async def _get_table_data(
        self,
        widget: DashboardWidget,
        time_range: Tuple[datetime, datetime],
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get data for table widgets."""
        
        config = widget.configuration
        data_source = config.get('data_source', 'recent_transactions')
        columns = config.get('columns', [])
        page_size = config.get('page_size', 20)
        
        start_time, end_time = time_range
        
        if data_source == 'recent_transactions':
            # Recent transactions table
            query = (
                self.db_session.query(
                    Transaction.id,
                    Transaction.created_at,
                    Transaction.amount,
                    Transaction.transaction_type,
                    Transaction.status,
                    Transaction.description,
                    User.username.label('user_name')
                )
                .join(User, Transaction.user_id == User.id)
                .filter(
                    Transaction.created_at >= start_time,
                    Transaction.created_at <= end_time
                )
                .order_by(Transaction.created_at.desc())
                .limit(page_size)
            )
            
            if user_id:
                query = query.filter(Transaction.user_id == user_id)
            
            results = query.all()
            
            table_data = []
            for row in results:
                table_data.append({
                    'id': row.id,
                    'date': row.created_at.strftime('%Y-%m-%d %H:%M'),
                    'amount': f"${row.amount:,.2f}",
                    'type': row.transaction_type.value,
                    'status': row.status.value,
                    'description': row.description or '',
                    'user': row.user_name
                })
        
        elif data_source == 'top_users':
            # Top users by transaction volume
            query_result = self.db_session.execute(text("""
                SELECT 
                    u.id,
                    u.username,
                    u.email,
                    count(t.id) as transaction_count,
                    sum(t.amount) as total_volume,
                    avg(t.amount) as avg_transaction,
                    max(t.created_at) as last_transaction
                FROM users u
                JOIN transactions t ON u.id = t.user_id
                WHERE t.created_at >= :start_time 
                    AND t.created_at <= :end_time
                    AND t.status = 'completed'
                GROUP BY u.id, u.username, u.email
                ORDER BY total_volume DESC
                LIMIT :limit
            """), {
                'start_time': start_time,
                'end_time': end_time,
                'limit': page_size
            })
            
            table_data = []
            for row in query_result:
                table_data.append({
                    'user_id': row.id,
                    'username': row.username,
                    'email': row.email,
                    'transaction_count': row.transaction_count,
                    'total_volume': f"${row.total_volume:,.2f}",
                    'avg_transaction': f"${row.avg_transaction:,.2f}",
                    'last_transaction': row.last_transaction.strftime('%Y-%m-%d')
                })
        
        else:
            table_data = []
        
        return {
            'columns': columns,
            'data': table_data,
            'total_rows': len(table_data),
            'page_size': page_size
        }

    async def _get_gauge_data(
        self,
        widget: DashboardWidget,
        time_range: Tuple[datetime, datetime],
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get data for gauge widgets."""
        
        config = widget.configuration
        metric_name = config.get('metric_name', 'system_load')
        min_value = config.get('min_value', 0)
        max_value = config.get('max_value', 100)
        target_value = config.get('target_value')
        
        start_time, end_time = time_range
        
        # Calculate current metric value
        if metric_name == 'system_load':
            # Calculate system load based on pending transactions and processing times
            current_value = await self._calculate_system_load_percentage()
        elif metric_name == 'success_rate':
            # Calculate transaction success rate
            current_value = await self._calculate_success_rate(start_time, end_time, user_id)
        elif metric_name == 'capacity_utilization':
            # Calculate capacity utilization
            current_value = await self._calculate_capacity_utilization()
        else:
            current_value = await self._calculate_metric_value(
                metric_name, start_time, end_time, user_id
            )
        
        # Determine gauge color based on thresholds
        gauge_color = self._determine_gauge_color(
            current_value,
            config.get('warning_threshold'),
            config.get('critical_threshold')
        )
        
        return {
            'current_value': current_value,
            'min_value': min_value,
            'max_value': max_value,
            'target_value': target_value,
            'percentage': (current_value - min_value) / (max_value - min_value) * 100,
            'color': gauge_color,
            'status': self._get_gauge_status(current_value, config)
        }

    async def _get_heatmap_data(
        self,
        widget: DashboardWidget,
        time_range: Tuple[datetime, datetime],
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Get data for heatmap widgets."""
        
        config = widget.configuration
        metric_name = config.get('metric_name', 'transaction_volume')
        start_time, end_time = time_range
        
        # Generate hourly heatmap data (24 hours x 7 days)
        query_result = self.db_session.execute(text("""
            SELECT 
                extract(dow from created_at) as day_of_week,
                extract(hour from created_at) as hour_of_day,
                count(*) as transaction_count,
                sum(amount) as total_amount
            FROM transactions
            WHERE created_at >= :start_time 
                AND created_at <= :end_time
                AND status = 'completed'
                {f'AND user_id = :user_id' if user_id else ''}
            GROUP BY day_of_week, hour_of_day
            ORDER BY day_of_week, hour_of_day
        """), {
            'start_time': start_time,
            'end_time': end_time,
            'user_id': user_id
        })
        
        # Initialize 7x24 grid
        heatmap_data = [[0 for _ in range(24)] for _ in range(7)]
        max_value = 0
        
        for row in query_result:
            day = int(row.day_of_week)  # 0 = Sunday, 6 = Saturday
            hour = int(row.hour_of_day)
            
            if metric_name == 'transaction_volume':
                value = float(row.total_amount or 0)
            else:
                value = row.transaction_count
            
            heatmap_data[day][hour] = value
            max_value = max(max_value, value)
        
        return {
            'data': heatmap_data,
            'max_value': max_value,
            'dimensions': {'rows': 7, 'cols': 24},
            'labels': {
                'rows': ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'],
                'cols': [f'{h:02d}:00' for h in range(24)]
            }
        }

    def _resolve_time_range(
        self,
        filters: Optional[DashboardFilter],
        widget_config: Dict[str, Any]
    ) -> Tuple[datetime, datetime]:
        """Resolve time range from filters or widget configuration."""
        
        if filters and filters.time_range == ChartTimeRange.CUSTOM:
            return filters.custom_start_date, filters.custom_end_date
        
        time_range = filters.time_range if filters else widget_config.get('time_range', '24h')
        
        now = datetime.utcnow()
        
        if time_range == '1h':
            start_time = now - timedelta(hours=1)
        elif time_range == '24h':
            start_time = now - timedelta(days=1)
        elif time_range == '7d':
            start_time = now - timedelta(days=7)
        elif time_range == '30d':
            start_time = now - timedelta(days=30)
        elif time_range == '90d':
            start_time = now - timedelta(days=90)
        else:
            start_time = now - timedelta(days=1)  # Default to 24h
        
        return start_time, now

    async def _calculate_metric_value(
        self,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        user_id: Optional[int] = None
    ) -> float:
        """Calculate specific metric values."""
        
        if metric_name == 'total_transactions':
            query = self.db_session.query(func.count(Transaction.id))
        elif metric_name == 'total_volume':
            query = self.db_session.query(func.sum(Transaction.amount))
        elif metric_name == 'avg_transaction_value':
            query = self.db_session.query(func.avg(Transaction.amount))
        elif metric_name == 'unique_users':
            query = self.db_session.query(func.count(func.distinct(Transaction.user_id)))
        elif metric_name == 'active_sessions':
            query = self.db_session.query(func.count(UserSession.id)).filter(
                UserSession.is_active == True
            )
            return float(query.scalar() or 0)
        else:
            # Custom metric - delegate to analytics service
            return await self.analytics_service.calculate_custom_metric(
                metric_name, start_time, end_time, user_id
            )
        
        # Apply time and user filters for transaction metrics
        query = query.filter(
            Transaction.created_at >= start_time,
            Transaction.created_at <= end_time,
            Transaction.status == TransactionStatus.COMPLETED
        )
        
        if user_id:
            query = query.filter(Transaction.user_id == user_id)
        
        result = query.scalar()
        return float(result or 0)

    async def _calculate_trend(
        self,
        metric_name: str,
        current_value: float,
        trend_period: str,
        user_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Calculate trend data for metrics."""
        
        # Determine comparison period
        if trend_period == '1h':
            compare_duration = timedelta(hours=1)
        elif trend_period == '24h':
            compare_duration = timedelta(days=1)
        elif trend_period == '7d':
            compare_duration = timedelta(days=7)
        else:
            compare_duration = timedelta(days=1)
        
        # Calculate previous period value
        now = datetime.utcnow()
        previous_end = now - compare_duration
        previous_start = previous_end - compare_duration
        
        previous_value = await self._calculate_metric_value(
            metric_name, previous_start, previous_end, user_id
        )
        
        # Calculate trend
        if previous_value > 0:
            change_percent = ((current_value - previous_value) / previous_value) * 100
        else:
            change_percent = 100.0 if current_value > 0 else 0.0
        
        return {
            'previous_value': previous_value,
            'current_value': current_value,
            'change_absolute': current_value - previous_value,
            'change_percent': round(change_percent, 2),
            'trend_direction': 'up' if change_percent > 0 else 'down' if change_percent < 0 else 'stable'
        }

    def _format_metric_value(
        self,
        value: float,
        format_type: str,
        precision: int
    ) -> str:
        """Format metric values based on type."""
        
        if format_type == 'currency':
            return f"${value:,.{precision}f}"
        elif format_type == 'percentage':
            return f"{value:.{precision}f}%"
        elif format_type == 'duration':
            # Convert to human readable duration
            if value < 60:
                return f"{value:.1f}s"
            elif value < 3600:
                return f"{value/60:.1f}m"
            else:
                return f"{value/3600:.1f}h"
        else:  # number
            if value >= 1000000:
                return f"{value/1000000:.{precision}f}M"
            elif value >= 1000:
                return f"{value/1000:.{precision}f}K"
            else:
                return f"{value:,.{precision}f}"

    def _check_metric_alerts(
        self,
        widget_id: int,
        metric_name: str,
        current_value: float,
        thresholds: Optional[Dict[str, float]]
    ) -> List[MetricAlert]:
        """Check metric against alert thresholds."""
        
        alerts = []
        
        if not thresholds:
            return alerts
        
        for severity, threshold_value in thresholds.items():
            if severity == 'critical' and current_value >= threshold_value:
                alerts.append(MetricAlert(
                    widget_id=widget_id,
                    metric_name=metric_name,
                    current_value=current_value,
                    threshold_value=threshold_value,
                    severity='critical',
                    message=f"{metric_name} has reached critical level: {current_value}",
                    triggered_at=datetime.utcnow()
                ))
            elif severity == 'warning' and current_value >= threshold_value:
                alerts.append(MetricAlert(
                    widget_id=widget_id,
                    metric_name=metric_name,
                    current_value=current_value,
                    threshold_value=threshold_value,
                    severity='medium',
                    message=f"{metric_name} is above warning threshold: {current_value}",
                    triggered_at=datetime.utcnow()
                ))
        
        return alerts

    def _get_widget_metadata(
        self,
        widget: DashboardWidget,
        time_range: Tuple[datetime, datetime]
    ) -> Dict[str, Any]:
        """Get metadata about widget and its data."""
        
        start_time, end_time = time_range
        
        return {
            'widget_name': widget.name,
            'widget_type': widget.widget_type,
            'time_range': {
                'start': start_time.isoformat(),
                'end': end_time.isoformat(),
                'duration_hours': (end_time - start_time).total_seconds() / 3600
            },
            'configuration': widget.configuration,
            'last_updated': widget.updated_at.isoformat() if widget.updated_at else None
        }

    async def _calculate_system_load_percentage(self) -> float:
        """Calculate system load as a percentage."""
        
        # Get current metrics
        pending_count = self.db_session.query(Transaction).filter_by(
            status=TransactionStatus.PENDING
        ).count()
        
        processing_count = self.db_session.query(Transaction).filter_by(
            status=TransactionStatus.PROCESSING
        ).count()
        
        # Get average processing time in last hour
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        avg_processing_time = self.db_session.query(
            func.avg(Transaction.processing_time_ms)
        ).filter(
            Transaction.created_at >= one_hour_ago,
            Transaction.status == TransactionStatus.COMPLETED
        ).scalar() or 0
        
        # Calculate load score (0-100)
        load_score = 0
        
        # Pending transactions contribute to load
        if pending_count > 100:
            load_score += 40
        elif pending_count > 50:
            load_score += 25
        elif pending_count > 20:
            load_score += 15
        
        # Processing transactions contribute to load
        if processing_count > 50:
            load_score += 30
        elif processing_count > 20:
            load_score += 20
        elif processing_count > 10:
            load_score += 10
        
        # High processing times contribute to load
        if avg_processing_time > 2000:  # > 2 seconds
            load_score += 30
        elif avg_processing_time > 1000:  # > 1 second
            load_score += 20
        elif avg_processing_time > 500:  # > 0.5 seconds
            load_score += 10
        
        return min(load_score, 100)  # Cap at 100%

    async def _calculate_success_rate(
        self,
        start_time: datetime,
        end_time: datetime,
        user_id: Optional[int] = None
    ) -> float:
        """Calculate transaction success rate as percentage."""
        
        query = self.db_session.query(
            func.count(Transaction.id).label('total'),
            func.count(
                case((Transaction.status == TransactionStatus.COMPLETED, Transaction.id), else_=None)
            ).label('completed')
        ).filter(
            Transaction.created_at >= start_time,
            Transaction.created_at <= end_time
        )
        
        if user_id:
            query = query.filter(Transaction.user_id == user_id)
        
        result = query.first()
        
        if result.total and result.total > 0:
            return (result.completed / result.total) * 100
        else:
            return 0.0

    async def _calculate_capacity_utilization(self) -> float:
        """Calculate system capacity utilization percentage."""
        
        # This would integrate with your infrastructure monitoring
        # For demonstration, calculate based on active connections and processing load
        
        # Get database connection stats
        db_stats = self.db_session.execute(text("""
            SELECT count(*) as active_connections
            FROM pg_stat_activity 
            WHERE datname = current_database() AND state = 'active'
        """)).first()
        
        active_connections = db_stats.active_connections if db_stats else 0
        max_connections = 200  # Your configured maximum
        
        connection_utilization = (active_connections / max_connections) * 100
        
        # Factor in transaction processing load
        load_percentage = await self._calculate_system_load_percentage()
        
        # Combined capacity utilization
        capacity_utilization = (connection_utilization * 0.6) + (load_percentage * 0.4)
        
        return min(capacity_utilization, 100)

    def _determine_gauge_color(
        self,
        value: float,
        warning_threshold: Optional[float],
        critical_threshold: Optional[float]
    ) -> str:
        """Determine gauge color based on thresholds."""
        
        if critical_threshold and value >= critical_threshold:
            return 'red'
        elif warning_threshold and value >= warning_threshold:
            return 'yellow'
        else:
            return 'green'

    def _get_gauge_status(
        self,
        current_value: float,
        config: Dict[str, Any]
    ) -> str:
        """Get status text for gauge widget."""
        
        warning_threshold = config.get('warning_threshold')
        critical_threshold = config.get('critical_threshold')
        
        if critical_threshold and current_value >= critical_threshold:
            return 'Critical'
        elif warning_threshold and current_value >= warning_threshold:
            return 'Warning'
        else:
            return 'Normal'


# Factory function
def create_dashboard_service(db_session: Session) -> DashboardService:
    """Create and configure dashboard service."""
    return DashboardService(db_session)
```


### 6.5 Dashboard API Endpoints and Real-time WebSocket Integration

**app/api/v1/dashboard.py** - Complete Dashboard API:
```python
"""
Complete dashboard API endpoints with real-time WebSocket support.
Demonstrates advanced API patterns with SQLAlchemy analytics integration.
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, Any, List, Optional

from fastapi import APIRouter, Depends, HTTPException, status, WebSocket, WebSocketDisconnect
from fastapi import BackgroundTasks, Query, Path
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import get_current_user, get_current_admin, get_pagination_params
from app.models.user import User
from app.models.analytics import Dashboard, DashboardWidget
from app.services.dashboard_service import DashboardService, create_dashboard_service
from app.services.real_time_analytics import RealTimeAnalyticsService, create_real_time_analytics_service
from app.schemas.dashboard import (
    DashboardCreate, DashboardUpdate, DashboardResponse,
    WidgetCreate, WidgetUpdate, WidgetResponse, WidgetDataResponse,
    DashboardFilter, WidgetQuery, ExportRequest, ExportResponse,
    DashboardShare, SharedDashboardResponse, DashboardLayoutUpdate
)
from app.schemas.common import APIResponse, PaginationParams
from app.core.exceptions import NotFoundError, ValidationError

router = APIRouter(prefix="/dashboard", tags=["dashboard"])


# Dashboard Management Endpoints
@router.get("/", response_model=APIResponse[List[DashboardResponse]])
async def get_user_dashboards(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    pagination: PaginationParams = Depends(get_pagination_params),
    include_public: bool = Query(False, description="Include public dashboards")
):
    """Get user's dashboards with optional public dashboards."""
    
    dashboard_service = create_dashboard_service(db)
    
    query = db.query(Dashboard).filter(
        or_(
            Dashboard.owner_id == current_user.id,
            and_(Dashboard.is_public == True, include_public == True)
        )
    )
    
    total_count = query.count()
    dashboards = (
        query.offset((pagination.page - 1) * pagination.size)
        .limit(pagination.size)
        .all()
    )
    
    dashboard_responses = []
    for dashboard in dashboards:
        widget_count = db.query(DashboardWidget).filter_by(
            dashboard_id=dashboard.id
        ).count()
        
        dashboard_responses.append(DashboardResponse(
            **dashboard.__dict__,
            widget_count=widget_count
        ))
    
    return APIResponse(
        data=dashboard_responses,
        message=f"Retrieved {len(dashboard_responses)} dashboards",
        metadata={
            'total_count': total_count,
            'page': pagination.page,
            'page_size': pagination.size,
            'include_public': include_public
        }
    )


@router.post("/", response_model=APIResponse[DashboardResponse])
async def create_dashboard(
    dashboard_data: DashboardCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Create a new dashboard."""
    
    try:
        # Check if user already has a default dashboard when creating one
        if dashboard_data.is_default:
            existing_default = db.query(Dashboard).filter_by(
                owner_id=current_user.id,
                is_default=True
            ).first()
            
            if existing_default:
                existing_default.is_default = False
                db.add(existing_default)
        
        # Create new dashboard
        dashboard = Dashboard(
            name=dashboard_data.name,
            description=dashboard_data.description,
            owner_id=current_user.id,
            is_public=dashboard_data.is_public,
            is_default=dashboard_data.is_default,
            layout_config=dashboard_data.layout_config or {}
        )
        
        db.add(dashboard)
        db.commit()
        db.refresh(dashboard)
        
        response = DashboardResponse(
            **dashboard.__dict__,
            widget_count=0
        )
        
        return APIResponse(
            data=response,
            message="Dashboard created successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error creating dashboard: {str(e)}"
        )


@router.get("/{dashboard_id}", response_model=APIResponse[Dict[str, Any]])
async def get_dashboard_data(
    dashboard_id: int = Path(..., gt=0),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    time_range: str = Query("24h", description="Time range for data"),
    force_refresh: bool = Query(False, description="Force refresh all widgets")
):
    """Get complete dashboard with all widget data."""
    
    # Verify dashboard access
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id and not dashboard.is_public:
        raise HTTPException(status_code=403, detail="Access denied")
    
    # Create dashboard service and get data
    dashboard_service = create_dashboard_service(db)
    
    # Parse filters
    filters = DashboardFilter(time_range=time_range)
    
    try:
        dashboard_data = await dashboard_service.get_dashboard_data(
            dashboard_id=dashboard_id,
            filters=filters,
            user_id=current_user.id
        )
        
        return APIResponse(
            data=dashboard_data,
            message="Dashboard data retrieved successfully"
        )
        
    except NotFoundError:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving dashboard data: {str(e)}"
        )


@router.put("/{dashboard_id}", response_model=APIResponse[DashboardResponse])
async def update_dashboard(
    dashboard_id: int = Path(..., gt=0),
    dashboard_data: DashboardUpdate = ...,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Update dashboard configuration."""
    
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        # Update dashboard fields
        update_data = dashboard_data.dict(exclude_unset=True)
        
        for field, value in update_data.items():
            setattr(dashboard, field, value)
        
        dashboard.updated_at = datetime.utcnow()
        db.commit()
        db.refresh(dashboard)
        
        widget_count = db.query(DashboardWidget).filter_by(
            dashboard_id=dashboard.id
        ).count()
        
        response = DashboardResponse(
            **dashboard.__dict__,
            widget_count=widget_count
        )
        
        return APIResponse(
            data=response,
            message="Dashboard updated successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error updating dashboard: {str(e)}"
        )


@router.delete("/{dashboard_id}")
async def delete_dashboard(
    dashboard_id: int = Path(..., gt=0),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Delete a dashboard and all its widgets."""
    
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        # Delete all widgets first
        db.query(DashboardWidget).filter_by(dashboard_id=dashboard_id).delete()
        
        # Delete dashboard
        db.delete(dashboard)
        db.commit()
        
        return APIResponse(
            data=None,
            message="Dashboard deleted successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error deleting dashboard: {str(e)}"
        )


# Widget Management Endpoints
@router.get("/{dashboard_id}/widgets", response_model=APIResponse[List[WidgetResponse]])
async def get_dashboard_widgets(
    dashboard_id: int = Path(..., gt=0),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get all widgets for a dashboard."""
    
    # Verify dashboard access
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id and not dashboard.is_public:
        raise HTTPException(status_code=403, detail="Access denied")
    
    widgets = (
        db.query(DashboardWidget)
        .filter_by(dashboard_id=dashboard_id)
        .order_by(DashboardWidget.position_y, DashboardWidget.position_x)
        .all()
    )
    
    widget_responses = [WidgetResponse(**widget.__dict__) for widget in widgets]
    
    return APIResponse(
        data=widget_responses,
        message=f"Retrieved {len(widget_responses)} widgets"
    )


@router.post("/{dashboard_id}/widgets", response_model=APIResponse[WidgetResponse])
async def create_widget(
    dashboard_id: int = Path(..., gt=0),
    widget_data: WidgetCreate = ...,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Create a new widget in a dashboard."""
    
    # Verify dashboard ownership
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        widget = DashboardWidget(
            dashboard_id=dashboard_id,
            name=widget_data.name,
            widget_type=widget_data.widget_type.value,
            position_x=widget_data.position_x,
            position_y=widget_data.position_y,
            is_visible=widget_data.is_visible,
            configuration=widget_data.configuration.dict()
        )
        
        db.add(widget)
        db.commit()
        db.refresh(widget)
        
        response = WidgetResponse(**widget.__dict__)
        
        return APIResponse(
            data=response,
            message="Widget created successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error creating widget: {str(e)}"
        )


@router.get("/widgets/{widget_id}/data", response_model=APIResponse[WidgetDataResponse])
async def get_widget_data(
    widget_id: int = Path(..., gt=0),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    force_refresh: bool = Query(False, description="Force refresh widget data"),
    time_range: str = Query("24h", description="Time range for data")
):
    """Get data for a specific widget."""
    
    widget = db.query(DashboardWidget).filter_by(id=widget_id).first()
    if not widget:
        raise HTTPException(status_code=404, detail="Widget not found")
    
    # Check dashboard access
    dashboard = db.query(Dashboard).filter_by(id=widget.dashboard_id).first()
    if dashboard.owner_id != current_user.id and not dashboard.is_public:
        raise HTTPException(status_code=403, detail="Access denied")
    
    dashboard_service = create_dashboard_service(db)
    filters = DashboardFilter(time_range=time_range)
    
    try:
        widget_data = await dashboard_service._get_widget_data(
            widget=widget,
            filters=filters,
            user_id=current_user.id
        )
        
        return APIResponse(
            data=widget_data,
            message="Widget data retrieved successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving widget data: {str(e)}"
        )


@router.put("/widgets/{widget_id}", response_model=APIResponse[WidgetResponse])
async def update_widget(
    widget_id: int = Path(..., gt=0),
    widget_data: WidgetUpdate = ...,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Update widget configuration."""
    
    widget = db.query(DashboardWidget).filter_by(id=widget_id).first()
    if not widget:
        raise HTTPException(status_code=404, detail="Widget not found")
    
    # Check dashboard ownership
    dashboard = db.query(Dashboard).filter_by(id=widget.dashboard_id).first()
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        # Update widget fields
        update_data = widget_data.dict(exclude_unset=True)
        
        for field, value in update_data.items():
            setattr(widget, field, value)
        
        widget.updated_at = datetime.utcnow()
        db.commit()
        db.refresh(widget)
        
        response = WidgetResponse(**widget.__dict__)
        
        return APIResponse(
            data=response,
            message="Widget updated successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error updating widget: {str(e)}"
        )


@router.delete("/widgets/{widget_id}")
async def delete_widget(
    widget_id: int = Path(..., gt=0),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Delete a widget."""
    
    widget = db.query(DashboardWidget).filter_by(id=widget_id).first()
    if not widget:
        raise HTTPException(status_code=404, detail="Widget not found")
    
    # Check dashboard ownership
    dashboard = db.query(Dashboard).filter_by(id=widget.dashboard_id).first()
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        db.delete(widget)
        db.commit()
        
        return APIResponse(
            data=None,
            message="Widget deleted successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error deleting widget: {str(e)}"
        )


# Layout Management
@router.put("/{dashboard_id}/layout", response_model=APIResponse[DashboardResponse])
async def update_dashboard_layout(
    dashboard_id: int = Path(..., gt=0),
    layout_data: DashboardLayoutUpdate = ...,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Update dashboard layout configuration."""
    
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        # Update widget positions
        for position in layout_data.layout_config.widget_positions:
            widget = db.query(DashboardWidget).filter_by(
                id=position.widget_id,
                dashboard_id=dashboard_id
            ).first()
            
            if widget:
                widget.position_x = position.x
                widget.position_y = position.y
                # Store width/height in configuration
                if not widget.configuration:
                    widget.configuration = {}
                widget.configuration.update({
                    'width': position.width,
                    'height': position.height
                })
        
        # Update dashboard layout config
        dashboard.layout_config = layout_data.layout_config.dict()
        dashboard.updated_at = datetime.utcnow()
        
        db.commit()
        db.refresh(dashboard)
        
        widget_count = db.query(DashboardWidget).filter_by(
            dashboard_id=dashboard.id
        ).count()
        
        response = DashboardResponse(
            **dashboard.__dict__,
            widget_count=widget_count
        )
        
        return APIResponse(
            data=response,
            message="Dashboard layout updated successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error updating layout: {str(e)}"
        )


# Real-time WebSocket Endpoint
@router.websocket("/{dashboard_id}/realtime")
async def dashboard_realtime_websocket(
    websocket: WebSocket,
    dashboard_id: int,
    db: Session = Depends(get_db)
):
    """WebSocket endpoint for real-time dashboard updates."""
    
    try:
        # Create real-time analytics service
        realtime_service = create_real_time_analytics_service(db)
        
        # Connect WebSocket
        await realtime_service.connect_websocket(websocket)
        
        try:
            while True:
                # Wait for client messages (ping/pong, filter updates, etc.)
                try:
                    message = await asyncio.wait_for(
                        websocket.receive_text(),
                        timeout=30.0  # 30 second timeout
                    )
                    
                    # Parse client message
                    try:
                        data = json.loads(message)
                        message_type = data.get('type')
                        
                        if message_type == 'ping':
                            await websocket.send_text(json.dumps({
                                'type': 'pong',
                                'timestamp': datetime.utcnow().isoformat()
                            }))
                        
                        elif message_type == 'update_filters':
                            # Handle filter updates for this specific dashboard
                            filters = data.get('filters', {})
                            # Apply filters and send updated data
                            dashboard_service = create_dashboard_service(db)
                            dashboard_data = await dashboard_service.get_dashboard_data(
                                dashboard_id,
                                filters=DashboardFilter(**filters)
                            )
                            
                            await websocket.send_text(json.dumps({
                                'type': 'dashboard_update',
                                'data': dashboard_data
                            }, default=str))
                        
                    except json.JSONDecodeError:
                        await websocket.send_text(json.dumps({
                            'type': 'error',
                            'message': 'Invalid JSON message'
                        }))
                
                except asyncio.TimeoutError:
                    # Send keepalive ping
                    await websocket.send_text(json.dumps({
                        'type': 'keepalive',
                        'timestamp': datetime.utcnow().isoformat()
                    }))
        
        except WebSocketDisconnect:
            pass
        
    except Exception as e:
        logger.error(f"WebSocket error for dashboard {dashboard_id}: {e}")
    
    finally:
        # Clean up connection
        realtime_service.disconnect_websocket(websocket)


# Dashboard Sharing and Export
@router.post("/{dashboard_id}/share", response_model=APIResponse[SharedDashboardResponse])
async def share_dashboard(
    dashboard_id: int = Path(..., gt=0),
    share_data: DashboardShare = ...,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Share a dashboard with specific users or publicly."""
    
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        # Generate share token
        import secrets
        share_token = secrets.token_urlsafe(32)
        
        # Store sharing configuration
        sharing_config = {
            'share_type': share_data.share_type,
            'share_token': share_token,
            'expires_at': share_data.expires_at.isoformat() if share_data.expires_at else None,
            'permissions': share_data.permissions,
            'allowed_users': share_data.allowed_users
        }
        
        # Update dashboard with sharing info
        if not dashboard.metadata:
            dashboard.metadata = {}
        dashboard.metadata['sharing'] = sharing_config
        dashboard.updated_at = datetime.utcnow()
        
        db.commit()
        
        response = SharedDashboardResponse(
            share_token=share_token,
            dashboard=DashboardResponse(**dashboard.__dict__, widget_count=0),
            permissions=share_data.permissions,
            expires_at=share_data.expires_at
        )
        
        return APIResponse(
            data=response,
            message="Dashboard shared successfully"
        )
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error sharing dashboard: {str(e)}"
        )


@router.post("/{dashboard_id}/export", response_model=APIResponse[ExportResponse])
async def export_dashboard_data(
    dashboard_id: int = Path(..., gt=0),
    export_request: ExportRequest = ...,
    background_tasks: BackgroundTasks = ...,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Export dashboard data in various formats."""
    
    dashboard = db.query(Dashboard).filter_by(id=dashboard_id).first()
    if not dashboard:
        raise HTTPException(status_code=404, detail="Dashboard not found")
    
    if dashboard.owner_id != current_user.id and not dashboard.is_public:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        import uuid
        export_id = str(uuid.uuid4())
        
        # Schedule background export task
        background_tasks.add_task(
            _process_dashboard_export,
            export_id,
            dashboard_id,
            export_request,
            current_user.id
        )
        
        # Generate temporary download URL
        download_url = f"/api/v1/dashboard/exports/{export_id}/download"
        
        response = ExportResponse(
            export_id=export_id,
            download_url=download_url,
            file_size_mb=0.0,  # Will be updated when export completes
            expires_at=datetime.utcnow() + timedelta(hours=24),
            record_count=0  # Will be updated when export completes
        )
        
        return APIResponse(
            data=response,
            message="Export started. Check download URL in a few minutes."
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error starting export: {str(e)}"
        )


# Background task for data export
async def _process_dashboard_export(
    export_id: str,
    dashboard_id: int,
    export_request: ExportRequest,
    user_id: int
):
    """Background task to process dashboard data export."""
    
    try:
        # This would implement the actual export logic
        # For demonstration, we'll create a simple implementation
        
        # Get dashboard data
        with get_db_session() as db:
            dashboard_service = create_dashboard_service(db)
            filters = DashboardFilter(
                time_range=export_request.time_range,
                custom_start_date=export_request.custom_start_date,
                custom_end_date=export_request.custom_end_date
            )
            
            dashboard_data = await dashboard_service.get_dashboard_data(
                dashboard_id, filters, user_id
            )
            
            # Convert to export format (CSV, JSON, Excel)
            if export_request.format_type == 'json':
                import json
                export_content = json.dumps(dashboard_data, default=str, indent=2)
                filename = f"dashboard_{dashboard_id}_{export_id}.json"
                
            elif export_request.format_type == 'csv':
                import csv
                import io
                
                output = io.StringIO()
                writer = csv.writer(output)
                
                # Write dashboard summary
                writer.writerow(['Dashboard Export'])
                writer.writerow(['Dashboard ID', dashboard_id])
                writer.writerow(['Export Time', datetime.utcnow().isoformat()])
                writer.writerow([])
                
                # Write widget data
                for widget in dashboard_data.get('widgets', []):
                    writer.writerow([f"Widget: {widget.get('widget_id')}"])
                    if isinstance(widget.get('data'), list):
                        for item in widget['data']:
                            writer.writerow([str(v) for v in item.values()])
                    writer.writerow([])
                
                export_content = output.getvalue()
                filename = f"dashboard_{dashboard_id}_{export_id}.csv"
            
            # Store export file (in production, use cloud storage)
            export_file_path = f"/tmp/{filename}"
            with open(export_file_path, 'w') as f:
                f.write(export_content)
            
            # Update export status (in production, store in database)
            logger.info(f"Export {export_id} completed: {export_file_path}")
    
    except Exception as e:
        logger.error(f"Export {export_id} failed: {e}")


# Analytics and Reporting Endpoints
@router.get("/analytics/overview", response_model=APIResponse[Dict[str, Any]])
async def get_dashboard_analytics_overview(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    days: int = Query(30, ge=1, le=365, description="Number of days for analytics")
):
    """Get overview analytics for all user dashboards."""
    
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # Get dashboard usage statistics
    dashboard_stats = db.execute(text("""
        SELECT 
            d.id,
            d.name,
            count(dw.id) as widget_count,
            d.created_at,
            d.updated_at,
            coalesce(d.metadata->>'view_count', '0')::int as view_count
        FROM dashboards d
        LEFT JOIN dashboard_widgets dw ON d.id = dw.dashboard_id
        WHERE d.owner_id = :user_id
            AND d.created_at >= :start_date
        GROUP BY d.id, d.name, d.created_at, d.updated_at, d.metadata
        ORDER BY view_count DESC, d.updated_at DESC
    """), {
        'user_id': current_user.id,
        'start_date': start_date
    }).fetchall()
    
    # Calculate summary statistics
    total_dashboards = len(dashboard_stats)
    total_widgets = sum(row.widget_count for row in dashboard_stats)
    total_views = sum(row.view_count for row in dashboard_stats)
    
    most_used_dashboard = dashboard_stats[0] if dashboard_stats else None
    
    analytics_data = {
        'summary': {
            'total_dashboards': total_dashboards,
            'total_widgets': total_widgets,
            'total_views': total_views,
            'avg_widgets_per_dashboard': total_widgets / total_dashboards if total_dashboards > 0 else 0,
            'period_days': days
        },
        'most_used_dashboard': {
            'id': most_used_dashboard.id,
            'name': most_used_dashboard.name,
            'view_count': most_used_dashboard.view_count,
            'widget_count': most_used_dashboard.widget_count
        } if most_used_dashboard else None,
        'dashboard_list': [
            {
                'id': row.id,
                'name': row.name,
                'widget_count': row.widget_count,
                'view_count': row.view_count,
                'last_updated': row.updated_at.isoformat()
            }
            for row in dashboard_stats
        ]
    }
    
    return APIResponse(
        data=analytics_data,
        message="Dashboard analytics retrieved successfully"
    )
```

**app/api/v1/analytics.py** - Analytics API Integration:
```python
"""
Analytics API endpoints integrating with the real-time analytics system.
Demonstrates advanced querying patterns and custom metric calculations.
"""

from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, Path
from sqlalchemy.orm import Session

from app.database import get_db
from app.api.deps import get_current_user, get_current_admin
from app.models.user import User
from app.services.analytics_query_service import AnalyticsQueryService
from app.services.real_time_analytics import publish_event
from app.schemas.common import APIResponse
from app.schemas.dashboard import ChartTimeRange

router = APIRouter(prefix="/analytics", tags=["analytics"])


@router.get("/metrics/{metric_name}", response_model=APIResponse[Dict[str, Any]])
async def get_metric_data(
    metric_name: str = Path(..., description="Name of the metric to retrieve"),
    time_range: ChartTimeRange = Query(ChartTimeRange.LAST_DAY),
    custom_start_date: Optional[datetime] = Query(None),
    custom_end_date: Optional[datetime] = Query(None),
    group_by: str = Query("hour", regex="^(hour|day|week|month)$"),
    user_id: Optional[int] = Query(None, description="Filter by specific user"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get specific metric data with time series aggregation."""
    
    analytics_service = AnalyticsQueryService(db)
    
    # Resolve time range
    if time_range == ChartTimeRange.CUSTOM:
        if not custom_start_date or not custom_end_date:
            raise HTTPException(status_code=400, detail="Custom dates required for custom time range")
        start_time, end_time = custom_start_date, custom_end_date
    else:
        now = datetime.utcnow()
        time_map = {
            ChartTimeRange.LAST_HOUR: timedelta(hours=1),
            ChartTimeRange.LAST_DAY: timedelta(days=1),
            ChartTimeRange.LAST_WEEK: timedelta(days=7),
            ChartTimeRange.LAST_MONTH: timedelta(days=30),
            ChartTimeRange.LAST_QUARTER: timedelta(days=90)
        }
        start_time = now - time_map.get(time_range, timedelta(days=1))
        end_time = now
    
    try:
        metric_data = await analytics_service.calculate_custom_metric(
            metric_name, start_time, end_time, user_id
        )
        
        # Get time series data
        time_series = await analytics_service.get_metric_time_series(
            metric_name, start_time, end_time, group_by, user_id
        )
        
        return APIResponse(
            data={
                'metric_name': metric_name,
                'current_value': metric_data,
                'time_series': time_series,
                'time_range': {
                    'start': start_time.isoformat(),
                    'end': end_time.isoformat(),
                    'group_by': group_by
                }
            },
            message="Metric data retrieved successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving metric data: {str(e)}"
        )


@router.get("/cohorts", response_model=APIResponse[Dict[str, Any]])
async def get_cohort_analysis(
    cohort_type: str = Query("monthly", regex="^(daily|weekly|monthly)$"),
    periods: int = Query(12, ge=1, le=24, description="Number of periods to analyze"),
    metric: str = Query("retention", regex="^(retention|revenue|transactions)$"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get cohort analysis data for user retention and behavior."""
    
    analytics_service = AnalyticsQueryService(db)
    
    try:
        cohort_data = await analytics_service.get_cohort_analysis(
            cohort_type, periods, metric
        )
        
        return APIResponse(
            data=cohort_data,
            message="Cohort analysis retrieved successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving cohort analysis: {str(e)}"
        )


@router.get("/funnels", response_model=APIResponse[Dict[str, Any]])
async def get_funnel_analysis(
    funnel_name: str = Query("user_journey", description="Name of the funnel to analyze"),
    time_range: ChartTimeRange = Query(ChartTimeRange.LAST_MONTH),
    segment: Optional[str] = Query(None, description="User segment to filter by"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get funnel analysis for user conversion tracking."""
    
    analytics_service = AnalyticsQueryService(db)
    
    # Resolve time range
    now = datetime.utcnow()
    if time_range == ChartTimeRange.LAST_DAY:
        start_time = now - timedelta(days=1)
    elif time_range == ChartTimeRange.LAST_WEEK:
        start_time = now - timedelta(days=7)
    elif time_range == ChartTimeRange.LAST_MONTH:
        start_time = now - timedelta(days=30)
    else:
        start_time = now - timedelta(days=7)
    
    try:
        funnel_data = await analytics_service.get_funnel_analysis(
            funnel_name, start_time, now, segment
        )
        
        return APIResponse(
            data=funnel_data,
            message="Funnel analysis retrieved successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving funnel analysis: {str(e)}"
        )


@router.post("/events", response_model=APIResponse[Dict[str, str]])
async def track_custom_event(
    event_type: str = Query(..., description="Type of event to track"),
    event_data: Dict[str, Any] = ...,
    current_user: User = Depends(get_current_user)
):
    """Track a custom analytics event."""
    
    try:
        # Add user context to event data
        event_data.update({
            'user_id': current_user.id,
            'username': current_user.username,
            'timestamp': datetime.utcnow().isoformat()
        })
        
        # Publish event for real-time processing
        await publish_event(event_type, event_data)
        
        return APIResponse(
            data={'status': 'success', 'event_type': event_type},
            message="Event tracked successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error tracking event: {str(e)}"
        )


@router.get("/real-time/summary", response_model=APIResponse[Dict[str, Any]])
async def get_real_time_summary(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get real-time system summary for quick overview."""
    
    analytics_service = AnalyticsQueryService(db)
    
    try:
        # Get key real-time metrics
        current_time = datetime.utcnow()
        last_hour = current_time - timedelta(hours=1)
        
        summary_data = {
            'active_users': await analytics_service.get_active_user_count(),
            'recent_transactions': await analytics_service.get_transaction_count(
                last_hour, current_time
            ),
            'system_health': await analytics_service.get_system_health_score(),
            'error_rate': await analytics_service.get_error_rate_percentage(
                last_hour, current_time
            ),
            'avg_response_time': await analytics_service.get_avg_response_time(
                last_hour, current_time
            ),
            'timestamp': current_time.isoformat()
        }
        
        return APIResponse(
            data=summary_data,
            message="Real-time summary retrieved successfully"
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving real-time summary: {str(e)}"
        )
```


### 6.6 Usage Examples and Best Practices for Real-time Analytics

**Example 1: Setting Up a Financial Dashboard**

```python
"""
Complete example: Setting up a real-time financial analytics dashboard.
Demonstrates end-to-end dashboard creation with multiple widget types.
"""

from datetime import datetime, timedelta
from app.models.analytics import Dashboard, DashboardWidget
from app.services.dashboard_service import create_dashboard_service
from app.schemas.dashboard import *

async def create_financial_dashboard(db_session, user_id: int) -> Dashboard:
    """Create a comprehensive financial dashboard with real-time widgets."""
    
    # Create main dashboard
    dashboard = Dashboard(
        name="Financial Analytics Dashboard",
        description="Real-time financial transaction monitoring and analytics",
        owner_id=user_id,
        is_public=False,
        layout_config={
            'grid_columns': 12,
            'grid_rows': 20,
            'auto_layout': True
        }
    )
    
    db_session.add(dashboard)
    db_session.commit()
    db_session.refresh(dashboard)
    
    # Widget 1: Total Transaction Volume (Metric Card)
    volume_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Total Transaction Volume",
        widget_type="metric_card",
        position_x=0,
        position_y=0,
        configuration={
            'metric_name': 'total_volume',
            'format_type': 'currency',
            'precision': 2,
            'show_trend': True,
            'trend_period': '24h',
            'refresh_interval': 30,
            'alert_thresholds': {
                'warning': 1000000,  # $1M
                'critical': 5000000  # $5M
            }
        }
    )
    
    # Widget 2: Transaction Count Trend (Line Chart)
    trend_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Transaction Trend",
        widget_type="chart_line",
        position_x=4,
        position_y=0,
        configuration={
            'metric_name': 'transaction_count',
            'time_range': '24h',
            'group_by': 'hour',
            'aggregation': 'count',
            'show_legend': True,
            'color_scheme': 'blue',
            'refresh_interval': 60
        }
    )
    
    # Widget 3: Transaction Success Rate (Gauge)
    success_rate_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Success Rate",
        widget_type="gauge",
        position_x=8,
        position_y=0,
        configuration={
            'metric_name': 'success_rate',
            'min_value': 0,
            'max_value': 100,
            'target_value': 99.5,
            'warning_threshold': 95,
            'critical_threshold': 90,
            'unit': '%',
            'refresh_interval': 30
        }
    )
    
    # Widget 4: Recent Transactions (Table)
    recent_transactions_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Recent Transactions",
        widget_type="table",
        position_x=0,
        position_y=4,
        configuration={
            'data_source': 'recent_transactions',
            'columns': [
                {'field': 'date', 'title': 'Date', 'width': '15%'},
                {'field': 'amount', 'title': 'Amount', 'width': '15%'},
                {'field': 'type', 'title': 'Type', 'width': '15%'},
                {'field': 'status', 'title': 'Status', 'width': '15%'},
                {'field': 'user', 'title': 'User', 'width': '20%'},
                {'field': 'description', 'title': 'Description', 'width': '20%'}
            ],
            'page_size': 20,
            'sortable': True,
            'filterable': True,
            'refresh_interval': 60
        }
    )
    
    # Widget 5: Transaction Volume by Type (Pie Chart)
    volume_by_type_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Volume by Transaction Type",
        widget_type="chart_pie",
        position_x=6,
        position_y=4,
        configuration={
            'metric_name': 'volume_by_type',
            'time_range': '24h',
            'aggregation': 'sum',
            'show_legend': True,
            'color_scheme': 'rainbow',
            'refresh_interval': 300
        }
    )
    
    # Widget 6: Transaction Activity Heatmap
    activity_heatmap_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Transaction Activity Heatmap",
        widget_type="heatmap",
        position_x=0,
        position_y=8,
        configuration={
            'metric_name': 'transaction_volume',
            'time_range': '7d',
            'color_scheme': 'heat',
            'refresh_interval': 300
        }
    )
    
    # Add all widgets
    widgets = [
        volume_widget, trend_widget, success_rate_widget,
        recent_transactions_widget, volume_by_type_widget,
        activity_heatmap_widget
    ]
    
    for widget in widgets:
        db_session.add(widget)
    
    db_session.commit()
    
    return dashboard

# Usage example
async def demo_financial_dashboard():
    """Demonstrate financial dashboard creation and usage."""
    
    with get_db_session() as db:
        # Create dashboard for user
        user_id = 1  # Example user ID
        dashboard = await create_financial_dashboard(db, user_id)
        
        # Get dashboard service
        dashboard_service = create_dashboard_service(db)
        
        # Get real-time dashboard data
        dashboard_data = await dashboard_service.get_dashboard_data(
            dashboard_id=dashboard.id,
            filters=DashboardFilter(time_range="24h"),
            user_id=user_id
        )
        
        print(f"Dashboard created with {len(dashboard_data['widgets'])} widgets")
        
        # Example: Update widget configuration
        volume_widget = db.query(DashboardWidget).filter_by(
            dashboard_id=dashboard.id,
            name="Total Transaction Volume"
        ).first()
        
        # Update alert thresholds
        volume_widget.configuration['alert_thresholds']['critical'] = 10000000
        db.commit()
        
        print("Dashboard configuration updated")
```

**Example 2: Real-time System Monitoring Dashboard**

```python
"""
Example: Creating a system monitoring dashboard with health metrics.
"""

async def create_system_monitoring_dashboard(db_session, admin_user_id: int) -> Dashboard:
    """Create a real-time system monitoring dashboard."""
    
    dashboard = Dashboard(
        name="System Health Monitor",
        description="Real-time system health and performance monitoring",
        owner_id=admin_user_id,
        is_public=True,  # Visible to all users
        layout_config={
            'grid_columns': 12,
            'grid_rows': 16,
            'refresh_interval': 30
        }
    )
    
    db_session.add(dashboard)
    db_session.commit()
    db_session.refresh(dashboard)
    
    # System Load Gauge
    system_load_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="System Load",
        widget_type="gauge",
        position_x=0,
        position_y=0,
        configuration={
            'metric_name': 'system_load',
            'min_value': 0,
            'max_value': 100,
            'warning_threshold': 70,
            'critical_threshold': 90,
            'unit': '%',
            'refresh_interval': 30
        }
    )
    
    # Database Connections Metric
    db_connections_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Database Connections",
        widget_type="metric_card",
        position_x=3,
        position_y=0,
        configuration={
            'metric_name': 'db_connections',
            'format_type': 'number',
            'show_trend': True,
            'refresh_interval': 30,
            'alert_thresholds': {
                'warning': 150,
                'critical': 200
            }
        }
    )
    
    # Error Rate Trend
    error_rate_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Error Rate Trend",
        widget_type="chart_line",
        position_x=6,
        position_y=0,
        configuration={
            'metric_name': 'error_rate',
            'time_range': '1h',
            'group_by': 'minute',
            'color_scheme': 'red',
            'refresh_interval': 60
        }
    )
    
    # System Metrics Table
    system_metrics_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="System Metrics",
        widget_type="table",
        position_x=0,
        position_y=4,
        configuration={
            'data_source': 'system_metrics',
            'columns': [
                {'field': 'timestamp', 'title': 'Time', 'width': '20%'},
                {'field': 'cpu_usage', 'title': 'CPU %', 'width': '15%'},
                {'field': 'memory_usage', 'title': 'Memory %', 'width': '15%'},
                {'field': 'disk_usage', 'title': 'Disk %', 'width': '15%'},
                {'field': 'active_connections', 'title': 'Connections', 'width': '15%'},
                {'field': 'response_time', 'title': 'Avg Response (ms)', 'width': '20%'}
            ],
            'page_size': 15,
            'refresh_interval': 60
        }
    )
    
    widgets = [system_load_widget, db_connections_widget, error_rate_widget, system_metrics_widget]
    
    for widget in widgets:
        db_session.add(widget)
    
    db_session.commit()
    return dashboard
```

**Example 3: User Engagement Analytics Dashboard**

```python
"""
Example: User engagement analytics with cohort analysis and funnel tracking.
"""

async def create_user_engagement_dashboard(db_session, user_id: int) -> Dashboard:
    """Create a user engagement analytics dashboard."""
    
    dashboard = Dashboard(
        name="User Engagement Analytics",
        description="User behavior analysis and engagement metrics",
        owner_id=user_id,
        layout_config={'grid_columns': 12, 'grid_rows': 20}
    )
    
    db_session.add(dashboard)
    db_session.commit()
    db_session.refresh(dashboard)
    
    # Active Users Metric
    active_users_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Active Users",
        widget_type="metric_card",
        position_x=0,
        position_y=0,
        configuration={
            'metric_name': 'active_users',
            'format_type': 'number',
            'show_trend': True,
            'trend_period': '24h',
            'refresh_interval': 300
        }
    )
    
    # User Registration Trend
    registration_trend_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="User Registration Trend",
        widget_type="chart_line",
        position_x=4,
        position_y=0,
        configuration={
            'metric_name': 'user_registrations',
            'time_range': '30d',
            'group_by': 'day',
            'aggregation': 'count',
            'color_scheme': 'green',
            'refresh_interval': 3600
        }
    )
    
    # Session Duration Distribution
    session_duration_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Session Duration Distribution",
        widget_type="chart_bar",
        position_x=8,
        position_y=0,
        configuration={
            'metric_name': 'session_duration',
            'time_range': '7d',
            'group_by': 'duration_bucket',
            'color_scheme': 'blue',
            'refresh_interval': 1800
        }
    )
    
    # Top User Actions
    top_actions_widget = DashboardWidget(
        dashboard_id=dashboard.id,
        name="Top User Actions",
        widget_type="chart_pie",
        position_x=0,
        position_y=4,
        configuration={
            'metric_name': 'user_actions',
            'time_range': '24h',
            'aggregation': 'count',
            'color_scheme': 'category20',
            'refresh_interval': 600
        }
    )
    
    widgets = [active_users_widget, registration_trend_widget, session_duration_widget, top_actions_widget]
    
    for widget in widgets:
        db_session.add(widget)
    
    db_session.commit()
    return dashboard
```

**Best Practices for Real-time Analytics Dashboards:**

```python
"""
Best practices and patterns for implementing production-ready analytics dashboards.
"""

class DashboardBestPractices:
    """Collection of best practices for dashboard implementation."""
    
    @staticmethod
    async def optimize_widget_performance(widget: DashboardWidget, db_session):
        """Optimize individual widget performance."""
        
        config = widget.configuration
        
        # 1. Set appropriate refresh intervals based on data volatility
        if config.get('metric_name') in ['system_load', 'error_rate']:
            config['refresh_interval'] = 30  # High volatility metrics
        elif config.get('metric_name') in ['total_volume', 'user_count']:
            config['refresh_interval'] = 300  # Medium volatility metrics
        else:
            config['refresh_interval'] = 1800  # Low volatility metrics
        
        # 2. Limit data points for chart widgets
        if widget.widget_type in ['chart_line', 'chart_bar']:
            if config.get('time_range') == '30d':
                config['group_by'] = 'day'  # Avoid too many data points
            elif config.get('time_range') == '7d':
                config['group_by'] = 'hour'
            else:
                config['group_by'] = 'minute'
        
        # 3. Set reasonable page sizes for table widgets
        if widget.widget_type == 'table':
            config['page_size'] = min(config.get('page_size', 50), 100)
        
        widget.configuration = config
        db_session.commit()
    
    @staticmethod
    def create_dashboard_layout_template() -> Dict[str, Any]:
        """Create optimized dashboard layout template."""
        
        return {
            'grid_columns': 12,
            'grid_rows': 20,
            'widget_spacing': 8,
            'auto_layout': False,
            'responsive_breakpoints': {
                'mobile': 768,
                'tablet': 1024,
                'desktop': 1440
            },
            'default_widget_sizes': {
                'metric_card': {'width': 3, 'height': 2},
                'chart_line': {'width': 6, 'height': 4},
                'chart_bar': {'width': 6, 'height': 4},
                'chart_pie': {'width': 4, 'height': 4},
                'table': {'width': 12, 'height': 6},
                'gauge': {'width': 3, 'height': 3},
                'heatmap': {'width': 8, 'height': 6}
            }
        }
    
    @staticmethod
    async def implement_caching_strategy(dashboard_service: DashboardService):
        """Implement intelligent caching for dashboard data."""
        
        import redis
        from datetime import timedelta
        
        redis_client = redis.Redis(decode_responses=True)
        
        # Cache strategy based on data type and refresh frequency
        cache_rules = {
            'real_time_metrics': timedelta(seconds=30),
            'hourly_aggregates': timedelta(minutes=15),
            'daily_aggregates': timedelta(hours=1),
            'static_data': timedelta(hours=24)
        }
        
        async def cached_get_widget_data(widget_id: int, filters: DashboardFilter):
            cache_key = f"widget_data:{widget_id}:{hash(str(filters.dict()))}"
            
            # Try to get from cache
            cached_data = redis_client.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
            
            # Get fresh data
            widget_data = await dashboard_service.get_widget_data(widget_id, filters)
            
            # Determine cache TTL based on data type
            if widget_data.refresh_interval <= 60:
                ttl = cache_rules['real_time_metrics']
            elif widget_data.refresh_interval <= 3600:
                ttl = cache_rules['hourly_aggregates']
            else:
                ttl = cache_rules['daily_aggregates']
            
            # Cache the data
            redis_client.setex(
                cache_key,
                int(ttl.total_seconds()),
                json.dumps(widget_data.dict(), default=str)
            )
            
            return widget_data
        
        return cached_get_widget_data
    
    @staticmethod
    def setup_alert_escalation_rules() -> List[Dict[str, Any]]:
        """Define alert escalation rules for different severity levels."""
        
        return [
            {
                'severity': 'critical',
                'escalation_delay_minutes': 5,
                'notification_channels': ['email', 'sms', 'slack'],
                'auto_escalate': True,
                'escalation_levels': [
                    {'level': 1, 'recipients': ['ops-team@company.com']},
                    {'level': 2, 'recipients': ['managers@company.com']},
                    {'level': 3, 'recipients': ['executives@company.com']}
                ]
            },
            {
                'severity': 'high',
                'escalation_delay_minutes': 15,
                'notification_channels': ['email', 'slack'],
                'auto_escalate': True,
                'escalation_levels': [
                    {'level': 1, 'recipients': ['ops-team@company.com']},
                    {'level': 2, 'recipients': ['managers@company.com']}
                ]
            },
            {
                'severity': 'medium',
                'escalation_delay_minutes': 60,
                'notification_channels': ['email'],
                'auto_escalate': False,
                'escalation_levels': [
                    {'level': 1, 'recipients': ['ops-team@company.com']}
                ]
            },
            {
                'severity': 'low',
                'escalation_delay_minutes': 240,
                'notification_channels': ['email'],
                'auto_escalate': False,
                'escalation_levels': [
                    {'level': 1, 'recipients': ['ops-team@company.com']}
                ]
            }
        ]
    
    @staticmethod
    async def implement_data_retention_policy(db_session):
        """Implement data retention policies for analytics data."""
        
        from datetime import datetime, timedelta
        
        retention_policies = {
            'events': timedelta(days=90),  # Keep events for 90 days
            'user_sessions': timedelta(days=30),  # Keep sessions for 30 days
            'page_views': timedelta(days=60),  # Keep page views for 60 days
            'system_metrics': timedelta(days=7),  # Keep system metrics for 7 days
            'aggregated_metrics': timedelta(days=365)  # Keep aggregated data for 1 year
        }
        
        current_time = datetime.utcnow()
        
        # Clean up old events
        events_cutoff = current_time - retention_policies['events']
        db_session.execute(text("""
            DELETE FROM events 
            WHERE created_at < :cutoff_date
        """), {'cutoff_date': events_cutoff})
        
        # Clean up old user sessions
        sessions_cutoff = current_time - retention_policies['user_sessions']
        db_session.execute(text("""
            DELETE FROM user_sessions 
            WHERE started_at < :cutoff_date
        """), {'cutoff_date': sessions_cutoff})
        
        # Clean up old page views
        pageviews_cutoff = current_time - retention_policies['page_views']
        db_session.execute(text("""
            DELETE FROM page_views 
            WHERE viewed_at < :cutoff_date
        """), {'cutoff_date': pageviews_cutoff})
        
        # Clean up old system metrics
        metrics_cutoff = current_time - retention_policies['system_metrics']
        db_session.execute(text("""
            DELETE FROM system_metrics 
            WHERE timestamp < :cutoff_date
        """), {'cutoff_date': metrics_cutoff})
        
        db_session.commit()
        
        logger.info("Data retention policy applied successfully")


# Performance optimization utilities
class DashboardPerformanceOptimizer:
    """Utilities for optimizing dashboard performance."""
    
    @staticmethod
    def create_analytics_indexes(db_session):
        """Create optimized indexes for analytics queries."""
        
        index_statements = [
            # Events table indexes
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_category_created_at ON events(category, created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_user_id_created_at ON events(user_id, created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_metadata_gin ON events USING gin(metadata)",
            
            # Transactions table indexes  
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transactions_status_created_at ON transactions(status, created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transactions_type_created_at ON transactions(transaction_type, created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transactions_amount_created_at ON transactions(amount, created_at)",
            
            # User sessions indexes
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_sessions_active_started ON user_sessions(is_active, started_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_sessions_user_id_started ON user_sessions(user_id, started_at)",
            
            # Page views indexes
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_page_views_session_viewed ON page_views(session_id, viewed_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_page_views_page_type_viewed ON page_views(page_type, viewed_at)",
            
            # System metrics indexes
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_system_metrics_timestamp ON system_metrics(timestamp DESC)",
        ]
        
        for statement in index_statements:
            try:
                db_session.execute(text(statement))
                db_session.commit()
                logger.info(f"Created index: {statement}")
            except Exception as e:
                db_session.rollback()
                logger.warning(f"Index creation failed (may already exist): {e}")
    
    @staticmethod
    def create_materialized_views(db_session):
        """Create materialized views for commonly accessed analytics data."""
        
        materialized_view_statements = [
            # Daily transaction summary
            """
            CREATE MATERIALIZED VIEW IF NOT EXISTS daily_transaction_summary AS
            SELECT 
                date_trunc('day', created_at) as date,
                transaction_type,
                status,
                count(*) as transaction_count,
                sum(amount) as total_amount,
                avg(amount) as avg_amount,
                min(amount) as min_amount,
                max(amount) as max_amount,
                avg(processing_time_ms) as avg_processing_time
            FROM transactions
            WHERE created_at >= current_date - interval '90 days'
            GROUP BY date_trunc('day', created_at), transaction_type, status
            """,
            
            # Hourly user activity summary
            """
            CREATE MATERIALIZED VIEW IF NOT EXISTS hourly_user_activity AS
            SELECT 
                date_trunc('hour', created_at) as hour,
                category,
                count(*) as event_count,
                count(distinct user_id) as unique_users
            FROM events
            WHERE created_at >= current_date - interval '7 days'
            GROUP BY date_trunc('hour', created_at), category
            """,
            
            # User engagement metrics
            """
            CREATE MATERIALIZED VIEW IF NOT EXISTS user_engagement_metrics AS
            SELECT 
                user_id,
                count(distinct session_id) as session_count,
                sum(duration_minutes) as total_session_time,
                avg(duration_minutes) as avg_session_duration,
                count(distinct date_trunc('day', started_at)) as active_days,
                max(started_at) as last_activity
            FROM user_sessions
            WHERE started_at >= current_date - interval '30 days'
            GROUP BY user_id
            """
        ]
        
        for statement in materialized_view_statements:
            try:
                db_session.execute(text(statement))
                db_session.commit()
                logger.info("Created materialized view")
            except Exception as e:
                db_session.rollback()
                logger.warning(f"Materialized view creation failed: {e}")
        
        # Create refresh function
        refresh_function = """
        CREATE OR REPLACE FUNCTION refresh_analytics_views()
        RETURNS void AS $$
        BEGIN
            REFRESH MATERIALIZED VIEW CONCURRENTLY daily_transaction_summary;
            REFRESH MATERIALIZED VIEW CONCURRENTLY hourly_user_activity;
            REFRESH MATERIALIZED VIEW CONCURRENTLY user_engagement_metrics;
        END;
        $$ LANGUAGE plpgsql;
        """
        
        try:
            db_session.execute(text(refresh_function))
            db_session.commit()
            logger.info("Created materialized view refresh function")
        except Exception as e:
            logger.warning(f"Refresh function creation failed: {e}")


# Dashboard testing utilities
class DashboardTestSuite:
    """Testing utilities for dashboard functionality."""
    
    @staticmethod
    async def test_dashboard_performance(dashboard_id: int, db_session):
        """Test dashboard loading performance and identify bottlenecks."""
        
        import time
        
        dashboard_service = create_dashboard_service(db_session)
        
        # Test dashboard data loading
        start_time = time.time()
        dashboard_data = await dashboard_service.get_dashboard_data(dashboard_id)
        total_load_time = time.time() - start_time
        
        # Test individual widget performance
        widget_performance = []
        for widget_data in dashboard_data['widgets']:
            widget_id = widget_data.get('widget_id')
            
            widget_start = time.time()
            widget_result = await dashboard_service.get_widget_data(widget_id)
            widget_time = time.time() - widget_start
            
            widget_performance.append({
                'widget_id': widget_id,
                'widget_type': widget_result.widget_type,
                'load_time_ms': widget_time * 1000,
                'data_size_kb': len(str(widget_result.data)) / 1024
            })
        
        # Performance analysis
        performance_report = {
            'dashboard_id': dashboard_id,
            'total_load_time_ms': total_load_time * 1000,
            'widget_count': len(widget_performance),
            'avg_widget_load_time_ms': sum(w['load_time_ms'] for w in widget_performance) / len(widget_performance),
            'slowest_widget': max(widget_performance, key=lambda w: w['load_time_ms']),
            'largest_widget': max(widget_performance, key=lambda w: w['data_size_kb']),
            'widgets': widget_performance,
            'recommendations': []
        }
        
        # Generate recommendations
        if performance_report['total_load_time_ms'] > 5000:  # > 5 seconds
            performance_report['recommendations'].append(
                "Consider implementing widget-level caching"
            )
        
        if any(w['load_time_ms'] > 2000 for w in widget_performance):  # > 2 seconds
            performance_report['recommendations'].append(
                "Some widgets are loading slowly - consider optimizing queries"
            )
        
        if any(w['data_size_kb'] > 100 for w in widget_performance):  # > 100KB
            performance_report['recommendations'].append(
                "Some widgets return large datasets - consider pagination"
            )
        
        return performance_report
    
    @staticmethod
    async def validate_dashboard_configuration(dashboard: Dashboard, db_session):
        """Validate dashboard and widget configurations."""
        
        validation_results = {
            'dashboard_id': dashboard.id,
            'is_valid': True,
            'warnings': [],
            'errors': []
        }
        
        # Get dashboard widgets
        widgets = db_session.query(DashboardWidget).filter_by(
            dashboard_id=dashboard.id
        ).all()
        
        # Validate widget configurations
        for widget in widgets:
            config = widget.configuration or {}
            
            # Check refresh intervals
            refresh_interval = config.get('refresh_interval', 60)
            if refresh_interval < 30 and widget.widget_type in ['metric_card', 'gauge']:
                validation_results['warnings'].append(
                    f"Widget {widget.name}: Very frequent refresh interval ({refresh_interval}s)"
                )
            
            # Check metric names
            metric_name = config.get('metric_name')
            if not metric_name:
                validation_results['errors'].append(
                    f"Widget {widget.name}: Missing metric_name configuration"
                )
                validation_results['is_valid'] = False
            
            # Validate chart configurations
            if widget.widget_type.startswith('chart_'):
                time_range = config.get('time_range', '24h')
                group_by = config.get('group_by', 'hour')
                
                if time_range == '90d' and group_by == 'minute':
                    validation_results['warnings'].append(
                        f"Widget {widget.name}: Too many data points (90d with minute grouping)"
                    )
        
        # Validate layout
        if dashboard.layout_config:
            layout = dashboard.layout_config
            
            # Check for overlapping widgets
            widget_positions = []
            for widget in widgets:
                pos = {
                    'x': widget.position_x,
                    'y': widget.position_y,
                    'width': widget.configuration.get('width', 4),
                    'height': widget.configuration.get('height', 3)
                }
                widget_positions.append(pos)
            
            # Simple overlap detection
            for i, pos1 in enumerate(widget_positions):
                for j, pos2 in enumerate(widget_positions[i+1:], i+1):
                    if (pos1['x'] < pos2['x'] + pos2['width'] and
                        pos1['x'] + pos1['width'] > pos2['x'] and
                        pos1['y'] < pos2['y'] + pos2['height'] and
                        pos1['y'] + pos1['height'] > pos2['y']):
                        validation_results['warnings'].append(
                            f"Widgets may be overlapping at positions ({pos1['x']},{pos1['y']}) and ({pos2['x']},{pos2['y']})"
                        )
        
        return validation_results
```

This completes Section 6 - Real-time Analytics Dashboard System! The section now includes:

✅ **6.1**: Analytics Models and Schema Design
✅ **6.2**: Advanced Query Service for Analytics  
✅ **6.3**: Real-time Data Streaming and Event Processing
✅ **6.4**: Dashboard Management Service with Widget Operations
✅ **6.5**: Complete Dashboard API with WebSocket Integration
✅ **6.6**: Comprehensive Usage Examples and Best Practices

**Key Features Demonstrated:**

1. **Advanced Analytics Models**: Event tracking, metrics, user sessions, system monitoring
2. **Real-time Processing**: WebSocket streaming, Redis integration, live updates
3. **Dashboard Management**: Complete CRUD operations, layout management, sharing
4. **Widget System**: Multiple widget types (charts, gauges, tables, heatmaps)
5. **Performance Optimization**: Caching strategies, materialized views, indexing
6. **Production Patterns**: Alert systems, data retention, testing frameworks

The real-time analytics system showcases sophisticated SQLAlchemy usage including JSONB operations, window functions, materialized views, advanced aggregations, and integration with external services for a complete production-ready analytics platform.

Should I continue with the next major section of the comprehensive SQLAlchemy guide?


---

## Part 7: Testing, Deployment & Best Practices

This section demonstrates comprehensive testing strategies for SQLAlchemy applications, deployment patterns, performance optimization, and production best practices.

### 7.1 Comprehensive Testing Framework

Testing SQLAlchemy applications requires specialized approaches for database interactions, transaction management, and complex business logic validation.

#### 7.1.1 Test Infrastructure Setup

**tests/conftest.py** - Complete Test Configuration:
```python
"""
Comprehensive test configuration with database fixtures, factories, and utilities.
Provides isolated test environments with proper transaction handling.
"""

import pytest
import asyncio
from typing import Generator, AsyncGenerator
from unittest.mock import Mock

import pytest_asyncio
from sqlalchemy import create_engine, event
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.pool import StaticPool
from httpx import AsyncClient
from factory import Factory

from app.config import get_settings
from app.database import get_db, get_async_db
from app.main import app
from app.models.base import Base
from app.core.security import create_access_token
from tests.factories import UserFactory, PostFactory, TransactionFactory


# Test database configuration
TEST_DATABASE_URL = "postgresql://postgres:password@localhost:5432/sqlalchemy_test"
TEST_DATABASE_URL_ASYNC = "postgresql+asyncpg://postgres:password@localhost:5432/sqlalchemy_test"

# Test engines with isolation
test_engine = create_engine(
    TEST_DATABASE_URL,
    poolclass=StaticPool,
    connect_args={
        "check_same_thread": False,
        "isolation_level": "AUTOCOMMIT"
    },
    echo=False  # Set to True for SQL debugging
)

test_async_engine = create_async_engine(
    TEST_DATABASE_URL_ASYNC,
    poolclass=StaticPool,
    echo=False
)

# Test session factories
TestSessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=test_engine
)

TestAsyncSessionLocal = sessionmaker(
    test_async_engine,
    class_=AsyncSession,
    autocommit=False,
    autoflush=False
)


@pytest.fixture(scope="session")
def event_loop():
    """Create event loop for async tests."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
def setup_test_database():
    """Set up test database schema."""
    # Create all tables
    Base.metadata.create_all(bind=test_engine)
    yield
    # Drop all tables after tests
    Base.metadata.drop_all(bind=test_engine)


@pytest.fixture
def db_session(setup_test_database) -> Generator[Session, None, None]:
    """Create a test database session with transaction rollback."""
    connection = test_engine.connect()
    transaction = connection.begin()
    session = TestSessionLocal(bind=connection)
    
    # Configure session for testing
    session.begin()
    
    yield session
    
    # Rollback transaction and close connection
    session.rollback()
    session.close()
    transaction.rollback()
    connection.close()


@pytest_asyncio.fixture
async def async_db_session(setup_test_database) -> AsyncGenerator[AsyncSession, None]:
    """Create an async test database session with transaction rollback."""
    async with test_async_engine.connect() as connection:
        async with connection.begin() as transaction:
            session = TestAsyncSessionLocal(bind=connection)
            
            yield session
            
            await session.rollback()
            await session.close()
            await transaction.rollback()


@pytest.fixture
def client(db_session: Session) -> Generator[AsyncClient, None, None]:
    """Create test client with database session override."""
    def get_test_db():
        yield db_session
    
    app.dependency_overrides[get_db] = get_test_db
    
    with AsyncClient(app=app, base_url="http://test") as client:
        yield client
    
    app.dependency_overrides.clear()


@pytest_asyncio.fixture
async def async_client(async_db_session: AsyncSession) -> AsyncGenerator[AsyncClient, None]:
    """Create async test client with database session override."""
    async def get_test_async_db():
        yield async_db_session
    
    app.dependency_overrides[get_async_db] = get_test_async_db
    
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client
    
    app.dependency_overrides.clear()


class TestDataManager:
    """Utility class for managing test data creation and cleanup."""
    
    def __init__(self, session: Session):
        self.session = session
        self.created_objects = []
    
    def create_user(self, **kwargs):
        """Create a test user with default values."""
        user = UserFactory.create(session=self.session, **kwargs)
        self.created_objects.append(user)
        return user
    
    def create_post(self, author=None, **kwargs):
        """Create a test post with optional author."""
        if not author:
            author = self.create_user()
        
        post = PostFactory.create(
            session=self.session,
            author=author,
            **kwargs
        )
        self.created_objects.append(post)
        return post
    
    def create_transaction(self, user=None, **kwargs):
        """Create a test transaction with optional user."""
        if not user:
            user = self.create_user()
        
        transaction = TransactionFactory.create(
            session=self.session,
            user=user,
            **kwargs
        )
        self.created_objects.append(transaction)
        return transaction
    
    def cleanup(self):
        """Clean up all created test objects."""
        for obj in reversed(self.created_objects):
            self.session.delete(obj)
        self.session.commit()
        self.created_objects.clear()


@pytest.fixture
def test_data(db_session: Session) -> Generator[TestDataManager, None, None]:
    """Provide test data manager for easy test data creation."""
    manager = TestDataManager(db_session)
    yield manager
    manager.cleanup()


# Authentication fixtures
@pytest.fixture
def admin_user(test_data: TestDataManager):
    """Create admin user for testing."""
    return test_data.create_user(
        email="admin@test.com",
        role=UserRole.ADMIN,
        is_active=True,
        is_verified=True
    )


@pytest.fixture
def regular_user(test_data: TestDataManager):
    """Create regular user for testing."""
    return test_data.create_user(
        email="user@test.com",
        role=UserRole.USER,
        is_active=True,
        is_verified=True
    )


@pytest.fixture
def admin_token(admin_user):
    """Create admin access token."""
    return create_access_token(subject=admin_user.email)


@pytest.fixture
def user_token(regular_user):
    """Create user access token."""
    return create_access_token(subject=regular_user.email)


@pytest.fixture
def auth_headers_admin(admin_token):
    """Create admin authorization headers."""
    return {"Authorization": f"Bearer {admin_token}"}


@pytest.fixture
def auth_headers_user(user_token):
    """Create user authorization headers."""
    return {"Authorization": f"Bearer {user_token}"}


# Database event monitoring for test debugging
@pytest.fixture
def monitor_db_queries():
    """Monitor database queries during testing."""
    queries = []
    
    @event.listens_for(test_engine, "before_cursor_execute")
    def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        queries.append({
            'statement': statement,
            'parameters': parameters,
            'timestamp': datetime.utcnow()
        })
    
    yield queries
    
    # Remove event listener
    event.remove(test_engine, "before_cursor_execute", receive_before_cursor_execute)


# Performance testing utilities
@pytest.fixture
def performance_monitor():
    """Monitor performance metrics during tests."""
    import time
    import psutil
    
    class PerformanceMonitor:
        def __init__(self):
            self.start_time = None
            self.start_memory = None
            self.metrics = {}
        
        def start_monitoring(self):
            self.start_time = time.time()
            self.start_memory = psutil.Process().memory_info().rss
        
        def stop_monitoring(self):
            if self.start_time:
                self.metrics['execution_time'] = time.time() - self.start_time
                self.metrics['memory_delta'] = psutil.Process().memory_info().rss - self.start_memory
        
        def get_metrics(self):
            return self.metrics.copy()
    
    return PerformanceMonitor()


# Mock external services
@pytest.fixture
def mock_redis():
    """Mock Redis client for testing."""
    mock_redis = Mock()
    mock_redis.get.return_value = None
    mock_redis.set.return_value = True
    mock_redis.delete.return_value = True
    mock_redis.exists.return_value = False
    return mock_redis


@pytest.fixture
def mock_email_service():
    """Mock email service for testing."""
    mock_email = Mock()
    mock_email.send_email.return_value = True
    mock_email.send_verification_email.return_value = True
    mock_email.send_password_reset_email.return_value = True
    return mock_email


# Test database utilities
class DatabaseTestUtils:
    """Utilities for database testing operations."""
    
    @staticmethod
    def count_table_rows(session: Session, model_class) -> int:
        """Count rows in a table."""
        return session.query(model_class).count()
    
    @staticmethod
    def table_exists(session: Session, table_name: str) -> bool:
        """Check if a table exists."""
        result = session.execute(text("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_name = :table_name
            )
        """), {'table_name': table_name})
        return result.scalar()
    
    @staticmethod
    def column_exists(session: Session, table_name: str, column_name: str) -> bool:
        """Check if a column exists in a table."""
        result = session.execute(text("""
            SELECT EXISTS (
                SELECT FROM information_schema.columns 
                WHERE table_name = :table_name 
                AND column_name = :column_name
            )
        """), {'table_name': table_name, 'column_name': column_name})
        return result.scalar()
    
    @staticmethod
    def get_table_constraints(session: Session, table_name: str) -> List[str]:
        """Get all constraints for a table."""
        result = session.execute(text("""
            SELECT constraint_name 
            FROM information_schema.table_constraints 
            WHERE table_name = :table_name
        """), {'table_name': table_name})
        return [row[0] for row in result.fetchall()]
    
    @staticmethod
    def get_table_indexes(session: Session, table_name: str) -> List[str]:
        """Get all indexes for a table."""
        result = session.execute(text("""
            SELECT indexname 
            FROM pg_indexes 
            WHERE tablename = :table_name
        """), {'table_name': table_name})
        return [row[0] for row in result.fetchall()]


@pytest.fixture
def db_utils():
    """Provide database testing utilities."""
    return DatabaseTestUtils()
```

#### 7.1.2 Factory Classes for Test Data

**tests/factories.py** - Test Data Factories:
```python
"""
Factory classes for creating test data with realistic, consistent values.
Uses Factory Boy for flexible and maintainable test data generation.
"""

import factory
from factory.alchemy import SQLAlchemyModelFactory
from factory import Faker, SubFactory, LazyAttribute, Sequence
from datetime import datetime, timedelta
import random
from decimal import Decimal

from app.models.user import User, UserRole, UserStatus
from app.models.blog import Post, Comment, Category, Tag, PostStatus
from app.models.analytics import Event, UserSession, Transaction
from tests.conftest import TestSessionLocal


class BaseFactory(SQLAlchemyModelFactory):
    """Base factory with common configuration."""
    
    class Meta:
        abstract = True
        sqlalchemy_session = TestSessionLocal
        sqlalchemy_session_persistence = "commit"


class UserFactory(BaseFactory):
    """Factory for creating User instances."""
    
    class Meta:
        model = User
    
    # Basic user information
    email = Faker('email')
    username = Sequence(lambda n: f"user{n}")
    first_name = Faker('first_name')
    last_name = Faker('last_name')
    
    # Profile information
    bio = Faker('text', max_nb_chars=200)
    location = Faker('city')
    website = Faker('url')
    birth_date = Faker('date_of_birth', minimum_age=18, maximum_age=80)
    
    # Account settings
    role = UserRole.USER
    status = UserStatus.ACTIVE
    is_active = True
    is_verified = True
    email_notifications = True
    language = 'en'
    timezone = 'UTC'
    
    # Security
    password_hash = '$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewYfpTF3z9e.3.K.'  # "password"
    last_login = Faker('date_time_this_month')
    login_count = Faker('random_int', min=1, max=100)
    
    # Audit fields are automatically handled by BaseModel


class AdminUserFactory(UserFactory):
    """Factory for creating admin users."""
    
    role = UserRole.ADMIN
    email = Sequence(lambda n: f"admin{n}@test.com")
    username = Sequence(lambda n: f"admin{n}")


class AuthorUserFactory(UserFactory):
    """Factory for creating author users."""
    
    role = UserRole.AUTHOR
    email = Sequence(lambda n: f"author{n}@test.com")
    username = Sequence(lambda n: f"author{n}")


class CategoryFactory(BaseFactory):
    """Factory for creating Category instances."""
    
    class Meta:
        model = Category
    
    name = Faker('word')
    slug = LazyAttribute(lambda obj: obj.name.lower().replace(' ', '-'))
    description = Faker('text', max_nb_chars=300)
    color = Faker('hex_color')
    is_visible = True
    sort_order = Faker('random_int', min=1, max=100)
    
    # SEO fields
    meta_title = LazyAttribute(lambda obj: f"{obj.name} - Category")
    meta_description = Faker('text', max_nb_chars=160)


class TagFactory(BaseFactory):
    """Factory for creating Tag instances."""
    
    class Meta:
        model = Tag
    
    name = Faker('word')
    slug = LazyAttribute(lambda obj: obj.name.lower().replace(' ', '-'))
    description = Faker('text', max_nb_chars=200)
    color = Faker('hex_color')
    usage_count = Faker('random_int', min=0, max=1000)


class PostFactory(BaseFactory):
    """Factory for creating Post instances."""
    
    class Meta:
        model = Post
    
    title = Faker('sentence', nb_words=6)
    slug = LazyAttribute(lambda obj: obj.title.lower().replace(' ', '-').replace('.', ''))
    content = Faker('text', max_nb_chars=2000)
    excerpt = Faker('text', max_nb_chars=200)
    
    # Relationships
    author = SubFactory(AuthorUserFactory)
    category = SubFactory(CategoryFactory)
    
    # Post settings
    status = PostStatus.PUBLISHED
    is_featured = Faker('boolean', chance_of_getting_true=20)
    is_pinned = Faker('boolean', chance_of_getting_true=10)
    allow_comments = True
    
    # SEO and metadata
    meta_title = LazyAttribute(lambda obj: obj.title)
    meta_description = LazyAttribute(lambda obj: obj.excerpt)
    featured_image_url = Faker('image_url')
    
    # Publishing
    published_at = Faker('date_time_this_year')
    scheduled_at = None
    
    # Engagement (will be updated by triggers in real app)
    view_count = Faker('random_int', min=0, max=10000)
    like_count = Faker('random_int', min=0, max=500)
    comment_count = Faker('random_int', min=0, max=100)
    share_count = Faker('random_int', min=0, max=50)
    
    # Content metadata
    read_time_minutes = Faker('random_int', min=1, max=30)
    word_count = Faker('random_int', min=100, max=3000)


class DraftPostFactory(PostFactory):
    """Factory for creating draft posts."""
    
    status = PostStatus.DRAFT
    published_at = None
    view_count = 0
    like_count = 0
    comment_count = 0


class ScheduledPostFactory(PostFactory):
    """Factory for creating scheduled posts."""
    
    status = PostStatus.SCHEDULED
    published_at = None
    scheduled_at = Faker('future_datetime', end_date='+30d')


class CommentFactory(BaseFactory):
    """Factory for creating Comment instances."""
    
    class Meta:
        model = Comment
    
    content = Faker('text', max_nb_chars=500)
    
    # Relationships
    author = SubFactory(UserFactory)
    post = SubFactory(PostFactory)
    # parent = None  # For threaded comments
    
    # Moderation
    status = CommentStatus.APPROVED
    is_spam = False
    moderation_reason = None
    
    # Metadata
    author_ip = Faker('ipv4')
    user_agent = Faker('user_agent')
    
    # Engagement
    like_count = Faker('random_int', min=0, max=20)


class SpamCommentFactory(CommentFactory):
    """Factory for creating spam comments."""
    
    status = CommentStatus.SPAM
    is_spam = True
    moderation_reason = "Detected as spam by automated system"
    content = "This is spam content with suspicious links."


class EventFactory(BaseFactory):
    """Factory for creating Event instances."""
    
    class Meta:
        model = Event
    
    category = Faker('random_element', elements=['user_action', 'system', 'transaction', 'error'])
    action = Faker('random_element', elements=['login', 'logout', 'create_post', 'purchase', 'error'])
    
    # Relationships
    user = SubFactory(UserFactory)
    
    # Event data
    metadata = factory.LazyFunction(lambda: {
        'user_agent': Faker('user_agent').generate(),
        'ip_address': Faker('ipv4').generate(),
        'page_url': Faker('url').generate(),
        'referrer': Faker('url').generate()
    })
    
    # Additional tracking
    session_id = Faker('uuid4')
    request_id = Faker('uuid4')


class UserSessionFactory(BaseFactory):
    """Factory for creating UserSession instances."""
    
    class Meta:
        model = UserSession
    
    # Relationships
    user = SubFactory(UserFactory)
    
    # Session data
    session_id = Faker('uuid4')
    ip_address = Faker('ipv4')
    user_agent = Faker('user_agent')
    
    # Timing
    started_at = Faker('date_time_this_month')
    ended_at = LazyAttribute(
        lambda obj: obj.started_at + timedelta(
            minutes=random.randint(5, 180)
        ) if obj.started_at else None
    )
    duration_minutes = LazyAttribute(
        lambda obj: int((obj.ended_at - obj.started_at).total_seconds() / 60) 
        if obj.ended_at and obj.started_at else None
    )
    
    # Session state
    is_active = Faker('boolean', chance_of_getting_true=30)
    
    # Activity tracking
    page_views = Faker('random_int', min=1, max=50)
    actions_performed = Faker('random_int', min=0, max=20)


class TransactionFactory(BaseFactory):
    """Factory for creating Transaction instances."""
    
    class Meta:
        model = Transaction
    
    # Relationships
    user = SubFactory(UserFactory)
    
    # Transaction details
    transaction_type = Faker('random_element', elements=['purchase', 'refund', 'transfer', 'deposit'])
    amount = Faker('pydecimal', left_digits=4, right_digits=2, positive=True, min_value=1, max_value=9999)
    currency = 'USD'
    description = Faker('sentence', nb_words=8)
    
    # Status and processing
    status = Faker('random_element', elements=['completed', 'pending', 'failed', 'cancelled'])
    payment_method = Faker('random_element', elements=['credit_card', 'debit_card', 'paypal', 'bank_transfer'])
    
    # References
    reference_id = Faker('uuid4')
    external_id = Faker('uuid4')
    
    # Metadata
    metadata = factory.LazyFunction(lambda: {
        'gateway': random.choice(['stripe', 'paypal', 'square']),
        'ip_address': Faker('ipv4').generate(),
        'device_type': random.choice(['mobile', 'desktop', 'tablet'])
    })
    
    # Processing
    processed_at = Faker('date_time_this_month')
    processing_time_ms = Faker('random_int', min=100, max=5000)


class FailedTransactionFactory(TransactionFactory):
    """Factory for creating failed transactions."""
    
    status = 'failed'
    processed_at = Faker('date_time_this_month')
    processing_time_ms = Faker('random_int', min=50, max=1000)


class LargeTransactionFactory(TransactionFactory):
    """Factory for creating large transactions."""
    
    amount = Faker('pydecimal', left_digits=5, right_digits=2, positive=True, min_value=10000, max_value=99999)
    transaction_type = 'purchase'
    status = 'completed'


# Trait factories for common variations
class Factories:
    """Container for all factory classes with utility methods."""
    
    User = UserFactory
    AdminUser = AdminUserFactory
    AuthorUser = AuthorUserFactory
    Category = CategoryFactory
    Tag = TagFactory
    Post = PostFactory
    DraftPost = DraftPostFactory
    ScheduledPost = ScheduledPostFactory
    Comment = CommentFactory
    SpamComment = SpamCommentFactory
    Event = EventFactory
    UserSession = UserSessionFactory
    Transaction = TransactionFactory
    FailedTransaction = FailedTransactionFactory
    LargeTransaction = LargeTransactionFactory
    
    @classmethod
    def create_blog_with_content(cls, author=None, category_count=3, post_count=10, comment_count=20):
        """Create a complete blog with categories, posts, and comments."""
        if not author:
            author = cls.AuthorUser.create()
        
        # Create categories
        categories = cls.Category.create_batch(category_count)
        
        # Create posts across categories
        posts = []
        for i in range(post_count):
            category = random.choice(categories)
            post = cls.Post.create(author=author, category=category)
            posts.append(post)
        
        # Create comments on posts
        for i in range(comment_count):
            post = random.choice(posts)
            cls.Comment.create(post=post)
        
        return {
            'author': author,
            'categories': categories,
            'posts': posts
        }
    
    @classmethod
    def create_user_with_activity(cls, session_count=5, transaction_count=10, event_count=50):
        """Create a user with realistic activity data."""
        user = cls.User.create()
        
        # Create user sessions
        sessions = cls.UserSession.create_batch(session_count, user=user)
        
        # Create transactions
        transactions = cls.Transaction.create_batch(transaction_count, user=user)
        
        # Create events
        events = cls.Event.create_batch(event_count, user=user)
        
        return {
            'user': user,
            'sessions': sessions,
            'transactions': transactions,
            'events': events
        }
```

This completes the first part of Section 7.1 - the testing infrastructure and factory setup. Should I continue with Part 2 of Section 7.1 (Unit Tests for Models and CRUD operations)?


#### 7.1.3 Unit Tests for Models and Relationships

**tests/test_models.py** - Comprehensive Model Testing:
```python
"""
Comprehensive model testing covering validation, relationships, constraints, and business logic.
Tests all SQLAlchemy features including hybrid properties, events, and complex relationships.
"""

import pytest
from datetime import datetime, timedelta
from decimal import Decimal
from sqlalchemy.exc import IntegrityError, DataError
from sqlalchemy.orm import Session

from app.models.user import User, UserRole, UserStatus, UserSession
from app.models.blog import Post, Comment, Category, Tag, PostStatus
from app.models.analytics import Event, Transaction
from tests.factories import Factories


class TestUserModel:
    """Test User model functionality and relationships."""
    
    def test_user_creation_with_defaults(self, db_session: Session):
        """Test user creation with default values."""
        user = Factories.User.create(session=db_session)
        
        assert user.id is not None
        assert user.uuid is not None
        assert user.created_at is not None
        assert user.updated_at is not None
        assert user.role == UserRole.USER
        assert user.status == UserStatus.ACTIVE
        assert user.is_active is True
        assert user.login_count >= 0
        
        # Test computed properties
        assert user.full_name == f"{user.first_name} {user.last_name}"
        assert user.is_admin == (user.role == UserRole.ADMIN)
    
    def test_user_email_validation(self, db_session: Session):
        """Test email validation and uniqueness constraints."""
        # Valid email
        user1 = Factories.User.create(session=db_session, email="test@example.com")
        assert user1.email == "test@example.com"
        
        # Duplicate email should raise IntegrityError
        with pytest.raises(IntegrityError):
            Factories.User.create(session=db_session, email="test@example.com")
            db_session.commit()
    
    def test_username_constraints(self, db_session: Session):
        """Test username validation and constraints."""
        # Valid username
        user = Factories.User.create(session=db_session, username="validuser123")
        assert user.username == "validuser123"
        
        # Username too short (assuming minimum length validation)
        with pytest.raises((IntegrityError, DataError)):
            Factories.User.create(session=db_session, username="ab")
            db_session.commit()
        
        # Duplicate username
        with pytest.raises(IntegrityError):
            Factories.User.create(session=db_session, username="validuser123")
            db_session.commit()
    
    def test_user_password_hashing(self, db_session: Session):
        """Test password hashing functionality."""
        user = Factories.User.create(session=db_session)
        
        # Password should be hashed
        assert user.password_hash is not None
        assert user.password_hash != "password"
        assert len(user.password_hash) > 50  # bcrypt hash length
        
        # Test password verification (if implemented)
        if hasattr(user, 'verify_password'):
            assert user.verify_password("password") is True
            assert user.verify_password("wrongpassword") is False
    
    def test_user_role_permissions(self, db_session: Session):
        """Test role-based permission checks."""
        regular_user = Factories.User.create(session=db_session, role=UserRole.USER)
        admin_user = Factories.AdminUser.create(session=db_session)
        author_user = Factories.AuthorUser.create(session=db_session)
        
        # Test role properties
        assert regular_user.is_admin is False
        assert admin_user.is_admin is True
        assert author_user.role == UserRole.AUTHOR
        
        # Test permission methods (if implemented)
        if hasattr(regular_user, 'can_create_posts'):
            assert regular_user.can_create_posts() is False
            assert author_user.can_create_posts() is True
            assert admin_user.can_create_posts() is True
    
    def test_user_relationships(self, db_session: Session):
        """Test user relationships with other models."""
        user = Factories.User.create(session=db_session)
        
        # Create related objects
        posts = Factories.Post.create_batch(3, session=db_session, author=user)
        comments = Factories.Comment.create_batch(5, session=db_session, author=user)
        sessions = Factories.UserSession.create_batch(2, session=db_session, user=user)
        
        db_session.refresh(user)
        
        # Test relationships
        assert len(user.posts) == 3
        assert len(user.comments) == 5
        assert len(user.sessions) == 2
        
        # Test relationship properties
        assert all(post.author_id == user.id for post in posts)
        assert all(comment.author_id == user.id for comment in comments)
    
    def test_user_soft_delete(self, db_session: Session):
        """Test soft delete functionality."""
        user = Factories.User.create(session=db_session)
        user_id = user.id
        
        # Soft delete
        user.is_deleted = True
        user.deleted_at = datetime.utcnow()
        db_session.commit()
        
        # User should still exist in database
        deleted_user = db_session.query(User).filter(User.id == user_id).first()
        assert deleted_user is not None
        assert deleted_user.is_deleted is True
        assert deleted_user.deleted_at is not None
        
        # Active users query should exclude deleted users
        active_users = db_session.query(User).filter(User.is_deleted == False).all()
        assert user not in active_users
    
    def test_user_audit_fields(self, db_session: Session):
        """Test automatic audit field population."""
        user = Factories.User.create(session=db_session)
        original_created_at = user.created_at
        original_updated_at = user.updated_at
        
        # Update user
        user.first_name = "Updated"
        db_session.commit()
        db_session.refresh(user)
        
        # created_at should remain unchanged
        assert user.created_at == original_created_at
        
        # updated_at should be newer
        assert user.updated_at > original_updated_at
    
    def test_user_statistics(self, db_session: Session):
        """Test user statistics calculations."""
        user = Factories.User.create(session=db_session)
        
        # Create activity data
        published_posts = Factories.Post.create_batch(
            3, session=db_session, author=user, status=PostStatus.PUBLISHED
        )
        draft_posts = Factories.DraftPost.create_batch(
            2, session=db_session, author=user
        )
        
        db_session.refresh(user)
        
        # Test computed statistics (if implemented)
        if hasattr(user, 'published_posts_count'):
            assert user.published_posts_count == 3
        
        if hasattr(user, 'total_posts_count'):
            assert user.total_posts_count == 5


class TestPostModel:
    """Test Post model functionality and relationships."""
    
    def test_post_creation_with_required_fields(self, db_session: Session):
        """Test post creation with required fields."""
        author = Factories.AuthorUser.create(session=db_session)
        category = Factories.Category.create(session=db_session)
        
        post = Factories.Post.create(
            session=db_session,
            author=author,
            category=category,
            title="Test Post",
            content="This is test content."
        )
        
        assert post.id is not None
        assert post.title == "Test Post"
        assert post.author_id == author.id
        assert post.category_id == category.id
        assert post.slug is not None
        assert post.created_at is not None
    
    def test_post_slug_generation(self, db_session: Session):
        """Test automatic slug generation from title."""
        post = Factories.Post.create(
            session=db_session,
            title="This is a Test Post Title!"
        )
        
        # Slug should be generated from title
        expected_slug = "this-is-a-test-post-title"
        assert post.slug.lower().replace('-', '') == expected_slug.replace('-', '')
    
    def test_post_status_transitions(self, db_session: Session):
        """Test post status transitions and validation."""
        post = Factories.DraftPost.create(session=db_session)
        
        # Draft to published
        post.status = PostStatus.PUBLISHED
        post.published_at = datetime.utcnow()
        db_session.commit()
        
        assert post.status == PostStatus.PUBLISHED
        assert post.published_at is not None
        
        # Published post should not be able to go back to draft
        # (if business logic is implemented)
        if hasattr(post, 'can_change_status'):
            assert post.can_change_status(PostStatus.DRAFT) is False
    
    def test_post_scheduling(self, db_session: Session):
        """Test post scheduling functionality."""
        future_date = datetime.utcnow() + timedelta(days=1)
        
        post = Factories.ScheduledPost.create(
            session=db_session,
            scheduled_at=future_date
        )
        
        assert post.status == PostStatus.SCHEDULED
        assert post.scheduled_at == future_date
        assert post.published_at is None
        
        # Test scheduled post queries (if implemented)
        if hasattr(Post, 'get_scheduled_for_publication'):
            scheduled_posts = Post.get_scheduled_for_publication(db_session)
            assert post in scheduled_posts
    
    def test_post_tags_relationship(self, db_session: Session):
        """Test many-to-many relationship with tags."""
        post = Factories.Post.create(session=db_session)
        tags = Factories.Tag.create_batch(3, session=db_session)
        
        # Add tags to post
        post.tags.extend(tags)
        db_session.commit()
        db_session.refresh(post)
        
        # Test relationship
        assert len(post.tags) == 3
        assert all(tag in post.tags for tag in tags)
        
        # Test reverse relationship
        for tag in tags:
            db_session.refresh(tag)
            assert post in tag.posts
    
    def test_post_comments_relationship(self, db_session: Session):
        """Test one-to-many relationship with comments."""
        post = Factories.Post.create(session=db_session)
        comments = Factories.Comment.create_batch(5, session=db_session, post=post)
        
        db_session.refresh(post)
        
        # Test relationship
        assert len(post.comments) == 5
        assert all(comment.post_id == post.id for comment in comments)
        
        # Test comment ordering (if implemented)
        if post.comments:
            comment_dates = [comment.created_at for comment in post.comments]
            assert comment_dates == sorted(comment_dates)
    
    def test_post_engagement_metrics(self, db_session: Session):
        """Test engagement metrics calculation."""
        post = Factories.Post.create(
            session=db_session,
            view_count=100,
            like_count=10,
            comment_count=5
        )
        
        # Test engagement calculations (if implemented)
        if hasattr(post, 'engagement_rate'):
            expected_rate = (post.like_count + post.comment_count) / max(post.view_count, 1)
            assert abs(post.engagement_rate - expected_rate) < 0.01
    
    def test_post_seo_fields(self, db_session: Session):
        """Test SEO-related fields and validation."""
        post = Factories.Post.create(
            session=db_session,
            meta_title="SEO Title",
            meta_description="SEO Description"
        )
        
        assert post.meta_title == "SEO Title"
        assert post.meta_description == "SEO Description"
        
        # Test meta description length validation
        if hasattr(post, 'validate_meta_description'):
            long_description = "x" * 200  # Assuming 160 char limit
            with pytest.raises(ValueError):
                post.meta_description = long_description
                post.validate_meta_description()


class TestCommentModel:
    """Test Comment model functionality and threading."""
    
    def test_comment_creation(self, db_session: Session):
        """Test basic comment creation."""
        post = Factories.Post.create(session=db_session)
        author = Factories.User.create(session=db_session)
        
        comment = Factories.Comment.create(
            session=db_session,
            post=post,
            author=author,
            content="This is a test comment."
        )
        
        assert comment.id is not None
        assert comment.post_id == post.id
        assert comment.author_id == author.id
        assert comment.content == "This is a test comment."
        assert comment.status == CommentStatus.APPROVED
    
    def test_comment_threading(self, db_session: Session):
        """Test threaded comment functionality."""
        post = Factories.Post.create(session=db_session)
        
        # Create parent comment
        parent_comment = Factories.Comment.create(
            session=db_session,
            post=post,
            content="Parent comment"
        )
        
        # Create child comments
        child_comment1 = Factories.Comment.create(
            session=db_session,
            post=post,
            parent=parent_comment,
            content="Child comment 1"
        )
        
        child_comment2 = Factories.Comment.create(
            session=db_session,
            post=post,
            parent=parent_comment,
            content="Child comment 2"
        )
        
        db_session.refresh(parent_comment)
        
        # Test relationships
        assert len(parent_comment.children) == 2
        assert child_comment1.parent_id == parent_comment.id
        assert child_comment2.parent_id == parent_comment.id
        assert child_comment1 in parent_comment.children
        assert child_comment2 in parent_comment.children
    
    def test_comment_moderation(self, db_session: Session):
        """Test comment moderation functionality."""
        comment = Factories.Comment.create(session=db_session)
        
        # Moderate comment
        comment.status = CommentStatus.SPAM
        comment.moderation_reason = "Contains spam content"
        comment.moderated_at = datetime.utcnow()
        db_session.commit()
        
        assert comment.status == CommentStatus.SPAM
        assert comment.moderation_reason == "Contains spam content"
        assert comment.moderated_at is not None
        
        # Test moderation queries (if implemented)
        if hasattr(Comment, 'get_pending_moderation'):
            pending_comments = Comment.get_pending_moderation(db_session)
            assert comment not in pending_comments
    
    def test_comment_spam_detection(self, db_session: Session):
        """Test automated spam detection."""
        spam_comment = Factories.SpamComment.create(session=db_session)
        
        assert spam_comment.is_spam is True
        assert spam_comment.status == CommentStatus.SPAM
        assert spam_comment.moderation_reason is not None


class TestCategoryModel:
    """Test Category model and hierarchical structure."""
    
    def test_category_creation(self, db_session: Session):
        """Test category creation with basic fields."""
        category = Factories.Category.create(
            session=db_session,
            name="Technology",
            description="Technology related posts"
        )
        
        assert category.name == "Technology"
        assert category.slug is not None
        assert category.description == "Technology related posts"
        assert category.is_visible is True
    
    def test_category_hierarchy(self, db_session: Session):
        """Test hierarchical category structure."""
        parent_category = Factories.Category.create(
            session=db_session,
            name="Technology"
        )
        
        child_category = Factories.Category.create(
            session=db_session,
            name="Web Development",
            parent=parent_category
        )
        
        db_session.refresh(parent_category)
        
        # Test relationships
        assert child_category.parent_id == parent_category.id
        assert child_category in parent_category.children
        assert len(parent_category.children) == 1
    
    def test_category_post_count(self, db_session: Session):
        """Test category post count calculation."""
        category = Factories.Category.create(session=db_session)
        posts = Factories.Post.create_batch(3, session=db_session, category=category)
        
        db_session.refresh(category)
        
        # Test post count (if implemented)
        if hasattr(category, 'post_count'):
            assert category.post_count == 3


class TestTransactionModel:
    """Test Transaction model and financial calculations."""
    
    def test_transaction_creation(self, db_session: Session):
        """Test transaction creation with validation."""
        user = Factories.User.create(session=db_session)
        
        transaction = Factories.Transaction.create(
            session=db_session,
            user=user,
            amount=Decimal('99.99'),
            currency='USD',
            transaction_type='purchase'
        )
        
        assert transaction.user_id == user.id
        assert transaction.amount == Decimal('99.99')
        assert transaction.currency == 'USD'
        assert transaction.reference_id is not None
    
    def test_transaction_amount_validation(self, db_session: Session):
        """Test transaction amount validation."""
        # Negative amount should not be allowed for purchases
        with pytest.raises((IntegrityError, ValueError)):
            Factories.Transaction.create(
                session=db_session,
                amount=Decimal('-10.00'),
                transaction_type='purchase'
            )
            db_session.commit()
    
    def test_transaction_status_tracking(self, db_session: Session):
        """Test transaction status tracking."""
        transaction = Factories.Transaction.create(
            session=db_session,
            status='pending'
        )
        
        # Update status
        transaction.status = 'completed'
        transaction.processed_at = datetime.utcnow()
        db_session.commit()
        
        assert transaction.status == 'completed'
        assert transaction.processed_at is not None
    
    def test_transaction_metadata(self, db_session: Session):
        """Test transaction metadata handling."""
        metadata = {
            'payment_gateway': 'stripe',
            'gateway_transaction_id': 'tx_123456789',
            'customer_ip': '192.168.1.1'
        }
        
        transaction = Factories.Transaction.create(
            session=db_session,
            metadata=metadata
        )
        
        assert transaction.metadata == metadata
        assert transaction.metadata['payment_gateway'] == 'stripe'


class TestEventModel:
    """Test Event model for analytics tracking."""
    
    def test_event_creation(self, db_session: Session):
        """Test event creation and categorization."""
        user = Factories.User.create(session=db_session)
        
        event = Factories.Event.create(
            session=db_session,
            user=user,
            category='user_action',
            action='login'
        )
        
        assert event.user_id == user.id
        assert event.category == 'user_action'
        assert event.action == 'login'
        assert event.created_at is not None
    
    def test_event_metadata_tracking(self, db_session: Session):
        """Test event metadata for detailed tracking."""
        metadata = {
            'user_agent': 'Mozilla/5.0...',
            'ip_address': '192.168.1.1',
            'page_url': '/dashboard',
            'referrer': 'https://google.com'
        }
        
        event = Factories.Event.create(
            session=db_session,
            metadata=metadata
        )
        
        assert event.metadata == metadata
        assert event.metadata['page_url'] == '/dashboard'
    
    def test_event_session_tracking(self, db_session: Session):
        """Test event session association."""
        user = Factories.User.create(session=db_session)
        session = Factories.UserSession.create(session=db_session, user=user)
        
        event = Factories.Event.create(
            session=db_session,
            user=user,
            session_id=session.session_id
        )
        
        assert event.session_id == session.session_id
```

Now let me continue with Part 3 of Section 7.1 - CRUD and API testing:

<function_calls>
<invoke name="run_in_terminal">
<parameter name="command">cat >> /Users/prashant/Workspaces/CopilotWorkspaces/comprehensive-sqlalchemy-guide.md << 'EOF'

#### 7.1.4 CRUD Operations Testing

**tests/test_crud.py** - CRUD Operations Testing:
```python
"""
Comprehensive CRUD operations testing covering all database operations,
error handling, pagination, filtering, and performance characteristics.
"""

import pytest
from datetime import datetime, timedelta
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import Session

from app.crud.user import user_crud
from app.crud.blog import post_crud, comment_crud, category_crud
from app.crud.session import session_crud
from app.models.user import User, UserRole, UserStatus
from app.models.blog import Post, PostStatus, Comment, CommentStatus
from app.schemas.user import UserCreate, UserUpdate, UserFilterParams
from app.schemas.blog import PostCreate, PostUpdate, PostFilterParams
from app.schemas.common import PaginationParams
from tests.factories import Factories


class TestUserCRUD:
    """Test User CRUD operations."""
    
    def test_create_user(self, db_session: Session):
        """Test user creation through CRUD."""
        user_data = UserCreate(
            email="test@example.com",
            username="testuser",
            first_name="Test",
            last_name="User",
            password="securepassword123"
        )
        
        user = user_crud.create(db=db_session, obj_in=user_data)
        
        assert user.email == "test@example.com"
        assert user.username == "testuser"
        assert user.first_name == "Test"
        assert user.last_name == "User"
        assert user.password_hash != "securepassword123"  # Should be hashed
        assert user.role == UserRole.USER  # Default role
        assert user.is_active is True
    
    def test_get_user_by_id(self, db_session: Session):
        """Test retrieving user by ID."""
        user = Factories.User.create(session=db_session)
        
        retrieved_user = user_crud.get(db=db_session, id=user.id)
        
        assert retrieved_user is not None
        assert retrieved_user.id == user.id
        assert retrieved_user.email == user.email
    
    def test_get_user_by_email(self, db_session: Session):
        """Test retrieving user by email."""
        user = Factories.User.create(session=db_session, email="unique@test.com")
        
        retrieved_user = user_crud.get_by_email(db=db_session, email="unique@test.com")
        
        assert retrieved_user is not None
        assert retrieved_user.email == "unique@test.com"
        assert retrieved_user.id == user.id
    
    def test_get_nonexistent_user(self, db_session: Session):
        """Test retrieving non-existent user returns None."""
        user = user_crud.get(db=db_session, id=99999)
        assert user is None
        
        user_by_email = user_crud.get_by_email(db=db_session, email="nonexistent@test.com")
        assert user_by_email is None
    
    def test_update_user(self, db_session: Session):
        """Test user update operations."""
        user = Factories.User.create(session=db_session)
        original_email = user.email
        
        update_data = UserUpdate(
            first_name="Updated",
            last_name="Name",
            bio="Updated bio"
        )
        
        updated_user = user_crud.update(db=db_session, db_obj=user, obj_in=update_data)
        
        assert updated_user.first_name == "Updated"
        assert updated_user.last_name == "Name"
        assert updated_user.bio == "Updated bio"
        assert updated_user.email == original_email  # Unchanged
        assert updated_user.updated_at > user.created_at
    
    def test_soft_delete_user(self, db_session: Session):
        """Test soft delete functionality."""
        user = Factories.User.create(session=db_session)
        user_id = user.id
        
        # Soft delete
        deleted_user = user_crud.remove(db=db_session, id=user_id)
        
        assert deleted_user.is_deleted is True
        assert deleted_user.deleted_at is not None
        
        # User should not be found in regular queries
        retrieved_user = user_crud.get(db=db_session, id=user_id)
        assert retrieved_user is None
        
        # But should be found with include_deleted=True
        if hasattr(user_crud, 'get_with_deleted'):
            deleted_retrieved = user_crud.get_with_deleted(db=db_session, id=user_id)
            assert deleted_retrieved is not None
            assert deleted_retrieved.is_deleted is True
    
    def test_get_users_with_pagination(self, db_session: Session):
        """Test paginated user retrieval."""
        # Create multiple users
        users = Factories.User.create_batch(15, session=db_session)
        
        # Get first page
        pagination = PaginationParams(page=1, size=10)
        result = user_crud.get_multi_with_pagination(db=db_session, pagination=pagination)
        
        assert len(result.items) == 10
        assert result.total >= 15
        assert result.page == 1
        assert result.size == 10
        assert result.pages >= 2
        
        # Get second page
        pagination = PaginationParams(page=2, size=10)
        result = user_crud.get_multi_with_pagination(db=db_session, pagination=pagination)
        
        assert len(result.items) >= 5  # At least 5 remaining users
        assert result.page == 2
    
    def test_user_filtering(self, db_session: Session):
        """Test user filtering by various criteria."""
        # Create users with different attributes
        active_user = Factories.User.create(
            session=db_session,
            is_active=True,
            role=UserRole.USER,
            created_at=datetime.utcnow() - timedelta(days=5)
        )
        
        admin_user = Factories.AdminUser.create(
            session=db_session,
            is_active=True,
            created_at=datetime.utcnow() - timedelta(days=10)
        )
        
        inactive_user = Factories.User.create(
            session=db_session,
            is_active=False,
            created_at=datetime.utcnow() - timedelta(days=15)
        )
        
        # Filter by role
        filters = UserFilterParams(role=UserRole.ADMIN)
        admin_users = user_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(admin_users) >= 1
        assert all(user.role == UserRole.ADMIN for user in admin_users)
        
        # Filter by active status
        filters = UserFilterParams(is_active=True)
        active_users = user_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(active_users) >= 2
        assert all(user.is_active is True for user in active_users)
        
        # Filter by date range
        week_ago = datetime.utcnow() - timedelta(days=7)
        filters = UserFilterParams(created_after=week_ago)
        recent_users = user_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(recent_users) >= 1
        assert all(user.created_at >= week_ago for user in recent_users)
    
    def test_user_search(self, db_session: Session):
        """Test user search functionality."""
        # Create users with searchable content
        user1 = Factories.User.create(
            session=db_session,
            first_name="John",
            last_name="Doe",
            username="johndoe",
            email="john@example.com"
        )
        
        user2 = Factories.User.create(
            session=db_session,
            first_name="Jane",
            last_name="Smith",
            username="janesmith",
            email="jane@example.com"
        )
        
        # Search by first name
        results = user_crud.search(db=db_session, query="John")
        assert len(results) >= 1
        assert any(user.first_name == "John" for user in results)
        
        # Search by email
        results = user_crud.search(db=db_session, query="jane@example.com")
        assert len(results) >= 1
        assert any(user.email == "jane@example.com" for user in results)
        
        # Search with no results
        results = user_crud.search(db=db_session, query="nonexistent")
        assert len(results) == 0
    
    def test_user_authentication_methods(self, db_session: Session):
        """Test authentication-related CRUD methods."""
        user = Factories.User.create(
            session=db_session,
            email="auth@test.com",
            password_hash="hashed_password"
        )
        
        # Test authentication
        authenticated_user = user_crud.authenticate(
            db=db_session,
            email="auth@test.com",
            password="password"  # Original password
        )
        
        # Should return user if password verification works
        # (This depends on your password verification implementation)
        if authenticated_user:
            assert authenticated_user.email == "auth@test.com"
        
        # Test failed authentication
        failed_auth = user_crud.authenticate(
            db=db_session,
            email="auth@test.com",
            password="wrongpassword"
        )
        assert failed_auth is None
    
    def test_user_statistics(self, db_session: Session):
        """Test user statistics calculations."""
        # Create users with different statuses and dates
        Factories.User.create_batch(5, session=db_session, is_active=True)
        Factories.User.create_batch(2, session=db_session, is_active=False)
        
        stats = user_crud.get_statistics(db=db_session)
        
        assert stats['total_users'] >= 7
        assert stats['active_users'] >= 5
        assert stats['inactive_users'] >= 2
        assert 'recent_registrations' in stats


class TestPostCRUD:
    """Test Post CRUD operations."""
    
    def test_create_post(self, db_session: Session):
        """Test post creation through CRUD."""
        author = Factories.AuthorUser.create(session=db_session)
        category = Factories.Category.create(session=db_session)
        
        post_data = PostCreate(
            title="Test Post",
            content="This is test content for the post.",
            category_id=category.id,
            status=PostStatus.DRAFT
        )
        
        post = post_crud.create_with_owner(
            db=db_session,
            obj_in=post_data,
            owner_id=author.id
        )
        
        assert post.title == "Test Post"
        assert post.content == "This is test content for the post."
        assert post.author_id == author.id
        assert post.category_id == category.id
        assert post.status == PostStatus.DRAFT
        assert post.slug is not None
    
    def test_get_post_by_slug(self, db_session: Session):
        """Test retrieving post by slug."""
        post = Factories.Post.create(
            session=db_session,
            title="Unique Post Title",
            slug="unique-post-title"
        )
        
        retrieved_post = post_crud.get_by_slug(db=db_session, slug="unique-post-title")
        
        assert retrieved_post is not None
        assert retrieved_post.id == post.id
        assert retrieved_post.slug == "unique-post-title"
    
    def test_publish_post(self, db_session: Session):
        """Test post publishing workflow."""
        draft_post = Factories.DraftPost.create(session=db_session)
        
        published_post = post_crud.publish(db=db_session, post_id=draft_post.id)
        
        assert published_post.status == PostStatus.PUBLISHED
        assert published_post.published_at is not None
        assert published_post.published_at <= datetime.utcnow()
    
    def test_post_filtering_and_search(self, db_session: Session):
        """Test post filtering and search capabilities."""
        category = Factories.Category.create(session=db_session, name="Technology")
        author = Factories.AuthorUser.create(session=db_session)
        
        # Create posts with different attributes
        published_post = Factories.Post.create(
            session=db_session,
            author=author,
            category=category,
            status=PostStatus.PUBLISHED,
            title="Published Tech Post",
            is_featured=True
        )
        
        draft_post = Factories.DraftPost.create(
            session=db_session,
            author=author,
            category=category,
            title="Draft Tech Post"
        )
        
        # Filter by status
        filters = PostFilterParams(status=PostStatus.PUBLISHED)
        published_posts = post_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(published_posts) >= 1
        assert all(post.status == PostStatus.PUBLISHED for post in published_posts)
        
        # Filter by category
        filters = PostFilterParams(category_id=category.id)
        category_posts = post_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(category_posts) >= 2
        assert all(post.category_id == category.id for post in category_posts)
        
        # Filter by author
        filters = PostFilterParams(author_id=author.id)
        author_posts = post_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(author_posts) >= 2
        assert all(post.author_id == author.id for post in author_posts)
        
        # Filter featured posts
        filters = PostFilterParams(is_featured=True)
        featured_posts = post_crud.get_multi_filtered(db=db_session, filters=filters)
        assert len(featured_posts) >= 1
        assert all(post.is_featured is True for post in featured_posts)
        
        # Search by title
        search_results = post_crud.search(db=db_session, query="Tech Post")
        assert len(search_results) >= 2
        assert all("Tech Post" in post.title for post in search_results)
    
    def test_post_engagement_updates(self, db_session: Session):
        """Test post engagement metric updates."""
        post = Factories.Post.create(
            session=db_session,
            view_count=10,
            like_count=5
        )
        
        # Update view count
        updated_post = post_crud.increment_view_count(db=db_session, post_id=post.id)
        assert updated_post.view_count == 11
        
        # Update like count
        updated_post = post_crud.increment_like_count(db=db_session, post_id=post.id)
        assert updated_post.like_count == 6
    
    def test_post_scheduling(self, db_session: Session):
        """Test post scheduling functionality."""
        future_date = datetime.utcnow() + timedelta(hours=24)
        
        post = Factories.Post.create(session=db_session, status=PostStatus.DRAFT)
        
        scheduled_post = post_crud.schedule_post(
            db=db_session,
            post_id=post.id,
            scheduled_at=future_date
        )
        
        assert scheduled_post.status == PostStatus.SCHEDULED
        assert scheduled_post.scheduled_at == future_date
        
        # Get posts ready for publication
        ready_posts = post_crud.get_ready_for_publication(db=db_session)
        assert scheduled_post not in ready_posts  # Not ready yet
        
        # Test with past date
        past_date = datetime.utcnow() - timedelta(hours=1)
        past_scheduled = post_crud.schedule_post(
            db=db_session,
            post_id=post.id,
            scheduled_at=past_date
        )
        
        ready_posts = post_crud.get_ready_for_publication(db=db_session)
        assert past_scheduled in ready_posts  # Ready for publication


class TestCommentCRUD:
    """Test Comment CRUD operations."""
    
    def test_create_comment(self, db_session: Session):
        """Test comment creation."""
        post = Factories.Post.create(session=db_session)
        author = Factories.User.create(session=db_session)
        
        comment_data = CommentCreate(
            content="This is a test comment.",
            post_id=post.id
        )
        
        comment = comment_crud.create_with_owner(
            db=db_session,
            obj_in=comment_data,
            owner_id=author.id
        )
        
        assert comment.content == "This is a test comment."
        assert comment.post_id == post.id
        assert comment.author_id == author.id
        assert comment.status == CommentStatus.APPROVED
    
    def test_create_threaded_comment(self, db_session: Session):
        """Test creating threaded (reply) comments."""
        post = Factories.Post.create(session=db_session)
        parent_comment = Factories.Comment.create(session=db_session, post=post)
        author = Factories.User.create(session=db_session)
        
        reply_data = CommentCreate(
            content="This is a reply.",
            post_id=post.id,
            parent_id=parent_comment.id
        )
        
        reply = comment_crud.create_with_owner(
            db=db_session,
            obj_in=reply_data,
            owner_id=author.id
        )
        
        assert reply.parent_id == parent_comment.id
        assert reply.post_id == post.id
        
        # Check parent-child relationship
        db_session.refresh(parent_comment)
        assert reply in parent_comment.children
    
    def test_moderate_comment(self, db_session: Session):
        """Test comment moderation functionality."""
        comment = Factories.Comment.create(session=db_session)
        
        moderated_comment = comment_crud.moderate_comment(
            db=db_session,
            comment_id=comment.id,
            status=CommentStatus.SPAM,
            reason="Contains inappropriate content"
        )
        
        assert moderated_comment.status == CommentStatus.SPAM
        assert moderated_comment.moderation_reason == "Contains inappropriate content"
        assert moderated_comment.moderated_at is not None
    
    def test_get_comments_for_post(self, db_session: Session):
        """Test retrieving comments for a specific post."""
        post = Factories.Post.create(session=db_session)
        other_post = Factories.Post.create(session=db_session)
        
        # Create comments for the post
        post_comments = Factories.Comment.create_batch(
            5, session=db_session, post=post
        )
        
        # Create comments for other post
        Factories.Comment.create_batch(3, session=db_session, post=other_post)
        
        # Get comments for specific post
        retrieved_comments = comment_crud.get_by_post(
            db=db_session,
            post_id=post.id
        )
        
        assert len(retrieved_comments) == 5
        assert all(comment.post_id == post.id for comment in retrieved_comments)
    
    def test_get_comments_for_moderation(self, db_session: Session):
        """Test retrieving comments that need moderation."""
        # Create comments with different statuses
        Factories.Comment.create_batch(
            3, session=db_session, status=CommentStatus.PENDING
        )
        Factories.Comment.create_batch(
            2, session=db_session, status=CommentStatus.APPROVED
        )
        Factories.SpamComment.create_batch(
            1, session=db_session
        )
        
        pending_comments = comment_crud.get_pending_moderation(db=db_session)
        
        assert len(pending_comments) >= 3
        assert all(comment.status == CommentStatus.PENDING for comment in pending_comments)


class TestCRUDPerformance:
    """Test CRUD operation performance characteristics."""
    
    def test_bulk_operations_performance(self, db_session: Session, performance_monitor):
        """Test performance of bulk operations."""
        performance_monitor.start_monitoring()
        
        # Create users in bulk
        user_data_list = [
            UserCreate(
                email=f"user{i}@test.com",
                username=f"user{i}",
                first_name=f"User{i}",
                last_name="Test",
                password="password123"
            )
            for i in range(100)
        ]
        
        users = user_crud.create_bulk(db=db_session, obj_in_list=user_data_list)
        
        performance_monitor.stop_monitoring()
        metrics = performance_monitor.get_metrics()
        
        assert len(users) == 100
        assert metrics['execution_time'] < 5.0  # Should complete in under 5 seconds
        
        # Test bulk update
        performance_monitor.start_monitoring()
        
        update_data = UserUpdate(bio="Bulk updated bio")
        updated_users = user_crud.update_bulk(
            db=db_session,
            ids=[user.id for user in users[:50]],
            obj_in=update_data
        )
        
        performance_monitor.stop_monitoring()
        metrics = performance_monitor.get_metrics()
        
        assert len(updated_users) == 50
        assert metrics['execution_time'] < 2.0  # Should complete in under 2 seconds
    
    def test_pagination_performance(self, db_session: Session, performance_monitor):
        """Test pagination performance with large datasets."""
        # Create large dataset
        Factories.User.create_batch(1000, session=db_session)
        
        # Test first page performance
        performance_monitor.start_monitoring()
        
        result = user_crud.get_multi_with_pagination(
            db=db_session,
            pagination=PaginationParams(page=1, size=50)
        )
        
        performance_monitor.stop_monitoring()
        metrics = performance_monitor.get_metrics()
        
        assert len(result.items) == 50
        assert result.total >= 1000
        assert metrics['execution_time'] < 1.0  # Should be fast
        
        # Test middle page performance
        performance_monitor.start_monitoring()
        
        result = user_crud.get_multi_with_pagination(
            db=db_session,
            pagination=PaginationParams(page=10, size=50)
        )
        
        performance_monitor.stop_monitoring()
        metrics = performance_monitor.get_metrics()
        
        assert len(result.items) == 50
        assert metrics['execution_time'] < 1.0  # Should remain fast
    
    def test_complex_query_performance(self, db_session: Session, performance_monitor):
        """Test performance of complex queries with joins."""
        # Create related data
        authors = Factories.AuthorUser.create_batch(10, session=db_session)
        categories = Factories.Category.create_batch(5, session=db_session)
        
        for author in authors:
            for category in categories:
                Factories.Post.create_batch(
                    2, session=db_session, author=author, category=category
                )
        
        performance_monitor.start_monitoring()
        
        # Complex query with multiple joins and filters
        results = post_crud.get_posts_with_details(
            db=db_session,
            include_author=True,
            include_category=True,
            include_comment_count=True,
            status=PostStatus.PUBLISHED
        )
        
        performance_monitor.stop_monitoring()
        metrics = performance_monitor.get_metrics()
        
        assert len(results) >= 50  # Should have many posts
        assert metrics['execution_time'] < 2.0  # Complex query should still be reasonable
        
        # Verify no N+1 query problems
        # (This would require query monitoring to verify)
```

This completes Part 3 of Section 7.1. Should I continue with Part 4 (API Testing) to finish the testing section before moving to the next major component?


#### 7.1.5 API Integration Testing

**tests/test_api.py** - Complete API Testing:
```python
"""
Comprehensive API integration testing covering all endpoints, authentication,
authorization, error handling, and API contract validation.
"""

import pytest
import json
from datetime import datetime, timedelta
from httpx import AsyncClient
from fastapi import status

from app.models.user import User, UserRole
from app.models.blog import Post, PostStatus
from app.core.security import create_access_token
from tests.factories import Factories


class TestAuthenticationAPI:
    """Test authentication endpoints."""
    
    @pytest.mark.asyncio
    async def test_user_registration(self, async_client: AsyncClient, async_db_session):
        """Test user registration endpoint."""
        registration_data = {
            "email": "newuser@test.com",
            "username": "newuser",
            "first_name": "New",
            "last_name": "User",
            "password": "securepassword123"
        }
        
        response = await async_client.post("/api/v1/auth/register", json=registration_data)
        
        assert response.status_code == status.HTTP_201_CREATED
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["email"] == "newuser@test.com"
        assert response_data["data"]["username"] == "newuser"
        assert response_data["data"]["role"] == "user"
        assert "password" not in response_data["data"]
    
    @pytest.mark.asyncio
    async def test_user_registration_duplicate_email(self, async_client: AsyncClient, async_db_session):
        """Test registration with duplicate email."""
        # Create existing user
        existing_user = await Factories.User.create(session=async_db_session)
        
        registration_data = {
            "email": existing_user.email,
            "username": "differentusername",
            "first_name": "Test",
            "last_name": "User",
            "password": "password123"
        }
        
        response = await async_client.post("/api/v1/auth/register", json=registration_data)
        
        assert response.status_code == status.HTTP_400_BAD_REQUEST
        response_data = response.json()
        assert response_data["success"] is False
        assert "email" in response_data["message"].lower()
    
    @pytest.mark.asyncio
    async def test_user_login_success(self, async_client: AsyncClient, async_db_session):
        """Test successful user login."""
        # Create user with known password
        user = await Factories.User.create(
            session=async_db_session,
            email="login@test.com",
            password="password123"
        )
        
        login_data = {
            "username": "login@test.com",  # Using email as username
            "password": "password123"
        }
        
        response = await async_client.post(
            "/api/v1/auth/login",
            data=login_data,  # OAuth2PasswordRequestForm expects form data
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert "access_token" in response_data
        assert "refresh_token" in response_data
        assert response_data["token_type"] == "bearer"
        assert response_data["user"]["email"] == "login@test.com"
    
    @pytest.mark.asyncio
    async def test_user_login_invalid_credentials(self, async_client: AsyncClient, async_db_session):
        """Test login with invalid credentials."""
        login_data = {
            "username": "nonexistent@test.com",
            "password": "wrongpassword"
        }
        
        response = await async_client.post(
            "/api/v1/auth/login",
            data=login_data,
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        assert response.status_code == status.HTTP_401_UNAUTHORIZED
        response_data = response.json()
        assert response_data["detail"] == "Incorrect email or password"
    
    @pytest.mark.asyncio
    async def test_token_refresh(self, async_client: AsyncClient, regular_user):
        """Test token refresh functionality."""
        # Create valid refresh token
        refresh_token = create_access_token(
            subject=regular_user.email,
            expires_delta=timedelta(days=7)
        )
        
        refresh_data = {
            "refresh_token": refresh_token
        }
        
        response = await async_client.post("/api/v1/auth/refresh", json=refresh_data)
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert "access_token" in response_data["data"]
        assert "refresh_token" in response_data["data"]
        assert response_data["data"]["user"]["email"] == regular_user.email
    
    @pytest.mark.asyncio
    async def test_get_current_user_profile(self, async_client: AsyncClient, auth_headers_user, regular_user):
        """Test getting current user profile."""
        response = await async_client.get("/api/v1/auth/me", headers=auth_headers_user)
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["email"] == regular_user.email
        assert response_data["data"]["id"] == regular_user.id
    
    @pytest.mark.asyncio
    async def test_unauthorized_access(self, async_client: AsyncClient):
        """Test access without authentication token."""
        response = await async_client.get("/api/v1/auth/me")
        
        assert response.status_code == status.HTTP_401_UNAUTHORIZED
        response_data = response.json()
        assert response_data["detail"] == "Not authenticated"
    
    @pytest.mark.asyncio
    async def test_invalid_token(self, async_client: AsyncClient):
        """Test access with invalid token."""
        headers = {"Authorization": "Bearer invalid_token"}
        response = await async_client.get("/api/v1/auth/me", headers=headers)
        
        assert response.status_code == status.HTTP_401_UNAUTHORIZED


class TestUserAPI:
    """Test user management endpoints."""
    
    @pytest.mark.asyncio
    async def test_get_users_list(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test getting users list with pagination."""
        # Create multiple users
        users = await Factories.User.create_batch(15, session=async_db_session)
        
        response = await async_client.get(
            "/api/v1/users/?page=1&size=10",
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert len(response_data["items"]) == 10
        assert response_data["total"] >= 15
        assert response_data["page"] == 1
        assert response_data["pages"] >= 2
    
    @pytest.mark.asyncio
    async def test_get_user_profile(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test getting specific user profile."""
        user = await Factories.User.create(session=async_db_session)
        
        response = await async_client.get(
            f"/api/v1/users/profile/{user.id}",
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["id"] == user.id
        assert response_data["data"]["email"] == user.email
    
    @pytest.mark.asyncio
    async def test_update_user_profile(self, async_client: AsyncClient, auth_headers_user, regular_user):
        """Test updating user profile."""
        update_data = {
            "first_name": "Updated",
            "last_name": "Name",
            "bio": "Updated bio content"
        }
        
        response = await async_client.put(
            "/api/v1/users/profile",
            json=update_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["first_name"] == "Updated"
        assert response_data["data"]["last_name"] == "Name"
        assert response_data["data"]["bio"] == "Updated bio content"
    
    @pytest.mark.asyncio
    async def test_change_password(self, async_client: AsyncClient, auth_headers_user):
        """Test password change functionality."""
        password_data = {
            "current_password": "password123",
            "new_password": "newsecurepassword456",
            "confirm_password": "newsecurepassword456"
        }
        
        response = await async_client.post(
            "/api/v1/users/change-password",
            json=password_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        assert response_data["success"] is True
    
    @pytest.mark.asyncio
    async def test_follow_user(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test following another user."""
        target_user = await Factories.User.create(session=async_db_session)
        
        follow_data = {
            "user_id": target_user.id
        }
        
        response = await async_client.post(
            "/api/v1/users/follow",
            json=follow_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        assert response_data["success"] is True
    
    @pytest.mark.asyncio
    async def test_admin_only_endpoints(self, async_client: AsyncClient, auth_headers_user, auth_headers_admin):
        """Test admin-only endpoints."""
        # Regular user should not have access
        response = await async_client.get(
            "/api/v1/users/statistics",
            headers=auth_headers_user
        )
        assert response.status_code == status.HTTP_403_FORBIDDEN
        
        # Admin should have access
        response = await async_client.get(
            "/api/v1/users/statistics",
            headers=auth_headers_admin
        )
        assert response.status_code == status.HTTP_200_OK


class TestBlogAPI:
    """Test blog management endpoints."""
    
    @pytest.mark.asyncio
    async def test_create_post(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test creating a new post."""
        category = await Factories.Category.create(session=async_db_session)
        
        post_data = {
            "title": "Test API Post",
            "content": "This is test content for the API post.",
            "category_id": category.id,
            "excerpt": "Test excerpt",
            "status": "draft"
        }
        
        response = await async_client.post(
            "/api/v1/blog/posts",
            json=post_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_201_CREATED
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["title"] == "Test API Post"
        assert response_data["data"]["status"] == "draft"
        assert response_data["data"]["category_id"] == category.id
    
    @pytest.mark.asyncio
    async def test_get_posts_list(self, async_client: AsyncClient, async_db_session):
        """Test getting posts list with filtering."""
        category = await Factories.Category.create(session=async_db_session)
        
        # Create published and draft posts
        published_posts = await Factories.Post.create_batch(
            5, session=async_db_session, status=PostStatus.PUBLISHED, category=category
        )
        draft_posts = await Factories.DraftPost.create_batch(
            3, session=async_db_session, category=category
        )
        
        # Get all published posts
        response = await async_client.get(
            "/api/v1/blog/posts?status=published&page=1&size=10"
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert len(response_data["items"]) >= 5
        assert all(post["status"] == "published" for post in response_data["items"])
        
        # Filter by category
        response = await async_client.get(
            f"/api/v1/blog/posts?category_id={category.id}"
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        assert all(post["category_id"] == category.id for post in response_data["items"])
    
    @pytest.mark.asyncio
    async def test_get_post_by_id(self, async_client: AsyncClient, async_db_session):
        """Test getting specific post by ID."""
        post = await Factories.Post.create(
            session=async_db_session,
            status=PostStatus.PUBLISHED
        )
        
        response = await async_client.get(f"/api/v1/blog/posts/{post.id}")
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["id"] == post.id
        assert response_data["data"]["title"] == post.title
    
    @pytest.mark.asyncio
    async def test_update_post(self, async_client: AsyncClient, auth_headers_user, async_db_session, regular_user):
        """Test updating a post."""
        post = await Factories.Post.create(
            session=async_db_session,
            author=regular_user,
            status=PostStatus.DRAFT
        )
        
        update_data = {
            "title": "Updated Post Title",
            "content": "Updated content",
            "excerpt": "Updated excerpt"
        }
        
        response = await async_client.put(
            f"/api/v1/blog/posts/{post.id}",
            json=update_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["title"] == "Updated Post Title"
        assert response_data["data"]["content"] == "Updated content"
    
    @pytest.mark.asyncio
    async def test_publish_post(self, async_client: AsyncClient, auth_headers_user, async_db_session, regular_user):
        """Test publishing a draft post."""
        post = await Factories.DraftPost.create(
            session=async_db_session,
            author=regular_user
        )
        
        response = await async_client.post(
            f"/api/v1/blog/posts/{post.id}/publish",
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["status"] == "published"
        assert response_data["data"]["published_at"] is not None
    
    @pytest.mark.asyncio
    async def test_unauthorized_post_modification(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test that users cannot modify others' posts."""
        other_user = await Factories.User.create(session=async_db_session)
        post = await Factories.Post.create(
            session=async_db_session,
            author=other_user
        )
        
        update_data = {
            "title": "Unauthorized Update"
        }
        
        response = await async_client.put(
            f"/api/v1/blog/posts/{post.id}",
            json=update_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_403_FORBIDDEN
    
    @pytest.mark.asyncio
    async def test_post_comments(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test post comments functionality."""
        post = await Factories.Post.create(
            session=async_db_session,
            status=PostStatus.PUBLISHED
        )
        
        # Create a comment
        comment_data = {
            "content": "This is a test comment on the post."
        }
        
        response = await async_client.post(
            f"/api/v1/blog/posts/{post.id}/comments",
            json=comment_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_201_CREATED
        response_data = response.json()
        
        assert response_data["success"] is True
        assert response_data["data"]["content"] == "This is a test comment on the post."
        assert response_data["data"]["post_id"] == post.id
        
        # Get comments for the post
        response = await async_client.get(f"/api/v1/blog/posts/{post.id}/comments")
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        assert len(response_data["items"]) >= 1
    
    @pytest.mark.asyncio
    async def test_like_post(self, async_client: AsyncClient, auth_headers_user, async_db_session):
        """Test liking a post."""
        post = await Factories.Post.create(
            session=async_db_session,
            status=PostStatus.PUBLISHED
        )
        
        response = await async_client.post(
            f"/api/v1/blog/posts/{post.id}/like",
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        assert response_data["success"] is True
    
    @pytest.mark.asyncio
    async def test_search_posts(self, async_client: AsyncClient, async_db_session):
        """Test post search functionality."""
        # Create posts with searchable content
        await Factories.Post.create(
            session=async_db_session,
            title="Python Programming Tutorial",
            content="Learn Python programming basics",
            status=PostStatus.PUBLISHED
        )
        
        await Factories.Post.create(
            session=async_db_session,
            title="JavaScript Guide",
            content="Complete JavaScript reference",
            status=PostStatus.PUBLISHED
        )
        
        # Search for Python posts
        response = await async_client.get(
            "/api/v1/blog/search?query=Python&page=1&size=10"
        )
        
        assert response.status_code == status.HTTP_200_OK
        response_data = response.json()
        
        assert len(response_data["items"]) >= 1
        assert any("Python" in post["title"] for post in response_data["items"])


class TestAPIErrorHandling:
    """Test API error handling and validation."""
    
    @pytest.mark.asyncio
    async def test_validation_errors(self, async_client: AsyncClient, auth_headers_user):
        """Test request validation errors."""
        # Invalid email format
        invalid_user_data = {
            "email": "invalid-email",
            "username": "testuser",
            "first_name": "Test",
            "last_name": "User",
            "password": "password123"
        }
        
        response = await async_client.post(
            "/api/v1/auth/register",
            json=invalid_user_data
        )
        
        assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY
        response_data = response.json()
        assert "detail" in response_data
        assert any("email" in str(error) for error in response_data["detail"])
    
    @pytest.mark.asyncio
    async def test_missing_required_fields(self, async_client: AsyncClient, auth_headers_user):
        """Test missing required fields."""
        incomplete_post_data = {
            "title": "Test Post"
            # Missing content and other required fields
        }
        
        response = await async_client.post(
            "/api/v1/blog/posts",
            json=incomplete_post_data,
            headers=auth_headers_user
        )
        
        assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY
    
    @pytest.mark.asyncio
    async def test_resource_not_found(self, async_client: AsyncClient):
        """Test 404 errors for non-existent resources."""
        response = await async_client.get("/api/v1/blog/posts/99999")
        
        assert response.status_code == status.HTTP_404_NOT_FOUND
        response_data = response.json()
        assert response_data["detail"] == "Post not found"
    
    @pytest.mark.asyncio
    async def test_rate_limiting(self, async_client: AsyncClient):
        """Test rate limiting functionality."""
        # This test would require implementing rate limiting
        # Make multiple rapid requests to trigger rate limiting
        for _ in range(100):
            response = await async_client.get("/api/v1/blog/posts")
            if response.status_code == status.HTTP_429_TOO_MANY_REQUESTS:
                assert "rate limit" in response.json()["detail"].lower()
                break
    
    @pytest.mark.asyncio
    async def test_server_error_handling(self, async_client: AsyncClient, monkeypatch):
        """Test server error handling."""
        # Mock a service to raise an exception
        def mock_failing_service(*args, **kwargs):
            raise Exception("Simulated server error")
        
        # This would require mocking the actual service
        # monkeypatch.setattr("app.crud.user.user_crud.get", mock_failing_service)
        
        # The test would verify that 500 errors are handled gracefully
        # and don't expose sensitive information


class TestAPIPerformance:
    """Test API performance characteristics."""
    
    @pytest.mark.asyncio
    async def test_response_times(self, async_client: AsyncClient, async_db_session, performance_monitor):
        """Test API response times."""
        # Create test data
        await Factories.Post.create_batch(100, session=async_db_session)
        
        performance_monitor.start_monitoring()
        
        response = await async_client.get("/api/v1/blog/posts?page=1&size=20")
        
        performance_monitor.stop_monitoring()
        metrics = performance_monitor.get_metrics()
        
        assert response.status_code == status.HTTP_200_OK
        assert metrics['execution_time'] < 1.0  # Should respond in under 1 second
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, async_client: AsyncClient, async_db_session):
        """Test handling of concurrent requests."""
        import asyncio
        
        # Create test data
        await Factories.Post.create_batch(50, session=async_db_session)
        
        # Make multiple concurrent requests
        async def make_request():
            return await async_client.get("/api/v1/blog/posts")
        
        tasks = [make_request() for _ in range(10)]
        responses = await asyncio.gather(*tasks)
        
        # All requests should succeed
        assert all(response.status_code == status.HTTP_200_OK for response in responses)
        
        # Response data should be consistent
        first_response_data = responses[0].json()
        assert all(
            response.json()["total"] == first_response_data["total"]
            for response in responses
        )


class TestAPIContractValidation:
    """Test API contract compliance and OpenAPI schema validation."""
    
    @pytest.mark.asyncio
    async def test_openapi_schema_compliance(self, async_client: AsyncClient):
        """Test that API responses match OpenAPI schema."""
        response = await async_client.get("/openapi.json")
        assert response.status_code == status.HTTP_200_OK
        
        openapi_schema = response.json()
        assert "openapi" in openapi_schema
        assert "paths" in openapi_schema
        assert "components" in openapi_schema
    
    @pytest.mark.asyncio
    async def test_response_schema_validation(self, async_client: AsyncClient, async_db_session):
        """Test that API responses match expected schemas."""
        post = await Factories.Post.create(session=async_db_session)
        
        response = await async_client.get(f"/api/v1/blog/posts/{post.id}")
        assert response.status_code == status.HTTP_200_OK
        
        response_data = response.json()
        
        # Validate response structure
        assert "success" in response_data
        assert "data" in response_data
        assert isinstance(response_data["success"], bool)
        assert isinstance(response_data["data"], dict)
        
        # Validate post data structure
        post_data = response_data["data"]
        required_fields = ["id", "title", "content", "status", "created_at"]
        assert all(field in post_data for field in required_fields)
    
    @pytest.mark.asyncio
    async def test_pagination_schema_consistency(self, async_client: AsyncClient, async_db_session):
        """Test that paginated responses follow consistent schema."""
        await Factories.Post.create_batch(25, session=async_db_session)
        
        response = await async_client.get("/api/v1/blog/posts?page=1&size=10")
        assert response.status_code == status.HTTP_200_OK
        
        response_data = response.json()
        
        # Validate pagination structure
        pagination_fields = ["items", "total", "page", "size", "pages"]
        assert all(field in response_data for field in pagination_fields)
        
        assert isinstance(response_data["items"], list)
        assert isinstance(response_data["total"], int)
        assert isinstance(response_data["page"], int)
        assert isinstance(response_data["size"], int)
        assert isinstance(response_data["pages"], int)
        
        assert len(response_data["items"]) <= response_data["size"]
        assert response_data["page"] >= 1
        assert response_data["pages"] >= 1
```

This completes Section 7.1 - Comprehensive Testing Framework! Now let me continue with Section 7.2 - Performance Optimization and Monitoring:


### 7.2 Performance Optimization and Monitoring

Performance optimization is crucial for SQLAlchemy applications in production. This section covers query optimization, caching strategies, monitoring, and performance profiling.

#### 7.2.1 Query Optimization and Performance Profiling

**app/utils/performance.py** - Performance Monitoring and Optimization Tools:
```python
"""
Comprehensive performance monitoring and optimization utilities for SQLAlchemy applications.
Includes query profiling, performance metrics, and optimization recommendations.
"""

import time
import logging
import functools
from typing import Dict, List, Any, Optional, Callable
from contextlib import contextmanager
from datetime import datetime, timedelta
from dataclasses import dataclass, field

import psutil
from sqlalchemy import event, text
from sqlalchemy.engine import Engine
from sqlalchemy.orm import Session
from sqlalchemy.pool import Pool

logger = logging.getLogger(__name__)


@dataclass
class QueryMetrics:
    """Container for query performance metrics."""
    statement: str
    parameters: Dict[str, Any]
    execution_time: float
    timestamp: datetime
    connection_id: str
    rows_affected: Optional[int] = None
    fetch_time: Optional[float] = None
    
    @property
    def total_time(self) -> float:
        """Total time including fetch time."""
        return self.execution_time + (self.fetch_time or 0)


@dataclass
class PerformanceReport:
    """Comprehensive performance report."""
    total_queries: int
    total_execution_time: float
    average_query_time: float
    slowest_queries: List[QueryMetrics]
    query_frequency: Dict[str, int]
    connection_pool_stats: Dict[str, Any]
    memory_usage: Dict[str, float]
    recommendations: List[str] = field(default_factory=list)


class QueryProfiler:
    """Advanced query profiler with performance analysis."""
    
    def __init__(self, slow_query_threshold: float = 1.0):
        self.slow_query_threshold = slow_query_threshold
        self.queries: List[QueryMetrics] = []
        self.enabled = False
        self._start_time = None
        
    def start_profiling(self):
        """Start query profiling."""
        self.enabled = True
        self._start_time = time.time()
        self.queries.clear()
        logger.info("Query profiling started")
    
    def stop_profiling(self):
        """Stop query profiling."""
        self.enabled = False
        duration = time.time() - (self._start_time or 0)
        logger.info(f"Query profiling stopped. Duration: {duration:.2f}s, Queries: {len(self.queries)}")
    
    def add_query(self, metrics: QueryMetrics):
        """Add query metrics to profiler."""
        if self.enabled:
            self.queries.append(metrics)
            
            if metrics.execution_time > self.slow_query_threshold:
                logger.warning(f"Slow query detected: {metrics.execution_time:.3f}s - {metrics.statement[:100]}...")
    
    def get_report(self) -> PerformanceReport:
        """Generate comprehensive performance report."""
        if not self.queries:
            return PerformanceReport(0, 0, 0, [], {}, {}, {})
        
        total_queries = len(self.queries)
        total_execution_time = sum(q.execution_time for q in self.queries)
        average_query_time = total_execution_time / total_queries
        
        # Find slowest queries
        slowest_queries = sorted(
            self.queries,
            key=lambda q: q.execution_time,
            reverse=True
        )[:10]
        
        # Analyze query frequency
        query_frequency = {}
        for query in self.queries:
            # Normalize query by removing parameters
            normalized = self._normalize_query(query.statement)
            query_frequency[normalized] = query_frequency.get(normalized, 0) + 1
        
        # Get system metrics
        memory_usage = self._get_memory_usage()
        
        # Generate recommendations
        recommendations = self._generate_recommendations(
            slowest_queries, query_frequency, average_query_time
        )
        
        return PerformanceReport(
            total_queries=total_queries,
            total_execution_time=total_execution_time,
            average_query_time=average_query_time,
            slowest_queries=slowest_queries,
            query_frequency=query_frequency,
            connection_pool_stats={},  # Will be populated by connection monitor
            memory_usage=memory_usage,
            recommendations=recommendations
        )
    
    def _normalize_query(self, statement: str) -> str:
        """Normalize query statement for frequency analysis."""
        # Simple normalization - replace parameter placeholders
        import re
        normalized = re.sub(r'%\([^)]+\)s', '?', statement)
        normalized = re.sub(r'\$\d+', '?', normalized)
        normalized = re.sub(r'\?', 'PARAM', normalized)
        return normalized.strip()
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """Get current memory usage statistics."""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': process.memory_percent()
        }
    
    def _generate_recommendations(
        self,
        slow_queries: List[QueryMetrics],
        query_frequency: Dict[str, int],
        avg_time: float
    ) -> List[str]:
        """Generate performance optimization recommendations."""
        recommendations = []
        
        # Check for slow queries
        if slow_queries and slow_queries[0].execution_time > 2.0:
            recommendations.append(
                f"Consider optimizing the slowest query: {slow_queries[0].execution_time:.3f}s"
            )
        
        # Check for frequently executed queries
        frequent_queries = [(q, count) for q, count in query_frequency.items() if count > 10]
        if frequent_queries:
            recommendations.append(
                f"Consider caching results for {len(frequent_queries)} frequently executed queries"
            )
        
        # Check for N+1 query patterns
        if len([q for q in query_frequency.values() if q > 50]) > 3:
            recommendations.append(
                "Possible N+1 query pattern detected. Consider using eager loading."
            )
        
        # Check average query time
        if avg_time > 0.5:
            recommendations.append(
                f"Average query time is high ({avg_time:.3f}s). Consider adding indexes or optimizing queries."
            )
        
        return recommendations


class ConnectionPoolMonitor:
    """Monitor database connection pool performance."""
    
    def __init__(self, engine: Engine):
        self.engine = engine
        self.pool = engine.pool
        self.connection_events: List[Dict[str, Any]] = []
        self.checkout_times: Dict[str, float] = {}
        
    def start_monitoring(self):
        """Start monitoring connection pool events."""
        @event.listens_for(self.pool, "connect")
        def log_connect(dbapi_connection, connection_record):
            self.connection_events.append({
                'event': 'connect',
                'timestamp': datetime.utcnow(),
                'connection_id': id(connection_record)
            })
        
        @event.listens_for(self.pool, "checkout")
        def log_checkout(dbapi_connection, connection_record, connection_proxy):
            connection_id = id(connection_record)
            self.checkout_times[connection_id] = time.time()
            self.connection_events.append({
                'event': 'checkout',
                'timestamp': datetime.utcnow(),
                'connection_id': connection_id
            })
        
        @event.listens_for(self.pool, "checkin")
        def log_checkin(dbapi_connection, connection_record):
            connection_id = id(connection_record)
            checkout_time = self.checkout_times.pop(connection_id, None)
            usage_time = time.time() - checkout_time if checkout_time else None
            
            self.connection_events.append({
                'event': 'checkin',
                'timestamp': datetime.utcnow(),
                'connection_id': connection_id,
                'usage_time': usage_time
            })
        
        logger.info("Connection pool monitoring started")
    
    def get_pool_stats(self) -> Dict[str, Any]:
        """Get current connection pool statistics."""
        pool_status = self.pool.status()
        
        # Calculate average usage time
        usage_times = [
            event['usage_time'] for event in self.connection_events
            if event['event'] == 'checkin' and event.get('usage_time')
        ]
        avg_usage_time = sum(usage_times) / len(usage_times) if usage_times else 0
        
        return {
            'pool_size': self.pool.size(),
            'checked_in': self.pool.checkedin(),
            'checked_out': self.pool.checkedout(),
            'overflow': self.pool.overflow(),
            'invalid': self.pool.invalid(),
            'average_usage_time': avg_usage_time,
            'total_connections_created': len([
                e for e in self.connection_events if e['event'] == 'connect'
            ]),
            'status': pool_status
        }


class PerformanceMiddleware:
    """FastAPI middleware for request performance monitoring."""
    
    def __init__(self, app, profiler: QueryProfiler):
        self.app = app
        self.profiler = profiler
        self.request_metrics: List[Dict[str, Any]] = []
    
    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
        
        start_time = time.time()
        self.profiler.start_profiling()
        
        # Process request
        await self.app(scope, receive, send)
        
        # Record metrics
        duration = time.time() - start_time
        self.profiler.stop_profiling()
        
        request_path = scope.get("path", "")
        method = scope.get("method", "")
        
        self.request_metrics.append({
            'path': request_path,
            'method': method,
            'duration': duration,
            'query_count': len(self.profiler.queries),
            'timestamp': datetime.utcnow()
        })
        
        # Log slow requests
        if duration > 2.0:
            logger.warning(f"Slow request: {method} {request_path} - {duration:.3f}s")


def performance_monitor(threshold: float = 1.0):
    """Decorator for monitoring function performance."""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                end_memory = psutil.Process().memory_info().rss
                memory_delta = (end_memory - start_memory) / 1024 / 1024  # MB
                
                if duration > threshold:
                    logger.warning(
                        f"Performance warning: {func.__name__} took {duration:.3f}s, "
                        f"memory delta: {memory_delta:.2f}MB"
                    )
                
        return wrapper
    return decorator


@contextmanager
def query_profiling(engine: Engine, threshold: float = 1.0):
    """Context manager for query profiling."""
    profiler = QueryProfiler(threshold)
    
    # Set up event listeners
    @event.listens_for(engine, "before_cursor_execute")
    def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        context._query_start_time = time.time()
        context._query_parameters = parameters
    
    @event.listens_for(engine, "after_cursor_execute")
    def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        execution_time = time.time() - context._query_start_time
        
        metrics = QueryMetrics(
            statement=statement,
            parameters=context._query_parameters or {},
            execution_time=execution_time,
            timestamp=datetime.utcnow(),
            connection_id=str(id(conn)),
            rows_affected=cursor.rowcount if hasattr(cursor, 'rowcount') else None
        )
        
        profiler.add_query(metrics)
    
    profiler.start_profiling()
    
    try:
        yield profiler
    finally:
        profiler.stop_profiling()
        
        # Remove event listeners
        event.remove(engine, "before_cursor_execute", receive_before_cursor_execute)
        event.remove(engine, "after_cursor_execute", receive_after_cursor_execute)


class DatabaseOptimizer:
    """Database optimization utilities and recommendations."""
    
    def __init__(self, session: Session):
        self.session = session
    
    def analyze_table_stats(self, table_name: str) -> Dict[str, Any]:
        """Analyze table statistics for optimization opportunities."""
        stats_query = text("""
            SELECT 
                schemaname,
                tablename,
                attname,
                n_distinct,
                correlation,
                most_common_vals,
                most_common_freqs,
                histogram_bounds
            FROM pg_stats 
            WHERE tablename = :table_name
        """)
        
        result = self.session.execute(stats_query, {'table_name': table_name})
        stats = result.fetchall()
        
        return {
            'table_name': table_name,
            'column_stats': [dict(row) for row in stats],
            'recommendations': self._generate_table_recommendations(stats)
        }
    
    def analyze_query_performance(self, query: str) -> Dict[str, Any]:
        """Analyze query performance using EXPLAIN ANALYZE."""
        explain_query = text(f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}")
        
        result = self.session.execute(explain_query)
        explain_result = result.fetchone()[0]
        
        return {
            'query': query,
            'plan': explain_result,
            'recommendations': self._analyze_execution_plan(explain_result)
        }
    
    def suggest_indexes(self, table_name: str) -> List[Dict[str, Any]]:
        """Suggest indexes based on query patterns."""
        # Get missing indexes from pg_stat_user_tables
        missing_indexes_query = text("""
            SELECT 
                schemaname,
                tablename,
                seq_scan,
                seq_tup_read,
                idx_scan,
                idx_tup_fetch,
                n_tup_ins,
                n_tup_upd,
                n_tup_del
            FROM pg_stat_user_tables 
            WHERE tablename = :table_name
        """)
        
        result = self.session.execute(missing_indexes_query, {'table_name': table_name})
        stats = result.fetchone()
        
        recommendations = []
        
        if stats:
            # High sequential scan ratio suggests missing indexes
            total_scans = (stats.seq_scan or 0) + (stats.idx_scan or 0)
            if total_scans > 0:
                seq_scan_ratio = (stats.seq_scan or 0) / total_scans
                if seq_scan_ratio > 0.7:
                    recommendations.append({
                        'type': 'index',
                        'priority': 'high',
                        'description': f'High sequential scan ratio ({seq_scan_ratio:.1%}) suggests missing indexes',
                        'table': table_name
                    })
        
        return recommendations
    
    def _generate_table_recommendations(self, stats) -> List[str]:
        """Generate optimization recommendations based on table statistics."""
        recommendations = []
        
        for stat in stats:
            # Check for low cardinality columns that might benefit from partial indexes
            if stat['n_distinct'] and stat['n_distinct'] < 10:
                recommendations.append(
                    f"Consider partial index on {stat['attname']} (low cardinality: {stat['n_distinct']})"
                )
            
            # Check correlation for ordering
            if stat['correlation'] and abs(stat['correlation']) > 0.8:
                recommendations.append(
                    f"Column {stat['attname']} has high correlation ({stat['correlation']:.2f}) - good for range queries"
                )
        
        return recommendations
    
    def _analyze_execution_plan(self, plan: List[Dict]) -> List[str]:
        """Analyze execution plan and generate recommendations."""
        recommendations = []
        
        def analyze_node(node):
            node_type = node.get('Node Type', '')
            
            # Check for expensive operations
            if node_type == 'Seq Scan':
                cost = node.get('Total Cost', 0)
                if cost > 1000:
                    relation = node.get('Relation Name', 'unknown')
                    recommendations.append(
                        f"Expensive sequential scan on {relation} (cost: {cost:.0f}) - consider adding index"
                    )
            
            elif node_type == 'Sort' and node.get('Sort Method') == 'external merge':
                recommendations.append(
                    "External merge sort detected - consider increasing work_mem"
                )
            
            elif 'Hash' in node_type and node.get('Peak Memory Usage', 0) > 50000:  # 50MB
                recommendations.append(
                    "Large hash table in memory - consider optimizing join conditions"
                )
            
            # Recursively analyze child nodes
            for child in node.get('Plans', []):
                analyze_node(child)
        
        for plan_node in plan:
            if 'Plan' in plan_node:
                analyze_node(plan_node['Plan'])
        
        return recommendations


# Application performance metrics collector
class ApplicationMetrics:
    """Collect and manage application performance metrics."""
    
    def __init__(self):
        self.metrics = {
            'requests': [],
            'database_queries': [],
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': []
        }
    
    def record_request(self, path: str, method: str, duration: float, status_code: int):
        """Record HTTP request metrics."""
        self.metrics['requests'].append({
            'path': path,
            'method': method,
            'duration': duration,
            'status_code': status_code,
            'timestamp': datetime.utcnow()
        })
    
    def record_database_query(self, query: str, duration: float):
        """Record database query metrics."""
        self.metrics['database_queries'].append({
            'query': query,
            'duration': duration,
            'timestamp': datetime.utcnow()
        })
    
    def record_cache_hit(self):
        """Record cache hit."""
        self.metrics['cache_hits'] += 1
    
    def record_cache_miss(self):
        """Record cache miss."""
        self.metrics['cache_misses'] += 1
    
    def record_error(self, error_type: str, message: str):
        """Record application error."""
        self.metrics['errors'].append({
            'type': error_type,
            'message': message,
            'timestamp': datetime.utcnow()
        })
    
    def get_summary(self, time_window: timedelta = timedelta(hours=1)) -> Dict[str, Any]:
        """Get metrics summary for specified time window."""
        now = datetime.utcnow()
        cutoff = now - time_window
        
        # Filter recent requests
        recent_requests = [
            r for r in self.metrics['requests'] 
            if r['timestamp'] >= cutoff
        ]
        
        # Filter recent queries
        recent_queries = [
            q for q in self.metrics['database_queries']
            if q['timestamp'] >= cutoff
        ]
        
        # Calculate averages
        avg_request_time = (
            sum(r['duration'] for r in recent_requests) / len(recent_requests)
            if recent_requests else 0
        )
        
        avg_query_time = (
            sum(q['duration'] for q in recent_queries) / len(recent_queries)
            if recent_queries else 0
        )
        
        # Cache hit ratio
        total_cache_operations = self.metrics['cache_hits'] + self.metrics['cache_misses']
        cache_hit_ratio = (
            self.metrics['cache_hits'] / total_cache_operations
            if total_cache_operations > 0 else 0
        )
        
        return {
            'time_window': str(time_window),
            'request_count': len(recent_requests),
            'average_request_time': avg_request_time,
            'query_count': len(recent_queries),
            'average_query_time': avg_query_time,
            'cache_hit_ratio': cache_hit_ratio,
            'error_count': len([
                e for e in self.metrics['errors']
                if e['timestamp'] >= cutoff
            ])
        }


# Global metrics instance
app_metrics = ApplicationMetrics()
```

This completes Section 7.2.1 - Performance Optimization and Profiling. Should I continue with Section 7.2.2 - Caching Strategies and Implementation?


#### 7.2.2 Caching Strategies and Implementation

**app/utils/caching.py** - Comprehensive Caching System:
```python
"""
Advanced caching system for SQLAlchemy applications with multiple cache layers,
intelligent invalidation, and performance optimization.
"""

import json
import hashlib
import logging
import functools
from typing import Any, Dict, List, Optional, Union, Callable, TypeVar
from datetime import datetime, timedelta
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict

import redis
from sqlalchemy.orm import Session
from sqlalchemy import event, text

logger = logging.getLogger(__name__)

T = TypeVar('T')


@dataclass
class CacheKey:
    """Structured cache key with metadata."""
    prefix: str
    identifier: str
    version: str = "v1"
    namespace: Optional[str] = None
    
    def to_string(self) -> str:
        """Convert to Redis key string."""
        parts = [self.prefix, self.version]
        if self.namespace:
            parts.append(self.namespace)
        parts.append(self.identifier)
        return ":".join(parts)
    
    @classmethod
    def from_string(cls, key_string: str) -> 'CacheKey':
        """Parse cache key from string."""
        parts = key_string.split(":")
        if len(parts) < 3:
            raise ValueError(f"Invalid cache key format: {key_string}")
        
        return cls(
            prefix=parts[0],
            version=parts[1],
            namespace=parts[2] if len(parts) > 3 else None,
            identifier=parts[-1]
        )


@dataclass
class CacheEntry:
    """Cache entry with metadata."""
    data: Any
    created_at: datetime
    expires_at: Optional[datetime] = None
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    tags: List[str] = None
    
    def is_expired(self) -> bool:
        """Check if cache entry is expired."""
        if self.expires_at is None:
            return False
        return datetime.utcnow() > self.expires_at
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'data': self.data,
            'created_at': self.created_at.isoformat(),
            'expires_at': self.expires_at.isoformat() if self.expires_at else None,
            'access_count': self.access_count,
            'last_accessed': self.last_accessed.isoformat() if self.last_accessed else None,
            'tags': self.tags or []
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CacheEntry':
        """Create from dictionary."""
        return cls(
            data=data['data'],
            created_at=datetime.fromisoformat(data['created_at']),
            expires_at=datetime.fromisoformat(data['expires_at']) if data.get('expires_at') else None,
            access_count=data.get('access_count', 0),
            last_accessed=datetime.fromisoformat(data['last_accessed']) if data.get('last_accessed') else None,
            tags=data.get('tags', [])
        )


class CacheBackend(ABC):
    """Abstract cache backend interface."""
    
    @abstractmethod
    def get(self, key: CacheKey) -> Optional[CacheEntry]:
        """Get cache entry by key."""
        pass
    
    @abstractmethod
    def set(self, key: CacheKey, entry: CacheEntry, ttl: Optional[int] = None) -> bool:
        """Set cache entry."""
        pass
    
    @abstractmethod
    def delete(self, key: CacheKey) -> bool:
        """Delete cache entry."""
        pass
    
    @abstractmethod
    def clear(self, pattern: Optional[str] = None) -> int:
        """Clear cache entries matching pattern."""
        pass
    
    @abstractmethod
    def exists(self, key: CacheKey) -> bool:
        """Check if key exists."""
        pass


class RedisBackend(CacheBackend):
    """Redis cache backend with advanced features."""
    
    def __init__(self, redis_client: redis.Redis, default_ttl: int = 3600):
        self.redis = redis_client
        self.default_ttl = default_ttl
    
    def get(self, key: CacheKey) -> Optional[CacheEntry]:
        """Get cache entry from Redis."""
        try:
            raw_data = self.redis.get(key.to_string())
            if raw_data is None:
                return None
            
            data = json.loads(raw_data)
            entry = CacheEntry.from_dict(data)
            
            if entry.is_expired():
                self.delete(key)
                return None
            
            # Update access statistics
            entry.access_count += 1
            entry.last_accessed = datetime.utcnow()
            self.set(key, entry)
            
            return entry
            
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.warning(f"Failed to deserialize cache entry {key.to_string()}: {e}")
            self.delete(key)
            return None
    
    def set(self, key: CacheKey, entry: CacheEntry, ttl: Optional[int] = None) -> bool:
        """Set cache entry in Redis."""
        try:
            ttl = ttl or self.default_ttl
            data = json.dumps(entry.to_dict(), default=str)
            
            result = self.redis.setex(key.to_string(), ttl, data)
            
            # Add to tag sets for invalidation
            if entry.tags:
                pipe = self.redis.pipeline()
                for tag in entry.tags:
                    pipe.sadd(f"tag:{tag}", key.to_string())
                    pipe.expire(f"tag:{tag}", ttl)
                pipe.execute()
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to set cache entry {key.to_string()}: {e}")
            return False
    
    def delete(self, key: CacheKey) -> bool:
        """Delete cache entry from Redis."""
        try:
            # Remove from tag sets
            entry = self.get(key)
            if entry and entry.tags:
                pipe = self.redis.pipeline()
                for tag in entry.tags:
                    pipe.srem(f"tag:{tag}", key.to_string())
                pipe.execute()
            
            result = self.redis.delete(key.to_string())
            return result > 0
            
        except Exception as e:
            logger.error(f"Failed to delete cache entry {key.to_string()}: {e}")
            return False
    
    def clear(self, pattern: Optional[str] = None) -> int:
        """Clear cache entries matching pattern."""
        try:
            if pattern is None:
                pattern = "*"
            
            keys = self.redis.keys(pattern)
            if keys:
                return self.redis.delete(*keys)
            return 0
            
        except Exception as e:
            logger.error(f"Failed to clear cache with pattern {pattern}: {e}")
            return 0
    
    def exists(self, key: CacheKey) -> bool:
        """Check if key exists in Redis."""
        return bool(self.redis.exists(key.to_string()))
    
    def invalidate_by_tags(self, tags: List[str]) -> int:
        """Invalidate all cache entries with given tags."""
        deleted_count = 0
        
        for tag in tags:
            tag_key = f"tag:{tag}"
            cached_keys = self.redis.smembers(tag_key)
            
            if cached_keys:
                # Delete cache entries
                deleted_count += self.redis.delete(*cached_keys)
                # Delete tag set
                self.redis.delete(tag_key)
        
        return deleted_count


class MemoryBackend(CacheBackend):
    """In-memory cache backend for testing or single-instance deployments."""
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        self.cache: Dict[str, CacheEntry] = {}
        self.max_size = max_size
        self.default_ttl = default_ttl
    
    def get(self, key: CacheKey) -> Optional[CacheEntry]:
        """Get cache entry from memory."""
        entry = self.cache.get(key.to_string())
        if entry is None:
            return None
        
        if entry.is_expired():
            del self.cache[key.to_string()]
            return None
        
        entry.access_count += 1
        entry.last_accessed = datetime.utcnow()
        return entry
    
    def set(self, key: CacheKey, entry: CacheEntry, ttl: Optional[int] = None) -> bool:
        """Set cache entry in memory."""
        # Evict if at capacity
        if len(self.cache) >= self.max_size:
            self._evict_lru()
        
        if ttl and entry.expires_at is None:
            entry.expires_at = datetime.utcnow() + timedelta(seconds=ttl)
        
        self.cache[key.to_string()] = entry
        return True
    
    def delete(self, key: CacheKey) -> bool:
        """Delete cache entry from memory."""
        return self.cache.pop(key.to_string(), None) is not None
    
    def clear(self, pattern: Optional[str] = None) -> int:
        """Clear cache entries matching pattern."""
        if pattern is None:
            count = len(self.cache)
            self.cache.clear()
            return count
        
        # Simple pattern matching (only supports wildcards)
        import fnmatch
        to_delete = [key for key in self.cache.keys() if fnmatch.fnmatch(key, pattern)]
        for key in to_delete:
            del self.cache[key]
        return len(to_delete)
    
    def exists(self, key: CacheKey) -> bool:
        """Check if key exists in memory."""
        return key.to_string() in self.cache
    
    def _evict_lru(self):
        """Evict least recently used entry."""
        if not self.cache:
            return
        
        lru_key = min(
            self.cache.keys(),
            key=lambda k: self.cache[k].last_accessed or self.cache[k].created_at
        )
        del self.cache[lru_key]


class CacheManager:
    """High-level cache manager with intelligent caching strategies."""
    
    def __init__(self, backend: CacheBackend, metrics_enabled: bool = True):
        self.backend = backend
        self.metrics_enabled = metrics_enabled
        self.metrics = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0,
            'errors': 0
        }
    
    def get(self, key: Union[str, CacheKey], default: Any = None) -> Any:
        """Get cached value."""
        if isinstance(key, str):
            key = CacheKey("app", key)
        
        try:
            entry = self.backend.get(key)
            if entry is not None:
                self._record_hit()
                return entry.data
            else:
                self._record_miss()
                return default
                
        except Exception as e:
            logger.error(f"Cache get error for key {key.to_string()}: {e}")
            self._record_error()
            return default
    
    def set(self, 
            key: Union[str, CacheKey], 
            value: Any, 
            ttl: Optional[int] = None,
            tags: Optional[List[str]] = None) -> bool:
        """Set cached value."""
        if isinstance(key, str):
            key = CacheKey("app", key)
        
        try:
            entry = CacheEntry(
                data=value,
                created_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(seconds=ttl) if ttl else None,
                tags=tags or []
            )
            
            result = self.backend.set(key, entry, ttl)
            if result:
                self._record_set()
            return result
            
        except Exception as e:
            logger.error(f"Cache set error for key {key.to_string()}: {e}")
            self._record_error()
            return False
    
    def delete(self, key: Union[str, CacheKey]) -> bool:
        """Delete cached value."""
        if isinstance(key, str):
            key = CacheKey("app", key)
        
        try:
            result = self.backend.delete(key)
            if result:
                self._record_delete()
            return result
            
        except Exception as e:
            logger.error(f"Cache delete error for key {key.to_string()}: {e}")
            self._record_error()
            return False
    
    def cached(self, 
               key_func: Optional[Callable] = None,
               ttl: int = 3600,
               tags: Optional[List[str]] = None,
               key_prefix: str = "func"):
        """Decorator for caching function results."""
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # Generate cache key
                if key_func:
                    cache_key_str = key_func(*args, **kwargs)
                else:
                    cache_key_str = self._generate_function_key(func, *args, **kwargs)
                
                cache_key = CacheKey(key_prefix, cache_key_str)
                
                # Try to get from cache
                cached_result = self.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # Execute function and cache result
                result = func(*args, **kwargs)
                self.set(cache_key, result, ttl=ttl, tags=tags)
                
                return result
            
            return wrapper
        return decorator
    
    def invalidate_by_tags(self, tags: List[str]) -> int:
        """Invalidate cache entries by tags."""
        if hasattr(self.backend, 'invalidate_by_tags'):
            return self.backend.invalidate_by_tags(tags)
        
        # Fallback for backends without tag support
        logger.warning("Backend does not support tag-based invalidation")
        return 0
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get cache performance metrics."""
        total_operations = self.metrics['hits'] + self.metrics['misses']
        hit_rate = self.metrics['hits'] / total_operations if total_operations > 0 else 0
        
        return {
            **self.metrics,
            'hit_rate': hit_rate,
            'total_operations': total_operations
        }
    
    def _generate_function_key(self, func: Callable, *args, **kwargs) -> str:
        """Generate cache key for function call."""
        # Create a hash of function name and arguments
        key_data = {
            'function': func.__name__,
            'args': args,
            'kwargs': sorted(kwargs.items())
        }
        
        key_string = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _record_hit(self):
        if self.metrics_enabled:
            self.metrics['hits'] += 1
    
    def _record_miss(self):
        if self.metrics_enabled:
            self.metrics['misses'] += 1
    
    def _record_set(self):
        if self.metrics_enabled:
            self.metrics['sets'] += 1
    
    def _record_delete(self):
        if self.metrics_enabled:
            self.metrics['deletes'] += 1
    
    def _record_error(self):
        if self.metrics_enabled:
            self.metrics['errors'] += 1


class QueryCache:
    """Specialized cache for SQLAlchemy query results."""
    
    def __init__(self, cache_manager: CacheManager, default_ttl: int = 300):
        self.cache_manager = cache_manager
        self.default_ttl = default_ttl
    
    def cached_query(self, 
                     ttl: Optional[int] = None,
                     tags: Optional[List[str]] = None,
                     key_func: Optional[Callable] = None):
        """Decorator for caching query results."""
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # Extract session from arguments
                session = None
                for arg in args:
                    if isinstance(arg, Session):
                        session = arg
                        break
                
                if session is None:
                    # No session found, execute without caching
                    logger.warning(f"No SQLAlchemy session found for {func.__name__}, skipping cache")
                    return func(*args, **kwargs)
                
                # Generate cache key
                if key_func:
                    cache_key_str = key_func(*args, **kwargs)
                else:
                    cache_key_str = self._generate_query_key(func, *args, **kwargs)
                
                cache_key = CacheKey("query", cache_key_str)
                
                # Try to get from cache
                cached_result = self.cache_manager.get(cache_key)
                if cached_result is not None:
                    logger.debug(f"Cache hit for query {func.__name__}")
                    return cached_result
                
                # Execute query and cache result
                logger.debug(f"Cache miss for query {func.__name__}, executing")
                result = func(*args, **kwargs)
                
                # Cache the result
                cache_ttl = ttl or self.default_ttl
                self.cache_manager.set(cache_key, result, ttl=cache_ttl, tags=tags)
                
                return result
            
            return wrapper
        return decorator
    
    def invalidate_model(self, model_class, operation: str = None):
        """Invalidate cache for specific model."""
        model_name = model_class.__name__.lower()
        tags = [f"model:{model_name}"]
        
        if operation:
            tags.append(f"model:{model_name}:{operation}")
        
        return self.cache_manager.invalidate_by_tags(tags)
    
    def _generate_query_key(self, func: Callable, *args, **kwargs) -> str:
        """Generate cache key for query function."""
        # Filter out session from arguments for key generation
        filtered_args = [arg for arg in args if not isinstance(arg, Session)]
        
        key_data = {
            'function': func.__name__,
            'args': filtered_args,
            'kwargs': sorted(kwargs.items())
        }
        
        key_string = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_string.encode()).hexdigest()


class CacheInvalidator:
    """Automatic cache invalidation based on database changes."""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.invalidation_rules: Dict[str, List[str]] = {}
    
    def register_invalidation_rule(self, model_name: str, tags: List[str]):
        """Register invalidation rule for model changes."""
        self.invalidation_rules[model_name] = tags
    
    def setup_sqlalchemy_events(self, session_factory):
        """Set up SQLAlchemy events for automatic invalidation."""
        @event.listens_for(session_factory, "after_commit")
        def after_commit(session):
            """Handle cache invalidation after successful commit."""
            if hasattr(session, '_cache_invalidation_tags'):
                tags = session._cache_invalidation_tags
                if tags:
                    self.cache_manager.invalidate_by_tags(list(tags))
                    logger.debug(f"Invalidated cache tags after commit: {tags}")
                session._cache_invalidation_tags.clear()
        
        @event.listens_for(session_factory, "after_rollback")
        def after_rollback(session):
            """Clear invalidation tags after rollback."""
            if hasattr(session, '_cache_invalidation_tags'):
                session._cache_invalidation_tags.clear()
    
    def mark_for_invalidation(self, session: Session, model_instance, operation: str):
        """Mark cache entries for invalidation."""
        if not hasattr(session, '_cache_invalidation_tags'):
            session._cache_invalidation_tags = set()
        
        model_name = model_instance.__class__.__name__.lower()
        
        # Add model-specific tags
        tags = [
            f"model:{model_name}",
            f"model:{model_name}:{operation}",
            f"instance:{model_name}:{model_instance.id}" if hasattr(model_instance, 'id') else None
        ]
        
        # Add custom invalidation rules
        if model_name in self.invalidation_rules:
            tags.extend(self.invalidation_rules[model_name])
        
        # Filter out None values
        tags = [tag for tag in tags if tag is not None]
        
        session._cache_invalidation_tags.update(tags)


# Global cache instances
cache_backend = None
cache_manager = None
query_cache = None
cache_invalidator = None


def setup_caching(redis_url: str = None, backend_type: str = "redis"):
    """Set up caching system."""
    global cache_backend, cache_manager, query_cache, cache_invalidator
    
    if backend_type == "redis" and redis_url:
        redis_client = redis.from_url(redis_url, decode_responses=True)
        cache_backend = RedisBackend(redis_client)
    else:
        cache_backend = MemoryBackend()
    
    cache_manager = CacheManager(cache_backend)
    query_cache = QueryCache(cache_manager)
    cache_invalidator = CacheInvalidator(cache_manager)
    
    logger.info(f"Caching system initialized with {backend_type} backend")


# Utility functions for common caching patterns
def cache_user_data(user_id: int, ttl: int = 1800):
    """Cache user data with automatic invalidation."""
    def decorator(func):
        return cache_manager.cached(
            key_func=lambda *args, **kwargs: f"user:{user_id}:data",
            ttl=ttl,
            tags=[f"user:{user_id}", "user_data"]
        )(func)
    return decorator


def cache_post_data(post_id: int, ttl: int = 3600):
    """Cache post data with automatic invalidation."""
    def decorator(func):
        return cache_manager.cached(
            key_func=lambda *args, **kwargs: f"post:{post_id}:data",
            ttl=ttl,
            tags=[f"post:{post_id}", "post_data"]
        )(func)
    return decorator


def cache_expensive_aggregation(ttl: int = 7200):
    """Cache expensive aggregation queries."""
    def decorator(func):
        return query_cache.cached_query(
            ttl=ttl,
            tags=["aggregation", "reports"]
        )(func)
    return decorator
```

This completes Section 7.2.2 - Caching Strategies. The implementation includes multi-level caching, intelligent invalidation, and production-ready patterns. Should I continue with Section 7.3 - Deployment and Production Configuration to complete Section 7?


### 7.3 Production Deployment and Configuration

#### 7.3.1 Environment Configuration Management

**config/environments.py** - Production-Ready Configuration:
```python
"""
Environment-specific configuration for production deployment with security,
monitoring, and scalability considerations.
"""

import os
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from urllib.parse import urlparse
from pathlib import Path

import yaml
from sqlalchemy import create_engine
from sqlalchemy.engine import Engine
from sqlalchemy.pool import QueuePool, NullPool


@dataclass
class DatabaseConfig:
    """Database configuration with connection pooling and optimization."""
    url: str
    pool_size: int = 20
    max_overflow: int = 0
    pool_pre_ping: bool = True
    pool_recycle: int = 3600
    echo: bool = False
    connect_args: Dict[str, Any] = field(default_factory=dict)
    
    # Read replicas for read-heavy workloads
    read_replicas: List[str] = field(default_factory=list)
    
    # Connection timeouts
    pool_timeout: int = 30
    connect_timeout: int = 10
    
    def create_engine(self, **kwargs) -> Engine:
        """Create SQLAlchemy engine with optimized settings."""
        engine_kwargs = {
            'url': self.url,
            'poolclass': QueuePool,
            'pool_size': self.pool_size,
            'max_overflow': self.max_overflow,
            'pool_pre_ping': self.pool_pre_ping,
            'pool_recycle': self.pool_recycle,
            'pool_timeout': self.pool_timeout,
            'echo': self.echo,
            'connect_args': {
                **self.connect_args,
                'connect_timeout': self.connect_timeout,
            },
            **kwargs
        }
        
        return create_engine(**engine_kwargs)
    
    def create_read_engine(self) -> Optional[Engine]:
        """Create read-only engine for read replicas."""
        if not self.read_replicas:
            return None
        
        # Simple round-robin selection
        read_url = self.read_replicas[0]
        
        return create_engine(
            read_url,
            poolclass=QueuePool,
            pool_size=self.pool_size // 2,  # Smaller pool for reads
            max_overflow=self.max_overflow,
            pool_pre_ping=self.pool_pre_ping,
            pool_recycle=self.pool_recycle,
            echo=self.echo,
            connect_args=self.connect_args
        )


@dataclass
class RedisConfig:
    """Redis configuration for caching and sessions."""
    url: str
    db: int = 0
    max_connections: int = 50
    retry_on_timeout: bool = True
    health_check_interval: int = 30
    socket_connect_timeout: int = 5
    socket_timeout: int = 5
    
    # Sentinel configuration for high availability
    sentinel_hosts: List[str] = field(default_factory=list)
    sentinel_service_name: str = "mymaster"


@dataclass
class SecurityConfig:
    """Security configuration settings."""
    secret_key: str
    jwt_secret_key: str
    password_salt: str
    
    # Token expiration times
    access_token_expire_minutes: int = 30
    refresh_token_expire_days: int = 30
    
    # Password requirements
    min_password_length: int = 8
    require_special_chars: bool = True
    
    # Rate limiting
    rate_limit_enabled: bool = True
    rate_limit_requests: int = 100
    rate_limit_window: int = 3600  # 1 hour
    
    # CORS settings
    cors_origins: List[str] = field(default_factory=list)
    cors_credentials: bool = True


@dataclass
class LoggingConfig:
    """Logging configuration for production monitoring."""
    level: str = "INFO"
    format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # File logging
    log_file: Optional[str] = None
    max_file_size: int = 10 * 1024 * 1024  # 10MB
    backup_count: int = 5
    
    # Structured logging
    json_logging: bool = True
    
    # External logging services
    sentry_dsn: Optional[str] = None
    datadog_api_key: Optional[str] = None
    
    # Database query logging
    slow_query_threshold: float = 1.0  # seconds
    log_all_queries: bool = False


@dataclass
class MonitoringConfig:
    """Monitoring and metrics configuration."""
    enabled: bool = True
    
    # Prometheus metrics
    prometheus_port: int = 9090
    metrics_path: str = "/metrics"
    
    # Health check endpoints
    health_check_path: str = "/health"
    readiness_check_path: str = "/ready"
    
    # Performance monitoring
    request_tracking: bool = True
    database_monitoring: bool = True
    cache_monitoring: bool = True
    
    # Alerting thresholds
    max_response_time: float = 2.0  # seconds
    max_error_rate: float = 0.05  # 5%
    max_cpu_usage: float = 0.8  # 80%
    max_memory_usage: float = 0.8  # 80%


class EnvironmentConfig:
    """Main configuration class with environment-specific settings."""
    
    def __init__(self, environment: str = None):
        self.environment = environment or os.getenv("ENVIRONMENT", "development")
        self.debug = self.environment == "development"
        
        # Load configuration
        self._load_config()
        
        # Initialize subsystem configs
        self.database = self._init_database_config()
        self.redis = self._init_redis_config()
        self.security = self._init_security_config()
        self.logging = self._init_logging_config()
        self.monitoring = self._init_monitoring_config()
        
        # Setup logging
        self._setup_logging()
    
    def _load_config(self):
        """Load configuration from files and environment variables."""
        self.config_data = {}
        
        # Load from YAML config file
        config_file = Path(f"config/{self.environment}.yaml")
        if config_file.exists():
            with open(config_file) as f:
                self.config_data = yaml.safe_load(f) or {}
        
        # Override with environment variables
        self._load_env_variables()
    
    def _load_env_variables(self):
        """Load configuration from environment variables."""
        env_mappings = {
            "DATABASE_URL": ("database", "url"),
            "REDIS_URL": ("redis", "url"),
            "SECRET_KEY": ("security", "secret_key"),
            "JWT_SECRET_KEY": ("security", "jwt_secret_key"),
            "SENTRY_DSN": ("logging", "sentry_dsn"),
            "LOG_LEVEL": ("logging", "level"),
        }
        
        for env_var, (section, key) in env_mappings.items():
            value = os.getenv(env_var)
            if value:
                if section not in self.config_data:
                    self.config_data[section] = {}
                self.config_data[section][key] = value
    
    def _init_database_config(self) -> DatabaseConfig:
        """Initialize database configuration."""
        db_config = self.config_data.get("database", {})
        
        # Production optimizations
        if self.environment == "production":
            db_config.setdefault("pool_size", 20)
            db_config.setdefault("max_overflow", 0)
            db_config.setdefault("pool_recycle", 3600)
            db_config.setdefault("echo", False)
        
        # Development settings
        elif self.environment == "development":
            db_config.setdefault("pool_size", 5)
            db_config.setdefault("echo", True)
        
        # Test settings
        elif self.environment == "test":
            db_config.setdefault("pool_size", 1)
            db_config.setdefault("poolclass", NullPool)
        
        return DatabaseConfig(**db_config)
    
    def _init_redis_config(self) -> RedisConfig:
        """Initialize Redis configuration."""
        redis_config = self.config_data.get("redis", {})
        
        if self.environment == "production":
            redis_config.setdefault("max_connections", 50)
            redis_config.setdefault("health_check_interval", 30)
        
        return RedisConfig(**redis_config)
    
    def _init_security_config(self) -> SecurityConfig:
        """Initialize security configuration."""
        security_config = self.config_data.get("security", {})
        
        # Ensure required secrets are present
        required_secrets = ["secret_key", "jwt_secret_key"]
        for secret in required_secrets:
            if secret not in security_config:
                if self.environment == "production":
                    raise ValueError(f"Missing required secret: {secret}")
                else:
                    # Generate development secrets
                    import secrets
                    security_config[secret] = secrets.token_urlsafe(32)
        
        return SecurityConfig(**security_config)
    
    def _init_logging_config(self) -> LoggingConfig:
        """Initialize logging configuration."""
        logging_config = self.config_data.get("logging", {})
        
        if self.environment == "production":
            logging_config.setdefault("level", "INFO")
            logging_config.setdefault("json_logging", True)
            logging_config.setdefault("log_file", "/var/log/app/app.log")
        
        elif self.environment == "development":
            logging_config.setdefault("level", "DEBUG")
            logging_config.setdefault("json_logging", False)
        
        return LoggingConfig(**logging_config)
    
    def _init_monitoring_config(self) -> MonitoringConfig:
        """Initialize monitoring configuration."""
        monitoring_config = self.config_data.get("monitoring", {})
        
        if self.environment == "production":
            monitoring_config.setdefault("enabled", True)
            monitoring_config.setdefault("request_tracking", True)
            monitoring_config.setdefault("database_monitoring", True)
        
        elif self.environment in ["development", "test"]:
            monitoring_config.setdefault("enabled", False)
        
        return MonitoringConfig(**monitoring_config)
    
    def _setup_logging(self):
        """Setup logging configuration."""
        import logging.config
        
        if self.logging.json_logging:
            # Structured JSON logging for production
            log_config = {
                'version': 1,
                'disable_existing_loggers': False,
                'formatters': {
                    'json': {
                        'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',
                        'format': '%(asctime)s %(name)s %(levelname)s %(message)s'
                    }
                },
                'handlers': {
                    'console': {
                        'class': 'logging.StreamHandler',
                        'formatter': 'json',
                        'level': self.logging.level,
                    }
                },
                'root': {
                    'level': self.logging.level,
                    'handlers': ['console']
                }
            }
            
            # Add file handler if specified
            if self.logging.log_file:
                log_config['handlers']['file'] = {
                    'class': 'logging.handlers.RotatingFileHandler',
                    'filename': self.logging.log_file,
                    'maxBytes': self.logging.max_file_size,
                    'backupCount': self.logging.backup_count,
                    'formatter': 'json',
                    'level': self.logging.level,
                }
                log_config['root']['handlers'].append('file')
        
        else:
            # Simple text logging for development
            log_config = {
                'version': 1,
                'disable_existing_loggers': False,
                'formatters': {
                    'simple': {
                        'format': self.logging.format
                    }
                },
                'handlers': {
                    'console': {
                        'class': 'logging.StreamHandler',
                        'formatter': 'simple',
                        'level': self.logging.level,
                    }
                },
                'root': {
                    'level': self.logging.level,
                    'handlers': ['console']
                }
            }
        
        logging.config.dictConfig(log_config)
        
        # Setup Sentry if configured
        if self.logging.sentry_dsn:
            try:
                import sentry_sdk
                from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration
                from sentry_sdk.integrations.logging import LoggingIntegration
                
                sentry_sdk.init(
                    dsn=self.logging.sentry_dsn,
                    environment=self.environment,
                    integrations=[
                        SqlalchemyIntegration(),
                        LoggingIntegration(level=logging.INFO, event_level=logging.ERROR)
                    ],
                    traces_sample_rate=0.1 if self.environment == "production" else 1.0,
                )
            except ImportError:
                logging.warning("Sentry SDK not installed, skipping Sentry setup")
    
    def get_database_url(self, read_only: bool = False) -> str:
        """Get database URL for connections."""
        if read_only and self.database.read_replicas:
            return self.database.read_replicas[0]
        return self.database.url
    
    def get_redis_url(self) -> str:
        """Get Redis URL for connections."""
        return self.redis.url
    
    def is_production(self) -> bool:
        """Check if running in production environment."""
        return self.environment == "production"
    
    def is_development(self) -> bool:
        """Check if running in development environment."""
        return self.environment == "development"
    
    def is_testing(self) -> bool:
        """Check if running in test environment."""
        return self.environment == "test"


# Global configuration instance
config = None


def get_config() -> EnvironmentConfig:
    """Get the global configuration instance."""
    global config
    if config is None:
        config = EnvironmentConfig()
    return config


def init_config(environment: str = None) -> EnvironmentConfig:
    """Initialize configuration for specific environment."""
    global config
    config = EnvironmentConfig(environment)
    return config
```

**config/production.yaml** - Production Configuration:
```yaml
database:
  url: "postgresql+psycopg2://user:password@localhost:5432/production_db"
  pool_size: 20
  max_overflow: 0
  pool_recycle: 3600
  pool_pre_ping: true
  connect_args:
    sslmode: require
    connect_timeout: 10
  read_replicas:
    - "postgresql+psycopg2://user:password@read-replica-1:5432/production_db"
    - "postgresql+psycopg2://user:password@read-replica-2:5432/production_db"

redis:
  url: "redis://redis-cluster:6379/0"
  max_connections: 50
  health_check_interval: 30
  sentinel_hosts:
    - "sentinel-1:26379"
    - "sentinel-2:26379"
    - "sentinel-3:26379"
  sentinel_service_name: "mymaster"

security:
  secret_key: "${SECRET_KEY}"
  jwt_secret_key: "${JWT_SECRET_KEY}"
  password_salt: "${PASSWORD_SALT}"
  access_token_expire_minutes: 15
  refresh_token_expire_days: 7
  rate_limit_enabled: true
  rate_limit_requests: 1000
  rate_limit_window: 3600
  cors_origins:
    - "https://myapp.com"
    - "https://www.myapp.com"

logging:
  level: "INFO"
  json_logging: true
  log_file: "/var/log/app/app.log"
  sentry_dsn: "${SENTRY_DSN}"
  slow_query_threshold: 1.0

monitoring:
  enabled: true
  prometheus_port: 9090
  request_tracking: true
  database_monitoring: true
  cache_monitoring: true
  max_response_time: 2.0
  max_error_rate: 0.05
```

This completes the production deployment configuration system. Should I continue with Section 7.3.2 - Container Deployment and Orchestration to finish Section 7?


#### 7.3.2 Container Deployment and Health Monitoring

**Dockerfile** - Production-Ready Container:
```dockerfile
FROM python:3.11-slim as base

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Set work directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt requirements-prod.txt ./
RUN pip install --upgrade pip && \
    pip install -r requirements-prod.txt

# Copy application code
COPY . .

# Create log directory
RUN mkdir -p /var/log/app && chown -R appuser:appuser /var/log/app

# Change ownership of app directory
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Default command
CMD ["gunicorn", "--config", "gunicorn.conf.py", "app.main:app"]
```

**gunicorn.conf.py** - Production WSGI Configuration:
```python
"""
Production Gunicorn configuration with optimized settings for SQLAlchemy applications.
"""

import multiprocessing
import os

# Server socket
bind = "0.0.0.0:8000"
backlog = 2048

# Worker processes
workers = multiprocessing.cpu_count() * 2 + 1
worker_class = "gevent"
worker_connections = 1000
max_requests = 1000
max_requests_jitter = 50
preload_app = True

# Timeout settings
timeout = 30
keepalive = 2
graceful_timeout = 30

# Process naming
proc_name = "sqlalchemy-app"

# Logging
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'
accesslog = "/var/log/app/access.log"
errorlog = "/var/log/app/error.log"
loglevel = "info"
capture_output = True
enable_stdio_inheritance = True

# Process management
pidfile = "/var/run/gunicorn.pid"
user = "appuser"
group = "appuser"
tmp_upload_dir = None

# Application settings
forwarded_allow_ips = "*"
secure_scheme_headers = {
    'X-FORWARDED-PROTOCOL': 'ssl',
    'X-FORWARDED-PROTO': 'https',
    'X-FORWARDED-SSL': 'on'
}


def when_ready(server):
    """Called just after the server is started."""
    server.log.info("Server is ready. Spawning workers")


def worker_int(worker):
    """Called just after a worker has been forked."""
    worker.log.info("Worker spawned (pid: %s)", worker.pid)


def pre_fork(server, worker):
    """Called just before a worker is forked."""
    server.log.info("Worker about to be forked (pid: %s)", worker.pid)


def post_fork(server, worker):
    """Called just after a worker has been forked."""
    # Initialize database connections per worker
    from app.database import init_db
    init_db()
    
    server.log.info("Worker spawned successfully (pid: %s)", worker.pid)


def pre_exec(server):
    """Called just before a new master process is forked."""
    server.log.info("Forked child, re-executing.")


def on_exit(server):
    """Called just before exiting."""
    server.log.info("Shutting down: %s", server.proc_name)


def worker_abort(worker):
    """Called when a worker receives the SIGABRT signal."""
    worker.log.info("Worker received SIGABRT signal")
```

**docker-compose.yml** - Complete Production Stack:
```yaml
version: '3.8'

services:
  app:
    build: .
    image: sqlalchemy-app:latest
    container_name: sqlalchemy-app
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - DATABASE_URL=postgresql+psycopg2://postgres:password@db:5432/production_db
      - REDIS_URL=redis://redis:6379/0
      - SECRET_KEY=${SECRET_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - SENTRY_DSN=${SENTRY_DSN}
    volumes:
      - ./logs:/var/log/app
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  db:
    image: postgres:15-alpine
    container_name: postgres-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: production_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=en_US.UTF-8 --lc-ctype=en_US.UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "5432:5432"
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis-cache
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - app
    networks:
      - app-network

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - app-network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - app-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  app-network:
    driver: bridge
```

**nginx/nginx.conf** - Production Nginx Configuration:
```nginx
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging format
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;

    # Performance settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 10M;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript 
               application/javascript application/xml+rss 
               application/json application/xml;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;

    # Upstream backend
    upstream app_backend {
        least_conn;
        server app:8000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }

    # HTTP to HTTPS redirect
    server {
        listen 80;
        server_name _;
        return 301 https://$host$request_uri;
    }

    # HTTPS server
    server {
        listen 443 ssl http2;
        server_name localhost;

        # SSL configuration
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_session_timeout 1d;
        ssl_session_cache shared:SSL:50m;
        ssl_session_tickets off;

        # Modern configuration
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;

        # HSTS
        add_header Strict-Transport-Security "max-age=63072000" always;

        # Security headers
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Referrer-Policy "strict-origin-when-cross-origin";

        # Main application
        location / {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://app_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
            
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
            proxy_busy_buffers_size 8k;
        }

        # Login endpoint with stricter rate limiting
        location /auth/login {
            limit_req zone=login burst=5 nodelay;
            
            proxy_pass http://app_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Health check endpoint
        location /health {
            access_log off;
            proxy_pass http://app_backend;
            proxy_set_header Host $host;
        }

        # Metrics endpoint (restrict access)
        location /metrics {
            allow 10.0.0.0/8;
            allow 172.16.0.0/12;
            allow 192.168.0.0/16;
            deny all;
            
            proxy_pass http://app_backend;
            proxy_set_header Host $host;
        }

        # Static files (if serving directly)
        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

**monitoring/prometheus.yml** - Prometheus Configuration:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets: []

rule_files: []

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'sqlalchemy-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['db:5432']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:80']
```

**scripts/deploy.sh** - Deployment Script:
```bash
#!/bin/bash
set -e

# Configuration
APP_NAME="sqlalchemy-app"
COMPOSE_FILE="docker-compose.yml"
BACKUP_DIR="/backups"
LOG_FILE="/var/log/deploy.log"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" >> "$LOG_FILE"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1" >> "$LOG_FILE"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1" >> "$LOG_FILE"
    exit 1
}

# Pre-deployment checks
pre_deploy_checks() {
    log "Running pre-deployment checks..."
    
    # Check if docker-compose is available
    if ! command -v docker-compose &> /dev/null; then
        error "docker-compose is not installed"
    fi
    
    # Check if .env file exists
    if [[ ! -f .env ]]; then
        error ".env file not found"
    fi
    
    # Validate environment variables
    source .env
    required_vars=("SECRET_KEY" "JWT_SECRET_KEY" "DATABASE_URL")
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var}" ]]; then
            error "Required environment variable $var is not set"
        fi
    done
    
    log "Pre-deployment checks passed"
}

# Backup database
backup_database() {
    log "Creating database backup..."
    
    mkdir -p "$BACKUP_DIR"
    backup_file="$BACKUP_DIR/backup_$(date +%Y%m%d_%H%M%S).sql"
    
    docker-compose exec -T db pg_dump -U postgres production_db > "$backup_file"
    
    if [[ $? -eq 0 ]]; then
        log "Database backup created: $backup_file"
        
        # Keep only last 7 backups
        find "$BACKUP_DIR" -name "backup_*.sql" -mtime +7 -delete
    else
        error "Database backup failed"
    fi
}

# Health check
health_check() {
    log "Performing health check..."
    
    local max_attempts=30
    local attempt=1
    
    while [[ $attempt -le $max_attempts ]]; do
        if curl -f http://localhost:8000/health &> /dev/null; then
            log "Health check passed"
            return 0
        fi
        
        log "Health check attempt $attempt/$max_attempts failed, retrying..."
        sleep 10
        ((attempt++))
    done
    
    error "Health check failed after $max_attempts attempts"
}

# Smoke tests
smoke_tests() {
    log "Running smoke tests..."
    
    # Test database connection
    if ! docker-compose exec -T app python -c "from app.database import engine; engine.execute('SELECT 1')"; then
        error "Database connection test failed"
    fi
    
    # Test Redis connection
    if ! docker-compose exec -T app python -c "from app.utils.caching import setup_caching; setup_caching()"; then
        error "Redis connection test failed"
    fi
    
    # Test API endpoints
    local api_tests=(
        "http://localhost:8000/health"
        "http://localhost:8000/metrics"
    )
    
    for endpoint in "${api_tests[@]}"; do
        if ! curl -f "$endpoint" &> /dev/null; then
            error "API test failed for $endpoint"
        fi
    done
    
    log "Smoke tests passed"
}

# Main deployment function
deploy() {
    log "Starting deployment of $APP_NAME..."
    
    # Pre-deployment
    pre_deploy_checks
    backup_database
    
    # Build and deploy
    log "Building new images..."
    docker-compose build --no-cache
    
    log "Stopping existing services..."
    docker-compose down
    
    log "Starting new services..."
    docker-compose up -d
    
    # Post-deployment
    health_check
    smoke_tests
    
    log "Deployment completed successfully!"
}

# Rollback function
rollback() {
    log "Starting rollback..."
    
    # Find latest backup
    local latest_backup=$(find "$BACKUP_DIR" -name "backup_*.sql" -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -d' ' -f2-)
    
    if [[ -z "$latest_backup" ]]; then
        error "No backup found for rollback"
    fi
    
    log "Rolling back to backup: $latest_backup"
    
    # Restore database
    docker-compose exec -T db psql -U postgres -d production_db < "$latest_backup"
    
    # Restart services
    docker-compose restart
    
    health_check
    
    log "Rollback completed successfully!"
}

# Main script logic
case "${1:-deploy}" in
    "deploy")
        deploy
        ;;
    "rollback")
        rollback
        ;;
    "health")
        health_check
        ;;
    "backup")
        backup_database
        ;;
    *)
        echo "Usage: $0 {deploy|rollback|health|backup}"
        exit 1
        ;;
esac
```

**Section 7 Summary**: We've built a comprehensive production deployment system with:
- **Testing Framework**: Complete test infrastructure with pytest, factories, and API testing
- **Performance Optimization**: Query profiling, connection monitoring, and caching strategies  
- **Production Configuration**: Environment-specific settings with security and monitoring
- **Container Deployment**: Docker, Nginx, monitoring stack, and automated deployment scripts

This completes Section 7! Now should I proceed with Section 8 as requested?


## 8. Advanced SQLAlchemy Patterns and Techniques

### 8.1 Advanced ORM Patterns

#### 8.1.1 Dynamic Model Generation and Metadata Introspection

**app/patterns/dynamic_models.py** - Dynamic Model Factory:
```python
"""
Advanced dynamic model generation and metadata introspection patterns
for building flexible, data-driven applications with SQLAlchemy.
"""

import re
import logging
from typing import Dict, Any, List, Optional, Type, Union, Callable
from datetime import datetime
from dataclasses import dataclass

from sqlalchemy import (
    create_engine, MetaData, Table, Column, Integer, String, Text, 
    DateTime, Boolean, Float, JSON, ForeignKey, Index, CheckConstraint,
    UniqueConstraint, inspect, text
)
from sqlalchemy.orm import declarative_base, relationship, Session, sessionmaker
from sqlalchemy.ext.declarative import declared_attr
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.sql import sqltypes
from sqlalchemy.schema import CreateTable
from sqlalchemy.dialects import postgresql, mysql, sqlite

logger = logging.getLogger(__name__)


@dataclass
class FieldDefinition:
    """Definition for a dynamic field."""
    name: str
    type: str
    nullable: bool = True
    default: Any = None
    length: Optional[int] = None
    precision: Optional[int] = None
    scale: Optional[int] = None
    foreign_key: Optional[str] = None
    unique: bool = False
    index: bool = False
    check_constraint: Optional[str] = None
    choices: Optional[List[str]] = None
    
    def to_column(self) -> Column:
        """Convert field definition to SQLAlchemy Column."""
        # Map string types to SQLAlchemy types
        type_mapping = {
            'string': String(self.length) if self.length else String(255),
            'text': Text,
            'integer': Integer,
            'float': Float(self.precision, self.scale) if self.precision else Float,
            'boolean': Boolean,
            'datetime': DateTime,
            'json': JSON,
        }
        
        if self.type not in type_mapping:
            raise ValueError(f"Unsupported field type: {self.type}")
        
        column_type = type_mapping[self.type]
        
        # Build column arguments
        column_args = [self.name, column_type]
        column_kwargs = {
            'nullable': self.nullable,
            'default': self.default,
            'unique': self.unique,
            'index': self.index,
        }
        
        # Add foreign key if specified
        if self.foreign_key:
            column_args.append(ForeignKey(self.foreign_key))
        
        # Remove None values
        column_kwargs = {k: v for k, v in column_kwargs.items() if v is not None}
        
        return Column(*column_args, **column_kwargs)


@dataclass
class ModelDefinition:
    """Definition for a dynamic model."""
    table_name: str
    fields: List[FieldDefinition]
    relationships: Optional[Dict[str, Dict[str, Any]]] = None
    indexes: Optional[List[Dict[str, Any]]] = None
    constraints: Optional[List[Dict[str, Any]]] = None
    mixins: Optional[List[Type]] = None
    
    def validate(self) -> bool:
        """Validate model definition."""
        # Check for required fields
        if not self.table_name:
            raise ValueError("Table name is required")
        
        if not self.fields:
            raise ValueError("At least one field is required")
        
        # Check for duplicate field names
        field_names = [field.name for field in self.fields]
        if len(field_names) != len(set(field_names)):
            raise ValueError("Duplicate field names found")
        
        # Validate table name format
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', self.table_name):
            raise ValueError("Invalid table name format")
        
        return True


class DynamicModelFactory:
    """Factory for creating dynamic SQLAlchemy models."""
    
    def __init__(self, base_class=None, metadata: MetaData = None):
        self.base_class = base_class or declarative_base()
        self.metadata = metadata or MetaData()
        self.created_models: Dict[str, Type] = {}
        self.model_registry: Dict[str, ModelDefinition] = {}
    
    def create_model(self, definition: ModelDefinition) -> Type:
        """Create a dynamic model from definition."""
        definition.validate()
        
        if definition.table_name in self.created_models:
            return self.created_models[definition.table_name]
        
        # Build class attributes
        class_attrs = {
            '__tablename__': definition.table_name,
            '__table_args__': self._build_table_args(definition),
        }
        
        # Add fields as columns
        for field in definition.fields:
            class_attrs[field.name] = field.to_column()
        
        # Add relationships
        if definition.relationships:
            for rel_name, rel_config in definition.relationships.items():
                class_attrs[rel_name] = relationship(**rel_config)
        
        # Add mixins
        bases = [self.base_class]
        if definition.mixins:
            bases.extend(definition.mixins)
        
        # Create the class
        model_class = type(
            f"Dynamic{definition.table_name.title()}",
            tuple(bases),
            class_attrs
        )
        
        # Store references
        self.created_models[definition.table_name] = model_class
        self.model_registry[definition.table_name] = definition
        
        logger.info(f"Created dynamic model: {model_class.__name__}")
        return model_class
    
    def _build_table_args(self, definition: ModelDefinition) -> tuple:
        """Build __table_args__ for the model."""
        table_args = []
        
        # Add indexes
        if definition.indexes:
            for index_def in definition.indexes:
                table_args.append(Index(**index_def))
        
        # Add constraints
        if definition.constraints:
            for constraint_def in definition.constraints:
                constraint_type = constraint_def.pop('type', 'check')
                if constraint_type == 'check':
                    table_args.append(CheckConstraint(**constraint_def))
                elif constraint_type == 'unique':
                    table_args.append(UniqueConstraint(**constraint_def))
        
        return tuple(table_args)
    
    def create_from_json(self, json_definition: Dict[str, Any]) -> Type:
        """Create model from JSON definition."""
        # Parse fields
        fields = []
        for field_data in json_definition.get('fields', []):
            fields.append(FieldDefinition(**field_data))
        
        # Create model definition
        definition = ModelDefinition(
            table_name=json_definition['table_name'],
            fields=fields,
            relationships=json_definition.get('relationships'),
            indexes=json_definition.get('indexes'),
            constraints=json_definition.get('constraints')
        )
        
        return self.create_model(definition)
    
    def get_model(self, table_name: str) -> Optional[Type]:
        """Get existing model by table name."""
        return self.created_models.get(table_name)
    
    def list_models(self) -> List[str]:
        """List all created model names."""
        return list(self.created_models.keys())
    
    def create_tables(self, engine, checkfirst: bool = True):
        """Create all dynamic tables in the database."""
        for model_class in self.created_models.values():
            model_class.metadata.create_all(engine, checkfirst=checkfirst)


class MetadataInspector:
    """Utility for inspecting and analyzing database metadata."""
    
    def __init__(self, engine):
        self.engine = engine
        self.inspector = inspect(engine)
    
    def introspect_table(self, table_name: str) -> ModelDefinition:
        """Introspect existing table and create model definition."""
        if not self.inspector.has_table(table_name):
            raise ValueError(f"Table {table_name} does not exist")
        
        # Get column information
        columns = self.inspector.get_columns(table_name)
        fields = []
        
        for column in columns:
            field = self._column_to_field_definition(column)
            fields.append(field)
        
        # Get foreign keys
        foreign_keys = self.inspector.get_foreign_keys(table_name)
        for fk in foreign_keys:
            for i, col_name in enumerate(fk['constrained_columns']):
                # Find the field and update with foreign key
                for field in fields:
                    if field.name == col_name:
                        field.foreign_key = f"{fk['referred_table']}.{fk['referred_columns'][i]}"
        
        # Get indexes
        indexes = self.inspector.get_indexes(table_name)
        index_definitions = []
        for index in indexes:
            if not index['unique']:  # Unique constraints handled separately
                index_definitions.append({
                    'name': index['name'],
                    'columns': index['column_names']
                })
        
        # Get unique constraints
        unique_constraints = self.inspector.get_unique_constraints(table_name)
        constraint_definitions = []
        for constraint in unique_constraints:
            constraint_definitions.append({
                'type': 'unique',
                'name': constraint['name'],
                'columns': constraint['column_names']
            })
        
        return ModelDefinition(
            table_name=table_name,
            fields=fields,
            indexes=index_definitions if index_definitions else None,
            constraints=constraint_definitions if constraint_definitions else None
        )
    
    def _column_to_field_definition(self, column: Dict[str, Any]) -> FieldDefinition:
        """Convert database column info to FieldDefinition."""
        # Map SQLAlchemy types to string representations
        col_type = column['type']
        type_mapping = {
            sqltypes.String: 'string',
            sqltypes.Text: 'text',
            sqltypes.Integer: 'integer',
            sqltypes.Float: 'float',
            sqltypes.Boolean: 'boolean',
            sqltypes.DateTime: 'datetime',
            sqltypes.JSON: 'json',
        }
        
        field_type = 'string'  # default
        length = None
        precision = None
        scale = None
        
        for sql_type, str_type in type_mapping.items():
            if isinstance(col_type, sql_type):
                field_type = str_type
                break
        
        # Extract type-specific attributes
        if hasattr(col_type, 'length') and col_type.length:
            length = col_type.length
        
        if hasattr(col_type, 'precision') and col_type.precision:
            precision = col_type.precision
            
        if hasattr(col_type, 'scale') and col_type.scale:
            scale = col_type.scale
        
        return FieldDefinition(
            name=column['name'],
            type=field_type,
            nullable=column['nullable'],
            default=column['default'],
            length=length,
            precision=precision,
            scale=scale
        )
    
    def get_table_schema(self, table_name: str) -> Dict[str, Any]:
        """Get comprehensive table schema information."""
        if not self.inspector.has_table(table_name):
            raise ValueError(f"Table {table_name} does not exist")
        
        return {
            'table_name': table_name,
            'columns': self.inspector.get_columns(table_name),
            'primary_key': self.inspector.get_pk_constraint(table_name),
            'foreign_keys': self.inspector.get_foreign_keys(table_name),
            'indexes': self.inspector.get_indexes(table_name),
            'unique_constraints': self.inspector.get_unique_constraints(table_name),
            'check_constraints': self.inspector.get_check_constraints(table_name),
        }
    
    def get_database_schema(self) -> Dict[str, Any]:
        """Get schema information for entire database."""
        tables = self.inspector.get_table_names()
        schema = {
            'tables': {},
            'views': self.inspector.get_view_names(),
        }
        
        for table_name in tables:
            schema['tables'][table_name] = self.get_table_schema(table_name)
        
        return schema
    
    def generate_model_code(self, table_name: str) -> str:
        """Generate SQLAlchemy model code from existing table."""
        definition = self.introspect_table(table_name)
        
        lines = [
            f"class {table_name.title()}(Base):",
            f"    __tablename__ = '{table_name}'",
            "",
        ]
        
        # Add columns
        for field in definition.fields:
            col_def = self._field_to_code(field)
            lines.append(f"    {field.name} = {col_def}")
        
        return "\n".join(lines)
    
    def _field_to_code(self, field: FieldDefinition) -> str:
        """Convert field definition to Python code."""
        type_map = {
            'string': f"String({field.length})" if field.length else "String(255)",
            'text': "Text",
            'integer': "Integer",
            'float': "Float",
            'boolean': "Boolean",
            'datetime': "DateTime",
            'json': "JSON",
        }
        
        col_type = type_map.get(field.type, "String(255)")
        
        args = [col_type]
        kwargs = []
        
        if field.foreign_key:
            args.append(f"ForeignKey('{field.foreign_key}')")
        
        if not field.nullable:
            kwargs.append("nullable=False")
        
        if field.unique:
            kwargs.append("unique=True")
        
        if field.index:
            kwargs.append("index=True")
        
        if field.default is not None:
            if isinstance(field.default, str):
                kwargs.append(f"default='{field.default}'")
            else:
                kwargs.append(f"default={field.default}")
        
        all_args = args + kwargs
        return f"Column({', '.join(all_args)})"


class SchemaEvolution:
    """Handles schema evolution and migrations for dynamic models."""
    
    def __init__(self, engine, factory: DynamicModelFactory):
        self.engine = engine
        self.factory = factory
        self.inspector = MetadataInspector(engine)
    
    def compare_schemas(self, old_definition: ModelDefinition, 
                       new_definition: ModelDefinition) -> Dict[str, Any]:
        """Compare two model definitions and return differences."""
        changes = {
            'added_fields': [],
            'removed_fields': [],
            'modified_fields': [],
            'added_indexes': [],
            'removed_indexes': [],
        }
        
        # Compare fields
        old_fields = {f.name: f for f in old_definition.fields}
        new_fields = {f.name: f for f in new_definition.fields}
        
        # Find added fields
        for name, field in new_fields.items():
            if name not in old_fields:
                changes['added_fields'].append(field)
        
        # Find removed fields
        for name, field in old_fields.items():
            if name not in new_fields:
                changes['removed_fields'].append(field)
        
        # Find modified fields
        for name in old_fields.keys() & new_fields.keys():
            old_field = old_fields[name]
            new_field = new_fields[name]
            if self._fields_differ(old_field, new_field):
                changes['modified_fields'].append((old_field, new_field))
        
        return changes
    
    def _fields_differ(self, field1: FieldDefinition, field2: FieldDefinition) -> bool:
        """Check if two field definitions are different."""
        return (
            field1.type != field2.type or
            field1.nullable != field2.nullable or
            field1.length != field2.length or
            field1.unique != field2.unique or
            field1.foreign_key != field2.foreign_key
        )
    
    def generate_migration_sql(self, table_name: str, changes: Dict[str, Any]) -> List[str]:
        """Generate SQL statements for schema migration."""
        sql_statements = []
        
        # Add new columns
        for field in changes['added_fields']:
            column = field.to_column()
            sql = f"ALTER TABLE {table_name} ADD COLUMN {self._column_to_sql(column)}"
            sql_statements.append(sql)
        
        # Drop columns (careful - data loss!)
        for field in changes['removed_fields']:
            sql = f"ALTER TABLE {table_name} DROP COLUMN {field.name}"
            sql_statements.append(sql)
        
        return sql_statements
    
    def _column_to_sql(self, column: Column) -> str:
        """Convert SQLAlchemy column to SQL definition."""
        # This is a simplified implementation
        # In practice, you'd want to use the specific dialect
        type_str = str(column.type.compile(self.engine.dialect))
        
        parts = [column.name, type_str]
        
        if not column.nullable:
            parts.append("NOT NULL")
        
        if column.default is not None:
            parts.append(f"DEFAULT {column.default}")
        
        return " ".join(parts)
```

This covers the foundation of dynamic model generation. Should I continue with Section 8.1.2 - Advanced Relationship Patterns to continue building out Section 8?


#### 8.1.2 Advanced Relationship Patterns and Polymorphism

**app/patterns/relationships.py** - Advanced Relationship Management:
```python
"""
Advanced relationship patterns including polymorphism, self-referential relationships,
association objects, and complex inheritance hierarchies in SQLAlchemy.
"""

import uuid
from typing import Dict, Any, List, Optional, Type, Union
from datetime import datetime
from enum import Enum

from sqlalchemy import (
    Column, Integer, String, Text, DateTime, Boolean, Float, 
    ForeignKey, Table, Index, CheckConstraint, UniqueConstraint,
    and_, or_, select, case, func, event
)
from sqlalchemy.orm import (
    declarative_base, relationship, Session, backref, 
    validates, synonym, hybrid_property, column_property
)
from sqlalchemy.ext.declarative import declared_attr
from sqlalchemy.ext.associationproxy import association_proxy
from sqlalchemy.ext.hybrid import hybrid_property, hybrid_method
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.sql import sqltypes

Base = declarative_base()


# ================================
# Self-Referential Relationships
# ================================

class Category(Base):
    """Hierarchical category system with self-referential relationships."""
    __tablename__ = 'categories'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False)
    description = Column(Text)
    parent_id = Column(Integer, ForeignKey('categories.id'), index=True)
    sort_order = Column(Integer, default=0)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Self-referential relationship
    children = relationship(
        "Category",
        backref=backref("parent", remote_side=[id]),
        cascade="all, delete-orphan",
        order_by="Category.sort_order"
    )
    
    # Hybrid properties for hierarchy operations
    @hybrid_property
    def is_root(self):
        """Check if this is a root category."""
        return self.parent_id is None
    
    @hybrid_property
    def is_leaf(self):
        """Check if this is a leaf category (no children)."""
        return len(self.children) == 0
    
    @hybrid_method
    def is_ancestor_of(self, other):
        """Check if this category is an ancestor of another."""
        current = other.parent
        while current:
            if current.id == self.id:
                return True
            current = current.parent
        return False
    
    def get_ancestors(self, include_self=False):
        """Get all ancestor categories up to root."""
        ancestors = []
        current = self if include_self else self.parent
        
        while current:
            ancestors.append(current)
            current = current.parent
        
        return list(reversed(ancestors))  # Root first
    
    def get_descendants(self, include_self=False):
        """Get all descendant categories."""
        descendants = []
        
        if include_self:
            descendants.append(self)
        
        def _collect_children(category):
            for child in category.children:
                descendants.append(child)
                _collect_children(child)
        
        _collect_children(self)
        return descendants
    
    def get_siblings(self, include_self=False):
        """Get sibling categories (same parent)."""
        if self.parent is None:
            return []
        
        siblings = [cat for cat in self.parent.children if cat.id != self.id]
        if include_self:
            siblings.append(self)
        
        return sorted(siblings, key=lambda x: x.sort_order)
    
    def get_path(self, separator=' > '):
        """Get the full path from root to this category."""
        ancestors = self.get_ancestors(include_self=True)
        return separator.join([cat.name for cat in ancestors])
    
    @hybrid_property
    def depth(self):
        """Get the depth level in the hierarchy."""
        return len(self.get_ancestors())
    
    def move_to_parent(self, new_parent_id: Optional[int]):
        """Move this category to a new parent."""
        if new_parent_id == self.id:
            raise ValueError("Category cannot be its own parent")
        
        # Check for circular reference
        if new_parent_id:
            new_parent = Category.query.get(new_parent_id)
            if new_parent and self.is_ancestor_of(new_parent):
                raise ValueError("Cannot move category to its own descendant")
        
        self.parent_id = new_parent_id
    
    def __repr__(self):
        return f"<Category(id={self.id}, name='{self.name}', parent_id={self.parent_id})>"


# ================================
# Association Objects
# ================================

class UserRoleAssociation(Base):
    """Association object for many-to-many with additional attributes."""
    __tablename__ = 'user_role_associations'
    
    user_id = Column(Integer, ForeignKey('users.id'), primary_key=True)
    role_id = Column(Integer, ForeignKey('roles.id'), primary_key=True)
    
    # Additional attributes on the association
    granted_by_id = Column(Integer, ForeignKey('users.id'))
    granted_at = Column(DateTime, default=datetime.utcnow)
    expires_at = Column(DateTime)
    is_active = Column(Boolean, default=True)
    context = Column(String(100))  # e.g., 'organization:123'
    
    # Relationships
    user = relationship("User", foreign_keys=[user_id], back_populates="role_associations")
    role = relationship("Role", back_populates="user_associations")
    granted_by = relationship("User", foreign_keys=[granted_by_id])
    
    @hybrid_property
    def is_expired(self):
        """Check if the role assignment has expired."""
        if self.expires_at is None:
            return False
        return datetime.utcnow() > self.expires_at
    
    @hybrid_property
    def is_valid(self):
        """Check if the role assignment is currently valid."""
        return self.is_active and not self.is_expired
    
    def __repr__(self):
        return f"<UserRoleAssociation(user_id={self.user_id}, role_id={self.role_id})>"


class User(Base):
    """User model with association object relationships."""
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    username = Column(String(50), unique=True, nullable=False)
    email = Column(String(120), unique=True, nullable=False)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Association object relationship
    role_associations = relationship(
        "UserRoleAssociation", 
        foreign_keys="UserRoleAssociation.user_id",
        back_populates="user",
        cascade="all, delete-orphan"
    )
    
    # Convenient proxy to roles
    roles = association_proxy(
        "role_associations", 
        "role",
        creator=lambda role: UserRoleAssociation(role=role)
    )
    
    def has_role(self, role_name: str, context: str = None) -> bool:
        """Check if user has a specific role, optionally in a context."""
        for assoc in self.role_associations:
            if (assoc.role.name == role_name and 
                assoc.is_valid and 
                (context is None or assoc.context == context)):
                return True
        return False
    
    def get_active_roles(self, context: str = None) -> List['Role']:
        """Get all active roles for the user."""
        roles = []
        for assoc in self.role_associations:
            if (assoc.is_valid and 
                (context is None or assoc.context == context)):
                roles.append(assoc.role)
        return roles
    
    def grant_role(self, role: 'Role', granted_by: 'User', 
                   expires_at: datetime = None, context: str = None):
        """Grant a role to the user."""
        association = UserRoleAssociation(
            user=self,
            role=role,
            granted_by=granted_by,
            expires_at=expires_at,
            context=context
        )
        self.role_associations.append(association)
        return association
    
    def revoke_role(self, role_name: str, context: str = None):
        """Revoke a role from the user."""
        for assoc in self.role_associations:
            if (assoc.role.name == role_name and 
                (context is None or assoc.context == context)):
                assoc.is_active = False


class Role(Base):
    """Role model for RBAC system."""
    __tablename__ = 'roles'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(50), unique=True, nullable=False)
    description = Column(Text)
    is_system_role = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Association object relationship
    user_associations = relationship(
        "UserRoleAssociation", 
        back_populates="role",
        cascade="all, delete-orphan"
    )
    
    # Convenient proxy to users
    users = association_proxy("user_associations", "user")


# ================================
# Polymorphic Inheritance
# ================================

class ContactInfo(Base):
    """Base class for polymorphic contact information."""
    __tablename__ = 'contact_info'
    
    id = Column(Integer, primary_key=True)
    type = Column(String(20), nullable=False)  # Discriminator column
    person_id = Column(Integer, ForeignKey('persons.id'), nullable=False)
    is_primary = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Polymorphic configuration
    __mapper_args__ = {
        'polymorphic_identity': 'contact_info',
        'polymorphic_on': type,
        'with_polymorphic': '*'
    }
    
    # Relationship back to person
    person = relationship("Person", back_populates="contact_info")


class EmailAddress(ContactInfo):
    """Email contact information."""
    __tablename__ = 'email_addresses'
    
    id = Column(Integer, ForeignKey('contact_info.id'), primary_key=True)
    email = Column(String(120), nullable=False)
    is_verified = Column(Boolean, default=False)
    verification_token = Column(String(100))
    
    __mapper_args__ = {
        'polymorphic_identity': 'email'
    }
    
    @validates('email')
    def validate_email(self, key, email):
        """Validate email format."""
        import re
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(pattern, email):
            raise ValueError("Invalid email format")
        return email.lower()


class PhoneNumber(ContactInfo):
    """Phone contact information."""
    __tablename__ = 'phone_numbers'
    
    id = Column(Integer, ForeignKey('contact_info.id'), primary_key=True)
    number = Column(String(20), nullable=False)
    country_code = Column(String(5), default='+1')
    phone_type = Column(String(10), default='mobile')  # mobile, home, work
    is_verified = Column(Boolean, default=False)
    
    __mapper_args__ = {
        'polymorphic_identity': 'phone'
    }
    
    @hybrid_property
    def formatted_number(self):
        """Get formatted phone number."""
        return f"{self.country_code} {self.number}"


class Address(ContactInfo):
    """Address contact information."""
    __tablename__ = 'addresses'
    
    id = Column(Integer, ForeignKey('contact_info.id'), primary_key=True)
    street_line1 = Column(String(100), nullable=False)
    street_line2 = Column(String(100))
    city = Column(String(50), nullable=False)
    state = Column(String(50))
    postal_code = Column(String(20))
    country = Column(String(50), nullable=False)
    address_type = Column(String(10), default='home')  # home, work, billing
    
    __mapper_args__ = {
        'polymorphic_identity': 'address'
    }
    
    @hybrid_property
    def full_address(self):
        """Get full formatted address."""
        lines = [self.street_line1]
        if self.street_line2:
            lines.append(self.street_line2)
        
        city_state = self.city
        if self.state:
            city_state += f", {self.state}"
        if self.postal_code:
            city_state += f" {self.postal_code}"
        
        lines.append(city_state)
        lines.append(self.country)
        
        return "\n".join(lines)


class Person(Base):
    """Person with polymorphic contact information."""
    __tablename__ = 'persons'
    
    id = Column(Integer, primary_key=True)
    first_name = Column(String(50), nullable=False)
    last_name = Column(String(50), nullable=False)
    date_of_birth = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Polymorphic relationship to contact info
    contact_info = relationship(
        "ContactInfo",
        back_populates="person",
        cascade="all, delete-orphan",
        order_by="ContactInfo.is_primary.desc()"
    )
    
    @hybrid_property
    def full_name(self):
        """Get full name."""
        return f"{self.first_name} {self.last_name}"
    
    def get_contact_info(self, contact_type: str, primary_only: bool = False):
        """Get contact information by type."""
        contacts = [c for c in self.contact_info if c.type == contact_type]
        if primary_only:
            contacts = [c for c in contacts if c.is_primary]
        return contacts
    
    def get_primary_email(self) -> Optional[EmailAddress]:
        """Get primary email address."""
        emails = self.get_contact_info('email', primary_only=True)
        return emails[0] if emails else None
    
    def get_primary_phone(self) -> Optional[PhoneNumber]:
        """Get primary phone number."""
        phones = self.get_contact_info('phone', primary_only=True)
        return phones[0] if phones else None
    
    def add_email(self, email: str, is_primary: bool = False) -> EmailAddress:
        """Add email address."""
        if is_primary:
            # Unset other primary emails
            for contact in self.contact_info:
                if isinstance(contact, EmailAddress):
                    contact.is_primary = False
        
        email_contact = EmailAddress(
            email=email,
            is_primary=is_primary,
            person=self
        )
        self.contact_info.append(email_contact)
        return email_contact
    
    def add_phone(self, number: str, country_code: str = '+1', 
                  phone_type: str = 'mobile', is_primary: bool = False) -> PhoneNumber:
        """Add phone number."""
        if is_primary:
            # Unset other primary phones
            for contact in self.contact_info:
                if isinstance(contact, PhoneNumber):
                    contact.is_primary = False
        
        phone_contact = PhoneNumber(
            number=number,
            country_code=country_code,
            phone_type=phone_type,
            is_primary=is_primary,
            person=self
        )
        self.contact_info.append(phone_contact)
        return phone_contact


# ================================
# Table Inheritance
# ================================

class Vehicle(Base):
    """Base vehicle class using joined table inheritance."""
    __tablename__ = 'vehicles'
    
    id = Column(Integer, primary_key=True)
    type = Column(String(20), nullable=False)  # Discriminator
    make = Column(String(50), nullable=False)
    model = Column(String(50), nullable=False)
    year = Column(Integer, nullable=False)
    color = Column(String(30))
    vin = Column(String(17), unique=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    __mapper_args__ = {
        'polymorphic_identity': 'vehicle',
        'polymorphic_on': type,
        'with_polymorphic': '*'
    }
    
    @hybrid_property
    def description(self):
        """Get vehicle description."""
        return f"{self.year} {self.make} {self.model}"


class Car(Vehicle):
    """Car-specific attributes."""
    __tablename__ = 'cars'
    
    id = Column(Integer, ForeignKey('vehicles.id'), primary_key=True)
    num_doors = Column(Integer, default=4)
    fuel_type = Column(String(20), default='gasoline')
    transmission = Column(String(20), default='automatic')
    is_convertible = Column(Boolean, default=False)
    
    __mapper_args__ = {
        'polymorphic_identity': 'car'
    }


class Motorcycle(Vehicle):
    """Motorcycle-specific attributes."""
    __tablename__ = 'motorcycles'
    
    id = Column(Integer, ForeignKey('vehicles.id'), primary_key=True)
    engine_size = Column(Integer)  # in cc
    has_sidecar = Column(Boolean, default=False)
    bike_type = Column(String(20))  # sport, cruiser, touring, etc.
    
    __mapper_args__ = {
        'polymorphic_identity': 'motorcycle'
    }


class Truck(Vehicle):
    """Truck-specific attributes."""
    __tablename__ = 'trucks'
    
    id = Column(Integer, ForeignKey('vehicles.id'), primary_key=True)
    bed_length = Column(Float)  # in feet
    towing_capacity = Column(Integer)  # in pounds
    is_four_wheel_drive = Column(Boolean, default=False)
    truck_type = Column(String(20))  # pickup, semi, delivery, etc.
    
    __mapper_args__ = {
        'polymorphic_identity': 'truck'
    }


# ================================
# Generic Foreign Keys Pattern
# ================================

class Comment(Base):
    """Generic comment that can be attached to any entity."""
    __tablename__ = 'comments'
    
    id = Column(Integer, primary_key=True)
    content = Column(Text, nullable=False)
    author_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Generic foreign key fields
    parent_type = Column(String(50), nullable=False)  # Table name
    parent_id = Column(Integer, nullable=False)       # Record ID
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_active = Column(Boolean, default=True)
    
    # Relationships
    author = relationship("User")
    
    __table_args__ = (
        Index('idx_comments_parent', 'parent_type', 'parent_id'),
    )
    
    @classmethod
    def get_comments_for(cls, obj, session: Session):
        """Get comments for any object."""
        return session.query(cls).filter(
            cls.parent_type == obj.__tablename__,
            cls.parent_id == obj.id,
            cls.is_active == True
        ).order_by(cls.created_at.desc()).all()
    
    @classmethod
    def add_comment(cls, obj, content: str, author: User, session: Session):
        """Add comment to any object."""
        comment = cls(
            content=content,
            author=author,
            parent_type=obj.__tablename__,
            parent_id=obj.id
        )
        session.add(comment)
        return comment


# Helper mixin for commentable entities
class CommentableMixin:
    """Mixin to add comment functionality to any model."""
    
    def get_comments(self, session: Session):
        """Get comments for this object."""
        return Comment.get_comments_for(self, session)
    
    def add_comment(self, content: str, author: User, session: Session):
        """Add comment to this object."""
        return Comment.add_comment(self, content, author, session)
    
    @hybrid_property
    def comment_count(self):
        """Get count of active comments."""
        return func.count(Comment.id).filter(
            Comment.parent_type == self.__tablename__,
            Comment.parent_id == self.id,
            Comment.is_active == True
        )


# Example usage of CommentableMixin
class BlogPost(Base, CommentableMixin):
    """Blog post that can have comments."""
    __tablename__ = 'blog_posts'
    
    id = Column(Integer, primary_key=True)
    title = Column(String(200), nullable=False)
    content = Column(Text, nullable=False)
    author_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    author = relationship("User")


# ================================
# Advanced Relationship Utilities
# ================================

class RelationshipHelper:
    """Utility class for advanced relationship operations."""
    
    @staticmethod
    def bulk_load_relationships(session: Session, objects: List[Base], 
                              relationship_name: str, batch_size: int = 100):
        """Efficiently load relationships for multiple objects."""
        if not objects:
            return
        
        # Get the relationship property
        mapper = objects[0].__class__.__mapper__
        relationship_prop = getattr(mapper.relationships, relationship_name)
        
        # Load in batches to avoid overwhelming the database
        for i in range(0, len(objects), batch_size):
            batch = objects[i:i + batch_size]
            object_ids = [obj.id for obj in batch]
            
            # Use selectinload for efficient loading
            session.query(objects[0].__class__).filter(
                objects[0].__class__.id.in_(object_ids)
            ).options(
                selectinload(getattr(objects[0].__class__, relationship_name))
            ).all()
    
    @staticmethod
    def count_relationship_items(session: Session, parent_class: Type[Base], 
                               relationship_name: str, parent_ids: List[int]) -> Dict[int, int]:
        """Count items in relationships for multiple parent objects."""
        mapper = parent_class.__mapper__
        relationship_prop = getattr(mapper.relationships, relationship_name)
        related_class = relationship_prop.mapper.class_
        
        # Get the foreign key column
        fk_column = relationship_prop.local_columns.pop()
        
        # Query counts
        counts = session.query(
            fk_column,
            func.count(related_class.id).label('count')
        ).filter(
            fk_column.in_(parent_ids)
        ).group_by(fk_column).all()
        
        # Convert to dictionary
        return {parent_id: count for parent_id, count in counts}
```

This completes the advanced relationship patterns section. Should I continue with Section 8.1.3 - Custom Column Types and Hybrid Properties to continue building out Section 8?


#### 8.1.3 Custom Column Types and Hybrid Properties

**app/patterns/custom_types.py** - Advanced Custom Types:
```python
"""
Custom SQLAlchemy column types and hybrid properties for specialized data handling
including encrypted fields, JSON operations, and geographic data.
"""

import json
import uuid
import base64
from typing import Any, Dict, List, Optional, Union
from datetime import datetime, date
from decimal import Decimal
from enum import Enum

from sqlalchemy import TypeDecorator, String, Text, Integer, Float, DateTime
from sqlalchemy.ext.hybrid import hybrid_property, hybrid_method
from sqlalchemy.sql import func, case
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import validates
from cryptography.fernet import Fernet


class EncryptedString(TypeDecorator):
    """Encrypted string column type."""
    impl = String
    cache_ok = True
    
    def __init__(self, key: bytes, *args, **kwargs):
        self.cipher = Fernet(key)
        super().__init__(*args, **kwargs)
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return self.cipher.encrypt(value.encode()).decode()
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return self.cipher.decrypt(value.encode()).decode()
        return value


class JSONType(TypeDecorator):
    """Enhanced JSON column with validation and querying."""
    impl = Text
    cache_ok = True
    
    def __init__(self, schema=None, *args, **kwargs):
        self.schema = schema
        super().__init__(*args, **kwargs)
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            if self.schema:
                self._validate_schema(value)
            return json.dumps(value, default=str)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return json.loads(value)
        return value
    
    def _validate_schema(self, value):
        """Basic schema validation."""
        if self.schema and not isinstance(value, self.schema):
            raise ValueError(f"Value must be of type {self.schema}")


class PhoneNumberType(TypeDecorator):
    """Phone number column with formatting and validation."""
    impl = String(20)
    cache_ok = True
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            # Normalize phone number
            cleaned = ''.join(filter(str.isdigit, str(value)))
            if len(cleaned) == 10:
                cleaned = '1' + cleaned  # Add US country code
            return cleaned
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None and len(value) == 11:
            # Format as +1 (555) 123-4567
            return f"+{value[0]} ({value[1:4]}) {value[4:7]}-{value[7:]}"
        return value


class MoneyType(TypeDecorator):
    """Money column with currency support."""
    impl = String(20)
    cache_ok = True
    
    def __init__(self, currency='USD', *args, **kwargs):
        self.currency = currency
        super().__init__(*args, **kwargs)
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            if isinstance(value, (int, float)):
                return f"{value:.2f}:{self.currency}"
            return str(value)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            amount, currency = value.split(':')
            return {'amount': Decimal(amount), 'currency': currency}
        return value


# Example model using custom types
class UserProfile(Base):
    """User profile with custom column types."""
    __tablename__ = 'user_profiles'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), unique=True)
    
    # Custom encrypted field
    social_security = Column(EncryptedString(key=b'your-encryption-key-here'))
    
    # Enhanced JSON field
    preferences = Column(JSONType(schema=dict))
    
    # Phone number with formatting
    phone = Column(PhoneNumberType())
    
    # Money field with currency
    salary = Column(MoneyType(currency='USD'))
    
    # Hybrid properties for computed values
    @hybrid_property
    def annual_salary(self):
        """Calculate annual salary."""
        if self.salary:
            return self.salary['amount'] * 12
        return None
    
    @hybrid_property
    def has_preferences(self):
        """Check if user has any preferences set."""
        return self.preferences is not None and len(self.preferences) > 0
    
    @hybrid_method
    def get_preference(self, key, default=None):
        """Get specific preference value."""
        if self.preferences:
            return self.preferences.get(key, default)
        return default


# Geographic data handling
class Point:
    """Simple point class for geographic coordinates."""
    def __init__(self, latitude: float, longitude: float):
        self.latitude = latitude
        self.longitude = longitude
    
    def __str__(self):
        return f"Point({self.latitude}, {self.longitude})"


class PointType(TypeDecorator):
    """Geographic point column type."""
    impl = String(50)
    cache_ok = True
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return f"{value.latitude},{value.longitude}"
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            lat, lng = value.split(',')
            return Point(float(lat), float(lng))
        return value


class Location(Base):
    """Location model with geographic point."""
    __tablename__ = 'locations'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False)
    coordinates = Column(PointType())
    
    @hybrid_method
    def distance_to(self, other_point):
        """Calculate distance to another point (simplified)."""
        if self.coordinates and other_point:
            # Simplified distance calculation
            lat_diff = abs(self.coordinates.latitude - other_point.latitude)
            lng_diff = abs(self.coordinates.longitude - other_point.longitude)
            return (lat_diff ** 2 + lng_diff ** 2) ** 0.5
        return None
```

This completes Section 8.1 with dynamic models, advanced relationships, and custom types. To stay within limits, should I now move to begin Section 10 (Production Deployment and DevOps) as you originally requested, or would you like me to continue with more of Section 8?


## 10. Production Deployment and DevOps Patterns

### 10.1 Database Migration Strategies

#### 10.1.1 Advanced Alembic Patterns and Blue-Green Deployments

**migrations/advanced_migration_manager.py** - Production Migration Management:
```python
"""
Advanced database migration patterns for zero-downtime deployments,
rollback strategies, and complex schema changes in production environments.
"""

import os
import logging
import subprocess
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
from pathlib import Path
from contextlib import contextmanager

from alembic import command, script
from alembic.config import Config
from alembic.runtime.migration import MigrationContext
from alembic.operations import Operations
from sqlalchemy import create_engine, text, MetaData, inspect
from sqlalchemy.orm import sessionmaker
from sqlalchemy.engine import Engine

logger = logging.getLogger(__name__)


class MigrationStrategy:
    """Base class for migration strategies."""
    
    def __init__(self, engine: Engine, alembic_config: Config):
        self.engine = engine
        self.config = alembic_config
        self.inspector = inspect(engine)
    
    def execute(self, target_revision: str = 'head') -> bool:
        """Execute the migration strategy."""
        raise NotImplementedError


class StandardMigration(MigrationStrategy):
    """Standard migration strategy with downtime."""
    
    def execute(self, target_revision: str = 'head') -> bool:
        """Execute standard migration."""
        try:
            logger.info("Starting standard migration")
            command.upgrade(self.config, target_revision)
            logger.info("Migration completed successfully")
            return True
        except Exception as e:
            logger.error(f"Migration failed: {e}")
            return False


class ZeroDowntimeMigration(MigrationStrategy):
    """Zero-downtime migration strategy using multiple phases."""
    
    def __init__(self, engine: Engine, alembic_config: Config):
        super().__init__(engine, alembic_config)
        self.phases = []
        self.current_phase = 0
    
    def add_phase(self, name: str, operations: List[str], 
                  validation_func: Optional[Callable] = None):
        """Add a migration phase."""
        self.phases.append({
            'name': name,
            'operations': operations,
            'validation': validation_func
        })
    
    def execute(self, target_revision: str = 'head') -> bool:
        """Execute multi-phase zero-downtime migration."""
        logger.info("Starting zero-downtime migration")
        
        for phase_num, phase in enumerate(self.phases, 1):
            logger.info(f"Executing phase {phase_num}: {phase['name']}")
            
            try:
                self._execute_phase(phase)
                
                if phase['validation']:
                    if not phase['validation']():
                        logger.error(f"Phase {phase_num} validation failed")
                        return False
                
                logger.info(f"Phase {phase_num} completed successfully")
                
            except Exception as e:
                logger.error(f"Phase {phase_num} failed: {e}")
                return False
        
        logger.info("Zero-downtime migration completed")
        return True
    
    def _execute_phase(self, phase: Dict[str, Any]):
        """Execute a single migration phase."""
        with self.engine.connect() as conn:
            trans = conn.begin()
            try:
                for operation in phase['operations']:
                    conn.execute(text(operation))
                trans.commit()
            except Exception:
                trans.rollback()
                raise


class BlueGreenMigration(MigrationStrategy):
    """Blue-green deployment migration strategy."""
    
    def __init__(self, blue_engine: Engine, green_engine: Engine, 
                 alembic_config: Config):
        super().__init__(blue_engine, alembic_config)
        self.blue_engine = blue_engine
        self.green_engine = green_engine
        self.switch_callback = None
    
    def set_switch_callback(self, callback: Callable):
        """Set callback function for switching traffic."""
        self.switch_callback = callback
    
    def execute(self, target_revision: str = 'head') -> bool:
        """Execute blue-green migration."""
        logger.info("Starting blue-green migration")
        
        try:
            # Step 1: Migrate green environment
            logger.info("Migrating green environment")
            green_config = self._create_config_for_engine(self.green_engine)
            command.upgrade(green_config, target_revision)
            
            # Step 2: Validate green environment
            if not self._validate_environment(self.green_engine):
                logger.error("Green environment validation failed")
                return False
            
            # Step 3: Switch traffic to green
            if self.switch_callback:
                logger.info("Switching traffic to green environment")
                self.switch_callback('green')
            
            # Step 4: Migrate blue environment (for rollback capability)
            logger.info("Migrating blue environment")
            blue_config = self._create_config_for_engine(self.blue_engine)
            command.upgrade(blue_config, target_revision)
            
            logger.info("Blue-green migration completed")
            return True
            
        except Exception as e:
            logger.error(f"Blue-green migration failed: {e}")
            
            # Rollback to blue if needed
            if self.switch_callback:
                logger.info("Rolling back to blue environment")
                self.switch_callback('blue')
            
            return False
    
    def _create_config_for_engine(self, engine: Engine) -> Config:
        """Create Alembic config for specific engine."""
        config = Config()
        config.set_main_option("script_location", 
                              self.config.get_main_option("script_location"))
        config.set_main_option("sqlalchemy.url", str(engine.url))
        return config
    
    def _validate_environment(self, engine: Engine) -> bool:
        """Validate migrated environment."""
        try:
            with engine.connect() as conn:
                # Perform basic validation queries
                conn.execute(text("SELECT 1"))
                return True
        except Exception as e:
            logger.error(f"Environment validation failed: {e}")
            return False


class MigrationManager:
    """Advanced migration manager with rollback and monitoring."""
    
    def __init__(self, engine: Engine, alembic_config_path: str):
        self.engine = engine
        self.config = Config(alembic_config_path)
        self.migration_history = []
        self.rollback_points = []
    
    def get_current_revision(self) -> Optional[str]:
        """Get current database revision."""
        with self.engine.connect() as conn:
            context = MigrationContext.configure(conn)
            return context.get_current_revision()
    
    def get_pending_migrations(self) -> List[str]:
        """Get list of pending migrations."""
        script_dir = script.ScriptDirectory.from_config(self.config)
        with self.engine.connect() as conn:
            context = MigrationContext.configure(conn)
            current_rev = context.get_current_revision()
            
            if current_rev is None:
                # No migrations applied yet
                return [rev.revision for rev in script_dir.walk_revisions()]
            
            # Get revisions between current and head
            revisions = []
            for rev in script_dir.walk_revisions(current_rev, "head"):
                if rev.revision != current_rev:
                    revisions.append(rev.revision)
            
            return list(reversed(revisions))
    
    def create_rollback_point(self, name: str) -> str:
        """Create a rollback point."""
        current_rev = self.get_current_revision()
        rollback_point = {
            'name': name,
            'revision': current_rev,
            'timestamp': datetime.utcnow(),
            'id': len(self.rollback_points)
        }
        self.rollback_points.append(rollback_point)
        
        logger.info(f"Created rollback point '{name}' at revision {current_rev}")
        return rollback_point['id']
    
    def rollback_to_point(self, rollback_id: str) -> bool:
        """Rollback to a specific rollback point."""
        try:
            rollback_point = next(
                (rp for rp in self.rollback_points if rp['id'] == rollback_id),
                None
            )
            
            if not rollback_point:
                logger.error(f"Rollback point {rollback_id} not found")
                return False
            
            target_revision = rollback_point['revision']
            logger.info(f"Rolling back to '{rollback_point['name']}' "
                       f"(revision {target_revision})")
            
            command.downgrade(self.config, target_revision)
            
            # Record rollback in history
            self.migration_history.append({
                'action': 'rollback',
                'target': target_revision,
                'timestamp': datetime.utcnow()
            })
            
            logger.info("Rollback completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Rollback failed: {e}")
            return False
    
    def validate_migration_safety(self, target_revision: str) -> Dict[str, Any]:
        """Validate if migration is safe for production."""
        safety_report = {
            'safe': True,
            'warnings': [],
            'blocking_issues': [],
            'estimated_downtime': 0
        }
        
        script_dir = script.ScriptDirectory.from_config(self.config)
        current_rev = self.get_current_revision()
        
        # Get all revisions between current and target
        revisions_to_apply = []
        for rev in script_dir.walk_revisions(current_rev, target_revision):
            if rev.revision != current_rev:
                revisions_to_apply.append(rev)
        
        for revision in revisions_to_apply:
            # Analyze migration operations
            operations = self._parse_migration_operations(revision)
            
            for op in operations:
                risk_assessment = self._assess_operation_risk(op)
                
                if risk_assessment['blocking']:
                    safety_report['blocking_issues'].append({
                        'revision': revision.revision,
                        'operation': op,
                        'issue': risk_assessment['issue']
                    })
                    safety_report['safe'] = False
                
                if risk_assessment['warning']:
                    safety_report['warnings'].append({
                        'revision': revision.revision,
                        'operation': op,
                        'warning': risk_assessment['warning']
                    })
                
                safety_report['estimated_downtime'] += risk_assessment.get('downtime', 0)
        
        return safety_report
    
    def _parse_migration_operations(self, revision) -> List[Dict[str, Any]]:
        """Parse operations from migration revision."""
        # This is a simplified implementation
        # In practice, you'd parse the actual migration file
        operations = []
        
        # Read migration file
        migration_file = Path(revision.path)
        if migration_file.exists():
            content = migration_file.read_text()
            
            # Simple parsing for common operations
            if 'create_table' in content:
                operations.append({'type': 'create_table'})
            if 'drop_table' in content:
                operations.append({'type': 'drop_table'})
            if 'add_column' in content:
                operations.append({'type': 'add_column'})
            if 'drop_column' in content:
                operations.append({'type': 'drop_column'})
            if 'create_index' in content:
                operations.append({'type': 'create_index'})
        
        return operations
    
    def _assess_operation_risk(self, operation: Dict[str, Any]) -> Dict[str, Any]:
        """Assess risk level of migration operation."""
        risk_matrix = {
            'create_table': {'downtime': 1, 'blocking': False, 'warning': None},
            'drop_table': {
                'downtime': 5,
                'blocking': True,
                'issue': 'Dropping table causes data loss'
            },
            'add_column': {'downtime': 2, 'blocking': False, 'warning': 'May lock table'},
            'drop_column': {
                'downtime': 10,
                'blocking': True,
                'issue': 'Dropping column causes data loss'
            },
            'create_index': {
                'downtime': 30,
                'blocking': False,
                'warning': 'Index creation may take significant time'
            }
        }
        
        op_type = operation.get('type', 'unknown')
        return risk_matrix.get(op_type, {
            'downtime': 0,
            'blocking': False,
            'warning': f'Unknown operation type: {op_type}'
        })
    
    def monitor_migration_progress(self, callback: Callable = None):
        """Monitor long-running migration progress."""
        if not callback:
            callback = self._default_progress_callback
        
        # This would integrate with your monitoring system
        # to track migration progress in real-time
        pass
    
    def _default_progress_callback(self, progress: Dict[str, Any]):
        """Default progress callback."""
        logger.info(f"Migration progress: {progress}")


class DatabaseMaintenanceManager:
    """Manager for database maintenance during migrations."""
    
    def __init__(self, engine: Engine):
        self.engine = engine
    
    @contextmanager
    def maintenance_mode(self, message: str = "System under maintenance"):
        """Context manager for maintenance mode."""
        logger.info("Entering maintenance mode")
        
        try:
            # Enable maintenance mode
            self._enable_maintenance_mode(message)
            yield
        finally:
            # Disable maintenance mode
            self._disable_maintenance_mode()
            logger.info("Exiting maintenance mode")
    
    def _enable_maintenance_mode(self, message: str):
        """Enable maintenance mode."""
        # Implementation would depend on your application architecture
        # Could involve:
        # - Setting a feature flag
        # - Creating a maintenance table
        # - Updating load balancer configuration
        pass
    
    def _disable_maintenance_mode(self):
        """Disable maintenance mode."""
        # Remove maintenance mode indicators
        pass
    
    def backup_database(self, backup_path: str) -> bool:
        """Create database backup before migration."""
        try:
            logger.info(f"Creating database backup at {backup_path}")
            
            # Get database URL
            db_url = str(self.engine.url)
            
            # Extract connection details (simplified)
            if 'postgresql' in db_url:
                return self._backup_postgresql(db_url, backup_path)
            elif 'mysql' in db_url:
                return self._backup_mysql(db_url, backup_path)
            else:
                logger.warning("Backup not supported for this database type")
                return False
            
        except Exception as e:
            logger.error(f"Database backup failed: {e}")
            return False
    
    def _backup_postgresql(self, db_url: str, backup_path: str) -> bool:
        """Backup PostgreSQL database."""
        try:
            cmd = f"pg_dump {db_url} > {backup_path}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("PostgreSQL backup completed successfully")
                return True
            else:
                logger.error(f"PostgreSQL backup failed: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"PostgreSQL backup error: {e}")
            return False
    
    def _backup_mysql(self, db_url: str, backup_path: str) -> bool:
        """Backup MySQL database."""
        try:
            # Parse connection details from URL
            # This is simplified - in practice, you'd use proper URL parsing
            cmd = f"mysqldump --single-transaction {db_url} > {backup_path}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("MySQL backup completed successfully")
                return True
            else:
                logger.error(f"MySQL backup failed: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"MySQL backup error: {e}")
            return False
```

This begins Section 10 with advanced migration strategies. Should I continue with Section 10.1.2 - CI/CD Pipeline Integration to complete the database migration patterns, or move to another subsection of Section 10?


#### 10.1.2 CI/CD Pipeline Integration and Automated Migration Testing

**devops/migration_pipeline.py** - CI/CD Migration Pipeline:
```python
"""
CI/CD pipeline integration for automated database migrations
with comprehensive testing, validation, and rollback capabilities.
"""

import yaml
import json
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime

from sqlalchemy import create_engine
from advanced_migration_manager import (
    MigrationManager, ZeroDowntimeMigration, BlueGreenMigration,
    DatabaseMaintenanceManager
)

logger = logging.getLogger(__name__)


@dataclass
class DeploymentEnvironment:
    """Configuration for deployment environment."""
    name: str
    database_url: str
    backup_location: str
    migration_strategy: str
    max_downtime_minutes: int
    rollback_enabled: bool
    validation_queries: List[str]
    notification_channels: List[str]


class MigrationPipeline:
    """Automated migration pipeline for CI/CD."""
    
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.environments = self._load_environments()
        self.current_deployment = None
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load pipeline configuration."""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def _load_environments(self) -> Dict[str, DeploymentEnvironment]:
        """Load environment configurations."""
        environments = {}
        
        for env_config in self.config.get('environments', []):
            env = DeploymentEnvironment(
                name=env_config['name'],
                database_url=env_config['database_url'],
                backup_location=env_config['backup_location'],
                migration_strategy=env_config.get('migration_strategy', 'standard'),
                max_downtime_minutes=env_config.get('max_downtime_minutes', 5),
                rollback_enabled=env_config.get('rollback_enabled', True),
                validation_queries=env_config.get('validation_queries', []),
                notification_channels=env_config.get('notification_channels', [])
            )
            environments[env.name] = env
        
        return environments
    
    def deploy_to_environment(self, environment_name: str, 
                            target_revision: str = 'head') -> bool:
        """Deploy migrations to specific environment."""
        if environment_name not in self.environments:
            logger.error(f"Environment '{environment_name}' not found")
            return False
        
        env = self.environments[environment_name]
        logger.info(f"Starting deployment to {environment_name}")
        
        try:
            # Create deployment record
            deployment = self._create_deployment_record(env, target_revision)
            self.current_deployment = deployment
            
            # Pre-deployment validation
            if not self._pre_deployment_validation(env, target_revision):
                logger.error("Pre-deployment validation failed")
                return False
            
            # Execute migration strategy
            success = self._execute_migration_strategy(env, target_revision)
            
            if success:
                # Post-deployment validation
                if not self._post_deployment_validation(env):
                    logger.error("Post-deployment validation failed")
                    if env.rollback_enabled:
                        self._rollback_deployment(env)
                    return False
                
                # Mark deployment as successful
                deployment['status'] = 'success'
                deployment['completed_at'] = datetime.utcnow()
                
                logger.info(f"Deployment to {environment_name} completed successfully")
                self._send_notifications(env, deployment)
                return True
            else:
                deployment['status'] = 'failed'
                deployment['completed_at'] = datetime.utcnow()
                
                if env.rollback_enabled:
                    self._rollback_deployment(env)
                
                self._send_notifications(env, deployment)
                return False
                
        except Exception as e:
            logger.error(f"Deployment to {environment_name} failed: {e}")
            if self.current_deployment and env.rollback_enabled:
                self._rollback_deployment(env)
            return False
    
    def _create_deployment_record(self, env: DeploymentEnvironment, 
                                target_revision: str) -> Dict[str, Any]:
        """Create deployment tracking record."""
        return {
            'environment': env.name,
            'target_revision': target_revision,
            'started_at': datetime.utcnow(),
            'status': 'in_progress',
            'strategy': env.migration_strategy,
            'backup_location': None,
            'rollback_point': None
        }
    
    def _pre_deployment_validation(self, env: DeploymentEnvironment,
                                 target_revision: str) -> bool:
        """Validate deployment prerequisites."""
        logger.info("Running pre-deployment validation")
        
        try:
            engine = create_engine(env.database_url)
            manager = MigrationManager(engine, self.config['alembic_config'])
            
            # Check database connectivity
            with engine.connect() as conn:
                conn.execute("SELECT 1")
            
            # Validate migration safety
            safety_report = manager.validate_migration_safety(target_revision)
            
            if not safety_report['safe']:
                logger.error("Migration safety validation failed:")
                for issue in safety_report['blocking_issues']:
                    logger.error(f"  - {issue['issue']}")
                return False
            
            # Check estimated downtime
            if safety_report['estimated_downtime'] > env.max_downtime_minutes * 60:
                logger.error(f"Estimated downtime ({safety_report['estimated_downtime']}s) "
                           f"exceeds limit ({env.max_downtime_minutes * 60}s)")
                return False
            
            # Log warnings
            for warning in safety_report['warnings']:
                logger.warning(f"Migration warning: {warning['warning']}")
            
            logger.info("Pre-deployment validation passed")
            return True
            
        except Exception as e:
            logger.error(f"Pre-deployment validation failed: {e}")
            return False
    
    def _execute_migration_strategy(self, env: DeploymentEnvironment,
                                  target_revision: str) -> bool:
        """Execute migration using configured strategy."""
        logger.info(f"Executing {env.migration_strategy} migration strategy")
        
        try:
            engine = create_engine(env.database_url)
            alembic_config = self.config['alembic_config']
            
            # Create backup before migration
            maintenance_manager = DatabaseMaintenanceManager(engine)
            backup_path = f"{env.backup_location}/backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.sql"
            
            if not maintenance_manager.backup_database(backup_path):
                logger.error("Database backup failed")
                return False
            
            self.current_deployment['backup_location'] = backup_path
            
            # Create rollback point
            manager = MigrationManager(engine, alembic_config)
            rollback_id = manager.create_rollback_point(f"pre_deployment_{env.name}")
            self.current_deployment['rollback_point'] = rollback_id
            
            # Execute migration based on strategy
            if env.migration_strategy == 'zero_downtime':
                return self._execute_zero_downtime_migration(engine, alembic_config, target_revision)
            elif env.migration_strategy == 'blue_green':
                return self._execute_blue_green_migration(env, target_revision)
            else:  # standard
                return self._execute_standard_migration(engine, alembic_config, target_revision)
                
        except Exception as e:
            logger.error(f"Migration execution failed: {e}")
            return False
    
    def _execute_standard_migration(self, engine, alembic_config: str,
                                  target_revision: str) -> bool:
        """Execute standard migration with downtime."""
        from advanced_migration_manager import StandardMigration
        from alembic.config import Config
        
        config = Config(alembic_config)
        migration = StandardMigration(engine, config)
        return migration.execute(target_revision)
    
    def _execute_zero_downtime_migration(self, engine, alembic_config: str,
                                       target_revision: str) -> bool:
        """Execute zero-downtime migration."""
        from alembic.config import Config
        
        config = Config(alembic_config)
        migration = ZeroDowntimeMigration(engine, config)
        
        # Configure phases for zero-downtime migration
        # This would be customized based on your specific migration needs
        migration.add_phase(
            "Schema preparation",
            ["CREATE INDEX CONCURRENTLY IF NOT EXISTS ..."],
            lambda: self._validate_schema_preparation()
        )
        
        migration.add_phase(
            "Data migration",
            ["UPDATE ... SET ... WHERE ..."],
            lambda: self._validate_data_migration()
        )
        
        return migration.execute(target_revision)
    
    def _execute_blue_green_migration(self, env: DeploymentEnvironment,
                                    target_revision: str) -> bool:
        """Execute blue-green migration."""
        # This requires additional configuration for blue/green environments
        blue_url = env.database_url
        green_url = env.database_url.replace('_blue', '_green')  # Simplified
        
        blue_engine = create_engine(blue_url)
        green_engine = create_engine(green_url)
        
        from alembic.config import Config
        config = Config(self.config['alembic_config'])
        
        migration = BlueGreenMigration(blue_engine, green_engine, config)
        migration.set_switch_callback(self._switch_traffic)
        
        return migration.execute(target_revision)
    
    def _switch_traffic(self, target_environment: str):
        """Switch traffic between blue/green environments."""
        logger.info(f"Switching traffic to {target_environment} environment")
        # Implementation would depend on your load balancer/proxy configuration
        pass
    
    def _validate_schema_preparation(self) -> bool:
        """Validate schema preparation phase."""
        # Custom validation logic
        return True
    
    def _validate_data_migration(self) -> bool:
        """Validate data migration phase."""
        # Custom validation logic
        return True
    
    def _post_deployment_validation(self, env: DeploymentEnvironment) -> bool:
        """Validate deployment success."""
        logger.info("Running post-deployment validation")
        
        try:
            engine = create_engine(env.database_url)
            
            with engine.connect() as conn:
                # Run validation queries
                for query in env.validation_queries:
                    result = conn.execute(query)
                    logger.info(f"Validation query result: {result.fetchone()}")
                
                # Check if migration was applied correctly
                from alembic.runtime.migration import MigrationContext
                context = MigrationContext.configure(conn)
                current_rev = context.get_current_revision()
                logger.info(f"Current database revision: {current_rev}")
            
            logger.info("Post-deployment validation passed")
            return True
            
        except Exception as e:
            logger.error(f"Post-deployment validation failed: {e}")
            return False
    
    def _rollback_deployment(self, env: DeploymentEnvironment):
        """Rollback failed deployment."""
        logger.info("Rolling back deployment")
        
        try:
            engine = create_engine(env.database_url)
            manager = MigrationManager(engine, self.config['alembic_config'])
            
            rollback_id = self.current_deployment.get('rollback_point')
            if rollback_id:
                success = manager.rollback_to_point(rollback_id)
                if success:
                    logger.info("Rollback completed successfully")
                else:
                    logger.error("Rollback failed")
            else:
                logger.error("No rollback point available")
                
        except Exception as e:
            logger.error(f"Rollback failed: {e}")
    
    def _send_notifications(self, env: DeploymentEnvironment, 
                          deployment: Dict[str, Any]):
        """Send deployment notifications."""
        message = self._format_notification_message(deployment)
        
        for channel in env.notification_channels:
            try:
                self._send_to_channel(channel, message)
            except Exception as e:
                logger.error(f"Failed to send notification to {channel}: {e}")
    
    def _format_notification_message(self, deployment: Dict[str, Any]) -> str:
        """Format notification message."""
        status = deployment['status']
        environment = deployment['environment']
        revision = deployment['target_revision']
        
        if status == 'success':
            return f"✅ Migration to {environment} (revision {revision}) completed successfully"
        else:
            return f"❌ Migration to {environment} (revision {revision}) failed"
    
    def _send_to_channel(self, channel: str, message: str):
        """Send message to notification channel."""
        # Implementation would depend on your notification system
        # (Slack, email, etc.)
        logger.info(f"Notification to {channel}: {message}")


class MigrationTestSuite:
    """Automated testing for database migrations."""
    
    def __init__(self, test_database_url: str):
        self.test_engine = create_engine(test_database_url)
        self.test_results = []
    
    def run_migration_tests(self, migration_revisions: List[str]) -> bool:
        """Run comprehensive migration tests."""
        logger.info("Starting migration test suite")
        
        all_passed = True
        
        for revision in migration_revisions:
            test_result = self._test_migration_revision(revision)
            self.test_results.append(test_result)
            
            if not test_result['passed']:
                all_passed = False
        
        self._generate_test_report()
        return all_passed
    
    def _test_migration_revision(self, revision: str) -> Dict[str, Any]:
        """Test individual migration revision."""
        logger.info(f"Testing migration revision {revision}")
        
        test_result = {
            'revision': revision,
            'passed': True,
            'tests': []
        }
        
        try:
            # Test forward migration
            forward_test = self._test_forward_migration(revision)
            test_result['tests'].append(forward_test)
            
            # Test backward migration
            backward_test = self._test_backward_migration(revision)
            test_result['tests'].append(backward_test)
            
            # Test data integrity
            integrity_test = self._test_data_integrity(revision)
            test_result['tests'].append(integrity_test)
            
            # Test performance impact
            performance_test = self._test_performance_impact(revision)
            test_result['tests'].append(performance_test)
            
            # Check if any test failed
            test_result['passed'] = all(test['passed'] for test in test_result['tests'])
            
        except Exception as e:
            test_result['passed'] = False
            test_result['error'] = str(e)
            logger.error(f"Migration test for {revision} failed: {e}")
        
        return test_result
    
    def _test_forward_migration(self, revision: str) -> Dict[str, Any]:
        """Test forward migration."""
        try:
            # Apply migration
            from alembic import command
            from alembic.config import Config
            
            config = Config('alembic.ini')  # Adjust path as needed
            command.upgrade(config, revision)
            
            return {'name': 'forward_migration', 'passed': True}
        except Exception as e:
            return {'name': 'forward_migration', 'passed': False, 'error': str(e)}
    
    def _test_backward_migration(self, revision: str) -> Dict[str, Any]:
        """Test backward migration."""
        try:
            # Get previous revision
            from alembic import command
            from alembic.config import Config
            
            config = Config('alembic.ini')
            # This is simplified - you'd need to get the actual previous revision
            previous_revision = 'previous'  # Implement proper logic
            
            command.downgrade(config, previous_revision)
            
            return {'name': 'backward_migration', 'passed': True}
        except Exception as e:
            return {'name': 'backward_migration', 'passed': False, 'error': str(e)}
    
    def _test_data_integrity(self, revision: str) -> Dict[str, Any]:
        """Test data integrity after migration."""
        try:
            with self.test_engine.connect() as conn:
                # Run data integrity checks
                # This would be customized based on your specific requirements
                result = conn.execute("SELECT COUNT(*) FROM information_schema.tables")
                table_count = result.fetchone()[0]
                
                if table_count > 0:
                    return {'name': 'data_integrity', 'passed': True}
                else:
                    return {'name': 'data_integrity', 'passed': False, 
                           'error': 'No tables found after migration'}
        except Exception as e:
            return {'name': 'data_integrity', 'passed': False, 'error': str(e)}
    
    def _test_performance_impact(self, revision: str) -> Dict[str, Any]:
        """Test performance impact of migration."""
        try:
            import time
            
            # Measure query performance before and after migration
            start_time = time.time()
            
            with self.test_engine.connect() as conn:
                # Run representative queries
                conn.execute("SELECT 1")
            
            execution_time = time.time() - start_time
            
            # Check if performance is within acceptable limits
            if execution_time < 1.0:  # 1 second threshold
                return {'name': 'performance_impact', 'passed': True, 
                       'execution_time': execution_time}
            else:
                return {'name': 'performance_impact', 'passed': False,
                       'error': f'Execution time {execution_time}s exceeds threshold'}
        except Exception as e:
            return {'name': 'performance_impact', 'passed': False, 'error': str(e)}
    
    def _generate_test_report(self):
        """Generate comprehensive test report."""
        logger.info("Generating migration test report")
        
        report = {
            'total_revisions': len(self.test_results),
            'passed_revisions': sum(1 for r in self.test_results if r['passed']),
            'failed_revisions': sum(1 for r in self.test_results if not r['passed']),
            'results': self.test_results
        }
        
        # Save report to file
        report_path = f"migration_test_report_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"Test report saved to {report_path}")
```

**GitHub Actions Workflow** - `.github/workflows/database_migration.yml`:
```yaml
name: Database Migration Pipeline

on:
  push:
    branches: [main, develop]
    paths: ['migrations/**']
  pull_request:
    branches: [main]
    paths: ['migrations/**']

env:
  POSTGRES_PASSWORD: postgres
  DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db

jobs:
  migration-tests:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run migration tests
      run: |
        python -m pytest tests/test_migrations.py -v --cov=migrations
    
    - name: Test migration safety
      run: |
        python scripts/validate_migration_safety.py
    
    - name: Generate migration report
      run: |
        python scripts/generate_migration_report.py
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      with:
        name: migration-test-results
        path: migration_test_report_*.json

  deploy-staging:
    needs: migration-tests
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to staging
      env:
        STAGING_DATABASE_URL: ${{ secrets.STAGING_DATABASE_URL }}
      run: |
        python scripts/deploy_migration.py staging
    
    - name: Run staging validation
      run: |
        python scripts/validate_deployment.py staging
    
    - name: Notify team
      if: always()
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  deploy-production:
    needs: migration-tests
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    environment: production
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Create deployment
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.repos.createDeployment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            ref: context.sha,
            environment: 'production',
            description: 'Database migration deployment'
          })
    
    - name: Deploy to production
      env:
        PRODUCTION_DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
      run: |
        python scripts/deploy_migration.py production
    
    - name: Run production validation
      run: |
        python scripts/validate_deployment.py production
    
    - name: Update deployment status
      if: always()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.repos.createDeploymentStatus({
            owner: context.repo.owner,
            repo: context.repo.repo,
            deployment_id: ${{ steps.deployment.outputs.deployment_id }},
            state: '${{ job.status }}' === 'success' ? 'success' : 'failure',
            description: 'Migration deployment ${{ job.status }}'
          })
```

This completes the CI/CD pipeline integration. Should I continue with Section 10.2 - Production Monitoring and Observability, or would you like me to focus on a different aspect of the production deployment patterns?


### 10.2 Production Monitoring and Observability

#### 10.2.1 Database Performance Monitoring

**monitoring/database_monitor.py** - Comprehensive Database Monitoring:
```python
"""
Production database monitoring with metrics collection,
alerting, and performance analysis for SQLAlchemy applications.
"""

import time
import logging
import asyncio
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from contextlib import contextmanager
from threading import Lock
import json

from sqlalchemy import create_engine, event, text
from sqlalchemy.engine import Engine
from sqlalchemy.pool import Pool
from sqlalchemy.orm import sessionmaker
import psutil

logger = logging.getLogger(__name__)


@dataclass
class MetricPoint:
    """Individual metric data point."""
    timestamp: datetime
    value: float
    tags: Dict[str, str] = field(default_factory=dict)
    labels: Dict[str, str] = field(default_factory=dict)


@dataclass
class AlertRule:
    """Alert rule configuration."""
    name: str
    metric: str
    threshold: float
    operator: str  # 'gt', 'lt', 'eq'
    duration: int  # seconds
    severity: str  # 'warning', 'critical'
    notification_channels: List[str]
    enabled: bool = True


class MetricsCollector:
    """Collect and store database metrics."""
    
    def __init__(self):
        self.metrics: Dict[str, List[MetricPoint]] = {}
        self.lock = Lock()
        self.collectors: List[Callable] = []
        
    def register_collector(self, collector: Callable):
        """Register a metric collector function."""
        self.collectors.append(collector)
    
    def add_metric(self, name: str, value: float, tags: Dict[str, str] = None,
                   labels: Dict[str, str] = None):
        """Add a metric point."""
        with self.lock:
            if name not in self.metrics:
                self.metrics[name] = []
            
            point = MetricPoint(
                timestamp=datetime.utcnow(),
                value=value,
                tags=tags or {},
                labels=labels or {}
            )
            
            self.metrics[name].append(point)
            
            # Keep only last 1000 points per metric
            if len(self.metrics[name]) > 1000:
                self.metrics[name] = self.metrics[name][-1000:]
    
    def get_metrics(self, name: str, since: datetime = None) -> List[MetricPoint]:
        """Get metrics for a specific name."""
        with self.lock:
            points = self.metrics.get(name, [])
            
            if since:
                points = [p for p in points if p.timestamp >= since]
            
            return points
    
    def collect_all(self):
        """Run all registered collectors."""
        for collector in self.collectors:
            try:
                collector()
            except Exception as e:
                logger.error(f"Collector {collector.__name__} failed: {e}")


class DatabasePerformanceMonitor:
    """Monitor database performance metrics."""
    
    def __init__(self, engine: Engine, metrics_collector: MetricsCollector):
        self.engine = engine
        self.metrics = metrics_collector
        self.query_stats = {}
        self.connection_stats = {}
        
        # Register event listeners
        self._setup_event_listeners()
        
        # Register collectors
        self.metrics.register_collector(self._collect_connection_pool_metrics)
        self.metrics.register_collector(self._collect_database_size_metrics)
        self.metrics.register_collector(self._collect_system_metrics)
    
    def _setup_event_listeners(self):
        """Setup SQLAlchemy event listeners."""
        
        @event.listens_for(self.engine, "before_cursor_execute")
        def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            context._query_start_time = time.time()
            context._query_statement = statement
        
        @event.listens_for(self.engine, "after_cursor_execute")
        def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            total_time = time.time() - context._query_start_time
            
            # Extract query type
            query_type = statement.strip().split()[0].upper()
            
            # Record query execution time
            self.metrics.add_metric(
                'database.query.execution_time',
                total_time,
                tags={'query_type': query_type}
            )
            
            # Update query statistics
            if query_type not in self.query_stats:
                self.query_stats[query_type] = {
                    'count': 0,
                    'total_time': 0,
                    'avg_time': 0,
                    'min_time': float('inf'),
                    'max_time': 0
                }
            
            stats = self.query_stats[query_type]
            stats['count'] += 1
            stats['total_time'] += total_time
            stats['avg_time'] = stats['total_time'] / stats['count']
            stats['min_time'] = min(stats['min_time'], total_time)
            stats['max_time'] = max(stats['max_time'], total_time)
        
        @event.listens_for(Pool, "connect")
        def on_connect(dbapi_conn, connection_record):
            self.metrics.add_metric('database.connections.created', 1)
        
        @event.listens_for(Pool, "checkout")
        def on_checkout(dbapi_conn, connection_record, connection_proxy):
            self.metrics.add_metric('database.connections.checkout', 1)
        
        @event.listens_for(Pool, "checkin")
        def on_checkin(dbapi_conn, connection_record):
            self.metrics.add_metric('database.connections.checkin', 1)
    
    def _collect_connection_pool_metrics(self):
        """Collect connection pool metrics."""
        pool = self.engine.pool
        
        self.metrics.add_metric('database.pool.size', pool.size())
        self.metrics.add_metric('database.pool.checked_in', pool.checkedin())
        self.metrics.add_metric('database.pool.checked_out', pool.checkedout())
        self.metrics.add_metric('database.pool.overflow', pool.overflow())
        self.metrics.add_metric('database.pool.invalid', pool.invalid())
    
    def _collect_database_size_metrics(self):
        """Collect database size metrics."""
        try:
            with self.engine.connect() as conn:
                if 'postgresql' in str(self.engine.url):
                    self._collect_postgresql_metrics(conn)
                elif 'mysql' in str(self.engine.url):
                    self._collect_mysql_metrics(conn)
        except Exception as e:
            logger.error(f"Failed to collect database size metrics: {e}")
    
    def _collect_postgresql_metrics(self, conn):
        """Collect PostgreSQL-specific metrics."""
        # Database size
        result = conn.execute(text("SELECT pg_database_size(current_database())"))
        db_size = result.fetchone()[0]
        self.metrics.add_metric('database.size.bytes', db_size)
        
        # Table sizes
        result = conn.execute(text("""
            SELECT schemaname, tablename, pg_total_relation_size(schemaname||'.'||tablename) as size
            FROM pg_tables 
            WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
        """))
        
        for row in result:
            self.metrics.add_metric(
                'database.table.size.bytes',
                row[2],
                tags={'schema': row[0], 'table': row[1]}
            )
        
        # Active connections
        result = conn.execute(text("SELECT count(*) FROM pg_stat_activity"))
        active_connections = result.fetchone()[0]
        self.metrics.add_metric('database.connections.active', active_connections)
        
        # Query statistics
        result = conn.execute(text("""
            SELECT query, calls, total_time, mean_time
            FROM pg_stat_statements
            ORDER BY total_time DESC
            LIMIT 10
        """))
        
        for row in result:
            query_hash = str(hash(row[0]))[:8]
            self.metrics.add_metric(
                'database.query.calls',
                row[1],
                tags={'query_id': query_hash}
            )
            self.metrics.add_metric(
                'database.query.total_time',
                row[2],
                tags={'query_id': query_hash}
            )
            self.metrics.add_metric(
                'database.query.mean_time',
                row[3],
                tags={'query_id': query_hash}
            )
    
    def _collect_mysql_metrics(self, conn):
        """Collect MySQL-specific metrics."""
        # Database size
        result = conn.execute(text("""
            SELECT SUM(data_length + index_length) as size
            FROM information_schema.tables
            WHERE table_schema = DATABASE()
        """))
        db_size = result.fetchone()[0]
        if db_size:
            self.metrics.add_metric('database.size.bytes', db_size)
        
        # Table sizes
        result = conn.execute(text("""
            SELECT table_name, data_length + index_length as size
            FROM information_schema.tables
            WHERE table_schema = DATABASE()
        """))
        
        for row in result:
            self.metrics.add_metric(
                'database.table.size.bytes',
                row[1],
                tags={'table': row[0]}
            )
        
        # Active connections
        result = conn.execute(text("SHOW STATUS LIKE 'Threads_connected'"))
        active_connections = result.fetchone()[1]
        self.metrics.add_metric('database.connections.active', int(active_connections))
    
    def _collect_system_metrics(self):
        """Collect system-level metrics."""
        # CPU usage
        cpu_percent = psutil.cpu_percent()
        self.metrics.add_metric('system.cpu.percent', cpu_percent)
        
        # Memory usage
        memory = psutil.virtual_memory()
        self.metrics.add_metric('system.memory.percent', memory.percent)
        self.metrics.add_metric('system.memory.available', memory.available)
        
        # Disk usage
        disk = psutil.disk_usage('/')
        self.metrics.add_metric('system.disk.percent', disk.percent)
        self.metrics.add_metric('system.disk.free', disk.free)
    
    def get_query_statistics(self) -> Dict[str, Any]:
        """Get aggregated query statistics."""
        return self.query_stats.copy()
    
    def get_slow_queries(self, threshold: float = 1.0) -> List[Dict[str, Any]]:
        """Get queries that exceed the time threshold."""
        slow_queries = []
        
        for query_type, stats in self.query_stats.items():
            if stats['max_time'] > threshold:
                slow_queries.append({
                    'query_type': query_type,
                    'max_time': stats['max_time'],
                    'avg_time': stats['avg_time'],
                    'count': stats['count']
                })
        
        return sorted(slow_queries, key=lambda x: x['max_time'], reverse=True)


class AlertManager:
    """Manage alerts based on metric thresholds."""
    
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics = metrics_collector
        self.rules: List[AlertRule] = []
        self.active_alerts: Dict[str, datetime] = {}
        self.notification_handlers = {}
    
    def add_rule(self, rule: AlertRule):
        """Add an alert rule."""
        self.rules.append(rule)
    
    def register_notification_handler(self, channel: str, handler: Callable):
        """Register a notification handler for a channel."""
        self.notification_handlers[channel] = handler
    
    def check_alerts(self):
        """Check all alert rules against current metrics."""
        current_time = datetime.utcnow()
        
        for rule in self.rules:
            if not rule.enabled:
                continue
            
            try:
                self._check_rule(rule, current_time)
            except Exception as e:
                logger.error(f"Error checking alert rule {rule.name}: {e}")
    
    def _check_rule(self, rule: AlertRule, current_time: datetime):
        """Check a specific alert rule."""
        # Get recent metrics
        since = current_time - timedelta(seconds=rule.duration)
        metric_points = self.metrics.get_metrics(rule.metric, since)
        
        if not metric_points:
            return
        
        # Calculate aggregated value (using average for now)
        avg_value = sum(p.value for p in metric_points) / len(metric_points)
        
        # Check threshold
        threshold_exceeded = False
        if rule.operator == 'gt' and avg_value > rule.threshold:
            threshold_exceeded = True
        elif rule.operator == 'lt' and avg_value < rule.threshold:
            threshold_exceeded = True
        elif rule.operator == 'eq' and abs(avg_value - rule.threshold) < 0.001:
            threshold_exceeded = True
        
        alert_key = f"{rule.name}:{rule.metric}"
        
        if threshold_exceeded:
            if alert_key not in self.active_alerts:
                # New alert
                self.active_alerts[alert_key] = current_time
                self._send_alert(rule, avg_value, 'fired')
        else:
            if alert_key in self.active_alerts:
                # Alert resolved
                del self.active_alerts[alert_key]
                self._send_alert(rule, avg_value, 'resolved')
    
    def _send_alert(self, rule: AlertRule, value: float, status: str):
        """Send alert notification."""
        message = {
            'rule': rule.name,
            'metric': rule.metric,
            'threshold': rule.threshold,
            'current_value': value,
            'severity': rule.severity,
            'status': status,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        for channel in rule.notification_channels:
            if channel in self.notification_handlers:
                try:
                    self.notification_handlers[channel](message)
                except Exception as e:
                    logger.error(f"Failed to send alert to {channel}: {e}")
    
    def get_active_alerts(self) -> List[Dict[str, Any]]:
        """Get list of currently active alerts."""
        return [
            {
                'alert': alert_key,
                'since': since.isoformat()
            }
            for alert_key, since in self.active_alerts.items()
        ]


class HealthChecker:
    """Perform health checks on database and application."""
    
    def __init__(self, engine: Engine):
        self.engine = engine
        self.checks = {}
    
    def register_check(self, name: str, check_func: Callable, 
                      timeout: int = 30, critical: bool = False):
        """Register a health check."""
        self.checks[name] = {
            'function': check_func,
            'timeout': timeout,
            'critical': critical
        }
    
    def run_all_checks(self) -> Dict[str, Any]:
        """Run all registered health checks."""
        results = {
            'status': 'healthy',
            'checks': {},
            'timestamp': datetime.utcnow().isoformat()
        }
        
        overall_healthy = True
        
        for name, check_config in self.checks.items():
            try:
                start_time = time.time()
                result = self._run_check_with_timeout(
                    check_config['function'],
                    check_config['timeout']
                )
                duration = time.time() - start_time
                
                results['checks'][name] = {
                    'status': 'passed',
                    'duration': duration,
                    'result': result
                }
                
            except Exception as e:
                results['checks'][name] = {
                    'status': 'failed',
                    'error': str(e)
                }
                
                if check_config['critical']:
                    overall_healthy = False
        
        results['status'] = 'healthy' if overall_healthy else 'unhealthy'
        return results
    
    def _run_check_with_timeout(self, check_func: Callable, timeout: int):
        """Run a check function with timeout."""
        # Simple implementation - in production, use proper async timeout
        return check_func()
    
    def basic_database_check(self) -> Dict[str, Any]:
        """Basic database connectivity check."""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                result.fetchone()
                return {'database_responsive': True}
        except Exception as e:
            raise Exception(f"Database check failed: {e}")
    
    def connection_pool_check(self) -> Dict[str, Any]:
        """Check connection pool health."""
        pool = self.engine.pool
        
        return {
            'pool_size': pool.size(),
            'checked_out': pool.checkedout(),
            'overflow': pool.overflow(),
            'invalid': pool.invalid()
        }


# Notification handlers
def slack_notification_handler(message: Dict[str, Any]):
    """Send alert to Slack."""
    import requests
    
    # This would use your actual Slack webhook URL
    webhook_url = "YOUR_SLACK_WEBHOOK_URL"
    
    color = "danger" if message['status'] == 'fired' else "good"
    emoji = "🚨" if message['severity'] == 'critical' else "⚠️"
    
    payload = {
        "attachments": [{
            "color": color,
            "title": f"{emoji} Alert: {message['rule']}",
            "fields": [
                {"title": "Metric", "value": message['metric'], "short": True},
                {"title": "Current Value", "value": str(message['current_value']), "short": True},
                {"title": "Threshold", "value": str(message['threshold']), "short": True},
                {"title": "Status", "value": message['status'], "short": True}
            ],
            "ts": int(datetime.fromisoformat(message['timestamp']).timestamp())
        }]
    }
    
    # requests.post(webhook_url, json=payload)
    logger.info(f"Slack notification: {payload}")


def email_notification_handler(message: Dict[str, Any]):
    """Send alert via email."""
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    
    # Email configuration would come from environment variables
    subject = f"Database Alert: {message['rule']} - {message['status']}"
    
    body = f"""
    Alert: {message['rule']}
    Metric: {message['metric']}
    Current Value: {message['current_value']}
    Threshold: {message['threshold']}
    Severity: {message['severity']}
    Status: {message['status']}
    Time: {message['timestamp']}
    """
    
    # Implementation would use actual SMTP settings
    logger.info(f"Email notification: {subject}")


class MonitoringDashboard:
    """Simple monitoring dashboard for metrics visualization."""
    
    def __init__(self, metrics_collector: MetricsCollector, 
                 alert_manager: AlertManager):
        self.metrics = metrics_collector
        self.alerts = alert_manager
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """Get data for monitoring dashboard."""
        current_time = datetime.utcnow()
        last_hour = current_time - timedelta(hours=1)
        
        dashboard_data = {
            'timestamp': current_time.isoformat(),
            'metrics': {},
            'alerts': self.alerts.get_active_alerts(),
            'summary': {}
        }
        
        # Get recent metrics
        for metric_name in self.metrics.metrics.keys():
            recent_points = self.metrics.get_metrics(metric_name, last_hour)
            if recent_points:
                values = [p.value for p in recent_points]
                dashboard_data['metrics'][metric_name] = {
                    'current': values[-1],
                    'average': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'count': len(values)
                }
        
        # Generate summary
        dashboard_data['summary'] = {
            'total_metrics': len(dashboard_data['metrics']),
            'active_alerts': len(dashboard_data['alerts']),
            'health_status': 'healthy' if not dashboard_data['alerts'] else 'degraded'
        }
        
        return dashboard_data
    
    def generate_html_dashboard(self) -> str:
        """Generate HTML dashboard."""
        data = self.get_dashboard_data()
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Database Monitoring Dashboard</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .metric {{ border: 1px solid #ddd; margin: 10px; padding: 10px; }}
                .alert {{ background-color: #ffebee; border-left: 4px solid #f44336; }}
                .healthy {{ color: green; }}
                .degraded {{ color: orange; }}
                .critical {{ color: red; }}
            </style>
        </head>
        <body>
            <h1>Database Monitoring Dashboard</h1>
            <p>Last Updated: {data['timestamp']}</p>
            
            <h2>System Status: <span class="{data['summary']['health_status']}">{data['summary']['health_status'].upper()}</span></h2>
            
            <h3>Active Alerts ({data['summary']['active_alerts']})</h3>
            {"".join(f'<div class="alert">{alert["alert"]} (since {alert["since"]})</div>' for alert in data['alerts'])}
            
            <h3>Metrics</h3>
            {"".join(f'''
                <div class="metric">
                    <h4>{name}</h4>
                    <p>Current: {metrics["current"]:.2f}</p>
                    <p>Average: {metrics["average"]:.2f}</p>
                    <p>Min/Max: {metrics["min"]:.2f} / {metrics["max"]:.2f}</p>
                </div>
            ''' for name, metrics in data['metrics'].items())}
        </body>
        </html>
        """
        
        return html
```

This completes Section 10.2.1 with comprehensive database performance monitoring. Should I continue with Section 10.2.2 - Application Performance Monitoring (APM) integration, or move to a different subsection of Section 10?

